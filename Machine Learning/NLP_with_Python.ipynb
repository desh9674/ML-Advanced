{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_with_Python.ipynb","provenance":[],"collapsed_sections":["L7tgv2ddG3ka","qSBFA65LU_Kf","_ZoxPne5ereG","JD4Q3Iji6ZwC","booMqMSZA7Hc","7gG252bsIhNx","nCgqg8VENkCn","08qB55mbAG7_","UqRKoWhSLRrQ","SHlh4-Vlp2eN","f9g_l3moVGAt","4Wp1XNM_bEC8","BFeijMl7wMzp"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"L7tgv2ddG3ka"},"source":["# Language Processing with Just Python. Not Deep Learning\n","\n","Tokenizing text using functions word_tokenize and sent_tokenize.\n","\n","Computing Frequencies with FreqDist and ConditionalFreqDist.\n","\n","Generating Bigrams and collocations with bigrams and collocations.\n","\n","Stemming word affixes using PorterStemmer and LancasterStemmer.\n","\n","Tagging words to their parts of speech using pos_tag."]},{"cell_type":"code","metadata":{"id":"EEsqtAMZGRHf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4kco9jHRCiX","executionInfo":{"status":"ok","timestamp":1572093478589,"user_tz":-330,"elapsed":2853,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"0c7e9278-b555-4e79-d560-2c58111cb8ef","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import nltk\n","nltk.download('punkt')\n","text = \"Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991.\"\n","sentences = nltk.sent_tokenize(text)\n","print(sentences)\n","len(sentences)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['Python is an interpreted high-level programming language for general-purpose programming.', 'Created by Guido van Rossum and first released in 1991.']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"-xlwWiGuRPq_"},"source":["nltk.download('book')\n","from nltk.book import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfUrEFBGUmLi"},"source":["**The command loads nine texts and nine sentences, from the collection book.**\n","\n","**Finding text in these words.**"]},{"cell_type":"code","metadata":{"id":"YE3FOBMST9Ed","executionInfo":{"status":"ok","timestamp":1572093503554,"user_tz":-330,"elapsed":27798,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"9ae5b53a-fd96-4b0e-a895-6b526c7369a9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text1.findall(\"<tri.*r>\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["triangular; triangular; triangular; triangular\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qSBFA65LU_Kf"},"source":["# Functions on text1\n","\n","* Total Word Count\n","* Unique Word Count\n","* Transforming Words\n","* Word Coverage\n","* Filtering Words\n","* Frequency Distribution  \n","\n"]},{"cell_type":"code","metadata":{"id":"cONWlnFsUZ7S"},"source":["n_words = len(text1)\n","n_unique_words = len(set(text1)) # no of unique words\n","text1_lcw = [ word.lower() for word in set(text1) ] # lower case words\n","n_unique_words_lc = len(set(text1_lcw)) # no of lower case words\n","word_coverage1 = n_words / n_unique_words  #average times word appears\n","word_coverage2 = n_words / n_unique_words_lc\n","big_words = [word for word in set(text1) if len(word) > 17 ] # filter words by specific criteria\n","sun_words = [word for word in set(text1) if word.startswith('Sun') ] # having prefix 'Sun'\n","text1_freq = nltk.FreqDist(text1) # frwquency of all words in test1\n","text1_freq['Sunday'] # prnt freq of 'Sunday'\n","top3_text1 = text1_freq.most_common(3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kgx07QPfW8pY","executionInfo":{"status":"ok","timestamp":1572093504201,"user_tz":-330,"elapsed":28432,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"ec394990-ae4b-41f9-cd0c-c7726c5d97bd","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["large_uncommon_words = [word for word in text1 if word.isalpha() and len(word) > 7 ]\n","text1_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n","text1_uncommon_freq.most_common(3)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Queequeg', 252), ('Starbuck', 196), ('something', 119)]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"_ZoxPne5ereG"},"source":["# TEST 1"]},{"cell_type":"code","metadata":{"id":"FmsxgG-oXchu","executionInfo":{"status":"ok","timestamp":1572093504207,"user_tz":-330,"elapsed":28423,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"2e7903c3-3cba-4cca-c297-59e355c46856","colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["\n","from nltk.book import text6\n","\n","n = len(text6)\n","u = len(set(text6))\n","wc = n/u\n","print(n)\n","print(u)\n","print(wc)\n","\n","ise_ending_words =[word for word in text6 if word.endswith('ly')] # unique words list = set(text1)\n","print(len(ise_ending_words))\n","\n","contains_z = [word for word in set(text6) if 'z' in word]\n","print(len(contains_z))\n","\n","contains_pt = [word for word in set(text6) if 'pt' in word]\n","print(len(contains_pt))\n","\n","Title_words = [word for word in text6 if word[0].isupper() &  word[1:].islower()] # this returns Name/Title words, Take whichever you need\n","title_words1 = [word for word in text6 if not word[0].isupper() &  word[1:].islower()] # this returns the  small words, Take whichever you need\n","#print(len(title1_words))\n","\n","title_words = [word for word in text6 if word.istitle()]\n","print(len(title_words))\n","\n","text6freq = nltk.FreqDist(text6)# frwquency of all words in test1\n","text6freq['ARTHUR']\n","\n","print(text6.collocations())\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["16967\n","2166\n","7.833333333333333\n","109\n","8\n","11\n","2672\n","BLACK KNIGHT; clop clop; HEAD KNIGHT; mumble mumble; Holy Grail;\n","squeak squeak; FRENCH GUARD; saw saw; Sir Robin; Run away; CARTOON\n","CHARACTER; King Arthur; Iesu domine; Pie Iesu; DEAD PERSON; Round\n","Table; clap clap; OLD MAN; dramatic chord; dona eis\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gDhK8N5s5kL8","executionInfo":{"status":"ok","timestamp":1572093504208,"user_tz":-330,"elapsed":28418,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"bafc32fa-f49b-43ab-a837-e4e36b274116","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["s = 'Python is awesome'\n","print(nltk.pos_tag(nltk.word_tokenize(s)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JD4Q3Iji6ZwC"},"source":["# Corpus"]},{"cell_type":"markdown","metadata":{"id":"Wgi1inVT5mep"},"source":["Genesis: It is a collection of few words across multiple languages.\n","\n","Brown: It is the first electronic corpus of one million English words.\n","\n","Other Corpus in nltk\n","\n","Gutenberg : Collections from Project Gutenberg\n","Inaugural : Collection of U.S Presidents inaugural speeches\n","\n","\n","\n","\n","---\n","\n","\n","\n","*   stopwords : Collection of stop words.\n","* reuters : Collection of news articles.\n","* cmudict : Collection of CMU Dictionary words.\n","* movie_reviews : Collection of Movie Reviews.\n","* np_chat : Collection of chat text.\n","* names : Collection of names associated with males and females.\n","* state_union : Collection of state union address.\n","* wordnet : Collection of all lexical entries.\n","* words : Collection of words in Wordlist corpus.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5NnoXWZ2kvJZ","executionInfo":{"status":"ok","timestamp":1572093505473,"user_tz":-330,"elapsed":29667,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"9940f817-0903-4126-fac4-6bb15ba54d35","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["from nltk.corpus import genesis\n","genesis.fileids()\n","\n","for fileid in genesis.fileids():\n","    n_chars = len(genesis.raw(fileid))\n","    n_words = len(genesis.words(fileid))\n","    n_sents = len(genesis.sents(fileid))\n","    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4 30 english-kjv.txt\n","4 19 english-web.txt\n","5 15 finnish.txt\n","4 23 french.txt\n","4 23 german.txt\n","4 20 lolcat.txt\n","4 27 portuguese.txt\n","4 30 swedish.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ol77A6Uv6-cO"},"source":["A text corpus is organized into any of the following four structures.\n","\n","Isolated - Holds Individual text collections.\n","\n","Categorized - Each text collection tagged to a category.\n","\n","Overlapping - Each text collection tagged to one or more categories, and\n","\n","Temporal - Each text collection tagged to a period, date, time, etc."]},{"cell_type":"code","metadata":{"id":"LKptf_7z6L0g"},"source":["from nltk.corpus import PlaintextCorpusReader\n","#corpus_root = '/usr/share/dict'\n","#wordlists = PlaintextCorpusReader(corpus_root, '.*')\n","#wordlists.fileids()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"booMqMSZA7Hc"},"source":["# corpus Test"]},{"cell_type":"code","metadata":{"id":"LMLXzW7Q8AOD","executionInfo":{"status":"ok","timestamp":1572093512840,"user_tz":-330,"elapsed":37025,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"36f87c38-afde-4d2f-99b1-5c890093fb7d","colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["from nltk.corpus import gutenberg as gt\n","gt.fileids()\n","\n","for fileid in gt.fileids():\n","    n_chars = len(gt.raw(fileid))\n","    n_words = len(gt.words(fileid))\n","    n_sents = len(gt.sents(fileid))\n","    \n","    n =  len(gt.raw(fileid))\n","    u = len(set(gt.raw(fileid)))\n","    \n","    print(n/u, fileid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11520.402597402597 austen-emma.txt\n","5978.102564102564 austen-persuasion.txt\n","8628.48717948718 austen-sense.txt\n","57767.386666666665 bible-kjv.txt\n","522.6438356164383 blake-poems.txt\n","3370.7972972972975 bryant-stories.txt\n","1128.84 burgess-busterbrown.txt\n","1978.013698630137 carroll-alice.txt\n","5082.777777777777 chesterton-ball.txt\n","5570.260273972603 chesterton-brown.txt\n","4162.662337662337 chesterton-thursday.txt\n","11689.475 edgeworth-parents.txt\n","15158.414634146342 melville-moby_dick.txt\n","5852.75 milton-paradise.txt\n","1581.8309859154929 shakespeare-caesar.txt\n","2431.0597014925374 shakespeare-hamlet.txt\n","1497.7761194029852 shakespeare-macbeth.txt\n","9002.721518987342 whitman-leaves.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"clYVuWgE-vue","executionInfo":{"status":"ok","timestamp":1572093512842,"user_tz":-330,"elapsed":37011,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"904d4bf2-1eeb-4dca-a96d-9ae5475b5b82","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["aus_words = gt.words('austen-sense.txt')\n","aus_words_alpha = [word for word in aus_words if word.isalpha()]\n","aus_words_gt4_z = [word for word in aus_words_alpha if (len(word)>4)  &  ('z' in word)]\n","print(len(aus_words_gt4_z))\n","print(aus_words_gt4_z)\n","\n","aus_unique_words = set([word.lower() for word in aus_words_alpha])\n","\n","from nltk.corpus import words as wd\n","engcorpus_words = wd.words()\n","engcorpus_unique_words = set([word.lower() for word in engcorpus_words])\n","\n","unusual_words = aus_unique_words - engcorpus_unique_words # - aus_unique_words\n","unusual_words = list(unusual_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["51\n","['amazing', 'aggrandizement', 'sympathize', 'zealously', 'amazement', 'amazement', 'apologized', 'puzzled', 'idolized', 'seized', 'amazement', 'amazement', 'gazing', 'puzzled', 'amazement', 'puzzled', 'temporizing', 'amazement', 'amazement', 'apologize', 'zealous', 'freeze', 'tranquilize', 'hazarded', 'amazement', 'Eliza', 'Eliza', 'Eliza', 'Eliza', 'amazingly', 'amazement', 'teazed', 'teazing', 'monopolize', 'puzzled', 'horizon', 'seizure', 'amazement', 'Eliza', 'Eliza', 'zealous', 'gazing', 'hazarding', 'Eliza', 'Eliza', 'hazard', 'amazement', 'familiarized', 'puzzle', 'puzzled', 'puzzled']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7gG252bsIhNx"},"source":["# Conditional Frequency"]},{"cell_type":"markdown","metadata":{"id":"_kyyQwwELo0q"},"source":["**Words by Genre**"]},{"cell_type":"code","metadata":{"id":"HLsfXoAGB7w1","executionInfo":{"status":"ok","timestamp":1572093515377,"user_tz":-330,"elapsed":1987,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"334b9f52-1337-47d3-cc0a-d1ffa9be1511","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]\n","cfd = nltk.ConditionalFreqDist(c_items)\n","cfd.conditions()\n","cfd['V']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'cabbage': 2, 'potato': 1})"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"E7oZMZa4LuDI"},"source":["**Word Count**"]},{"cell_type":"code","metadata":{"id":"5KkpWVtlIwrW","executionInfo":{"status":"ok","timestamp":1569242933729,"user_tz":-330,"elapsed":4330,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"021a2aaa-4cae-426e-ccdd-30ef4c90d9dc","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["from nltk.corpus import brown\n","\n","cfd = nltk.ConditionalFreqDist([ (genre, word) for genre in brown.categories() for word in brown.words(categories=genre) ])\n","cfd.conditions()\n","\n","cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["           leadership    worship   hardship \n","government         12          3          2 \n","     humor          1          0          0 \n","   reviews         14          1          2 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i_OtU3lwLikz"},"source":["**Cumulative**"]},{"cell_type":"code","metadata":{"id":"VAcnb2E4J3Q9","executionInfo":{"status":"ok","timestamp":1569243142086,"user_tz":-330,"elapsed":962,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"a7014f84-d64e-4a9a-e007-5ef56456012a","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["           leadership    worship   hardship \n","government         12         15         17 \n","     humor          1          1          1 \n","   reviews         14         15         17 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LuQBmEikMKsO"},"source":["**Individual Word Frequency**"]},{"cell_type":"code","metadata":{"id":"AEqeFs_oLhOC","executionInfo":{"status":"ok","timestamp":1569243368714,"user_tz":-330,"elapsed":916,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"0f3dd31c-85a4-4371-fb11-b23c11942c6c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["news_fd = cfd['news']\n","news_fd.most_common(3)\n","news_fd['the']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5580"]},"metadata":{"tags":[]},"execution_count":136}]},{"cell_type":"markdown","metadata":{"id":"b16MxkLpM2ak"},"source":["**Compare Frequencies**\n","\n","The expression cfd2['female'] > cfd2['male'] checks if the last characters in females occur more frequently than the last characters in males."]},{"cell_type":"code","metadata":{"id":"wb-P77pSMYj8","executionInfo":{"status":"ok","timestamp":1569243564366,"user_tz":-330,"elapsed":898,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"b886a64e-97bb-4343-e3d2-c6fc086c51bc","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from nltk.corpus import names\n","nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids() for name in names.words(fid) ]\n","cfd2 = nltk.ConditionalFreqDist(nt)\n","cfd2['female'] > cfd2['male']\n","cfd2.tabulate(samples=['a', 'e']) # freq of 'a' and 'e' in names"],"execution_count":null,"outputs":[{"output_type":"stream","text":["          a    e \n","female 1773 1432 \n","  male   29  468 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nCgqg8VENkCn"},"source":["# FreqDist Test"]},{"cell_type":"code","metadata":{"id":"txNrcCShMz9t"},"source":["import nltk\n","from nltk.corpus import brown\n","\n","\n","#print(brown.words(categories='news'))\n","#nltk.ConditionalFreqDist()\n","\n","brown_cfd = nltk.ConditionalFreqDist([(genre, word.lower()) for genre in brown.categories() for word in brown.words(categories=genre)])\n","brown_cfd.conditions()\n","print(brown_cfd.tabulate(conditions=['news', 'religion', 'romance'], samples=['can', 'could', 'may', 'might', 'must', 'will']))\n","# samples= words to find\n","# conditions = categories to look into\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6VwHllvYsLt"},"source":["from nltk.corpus import inaugural\n","import nltk\n","nltk.download('book')\n","ac_cfd = nltk.ConditionalFreqDist(\n","           (fileid[:4], target)   ### be careful with this axes, exchange them to get required graph or table.\n","           for fileid in inaugural.fileids()\n","           for w in inaugural.words(fileid)\n","           for target in ['america', 'citizen']\n","           if w.lower().startswith(target))\n","#ac_cfd.plot()\n","A = ['1841', '1993']\n","B = ['america', 'citizen']\n","print(ac_cfd.tabulate(conditions=A, samples=B))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcc0w_yZOtWU","executionInfo":{"status":"ok","timestamp":1569406638001,"user_tz":-330,"elapsed":8093,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"858ed6ec-a58b-44dc-fcc8-1b00df8d1885","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from nltk.corpus import reuters as ret\n","cfd1 = nltk.ConditionalFreqDist([ (genre, word) for genre in ret.categories() for word in ret.words(categories=genre) ])\n","print(cfd1['zinc']['lead'])\n","cfd1['zinc']['smelter']\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["40\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["33"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"ougVn01cYWlL"},"source":["# Processing Raw Text and Gathering data\n","\n","Tokenization is a step in which a text is broken down into words and punctuation.\n","\n","Using the obtained list of tokens, an object of NLTK text can be created as shown below.\n","\n","Thus obtained text can be used for further linguistic processing."]},{"cell_type":"code","metadata":{"id":"ZnCmShMEWB1X"},"source":["import re\n","\n","## getting txt file for raw txt data processing\n","from urllib import request   # requests module\n","url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n","content1 = request.urlopen(url).read()\n","\n","\n","\n","## \n","from urllib import request\n","\n","url = \"http://www.bbc.com/news/health-42802191\"\n","html_content = request.urlopen(url).read()   # accessing url\n","\n","\n","from bs4 import BeautifulSoup\n","soup = BeautifulSoup(html_content, 'html.parser') # accing url and reading it, no bowser required\n","inner_body = soup.find_all('div', attrs={'class':'story-body__inner'}) # its beautiful soup,  get obj under class 'story-body_inner'\n","inner_text = [elm.text for elm in inner_body[0].find_all(['h1', 'h2', 'p', 'li']) ]#extract text from that object\n","text_content2 = '\\n'.join(inner_text)\n","\n","\n","\n","####      TOKENIZATION   ############\n","text_content1 = content1.decode('unicode_escape') # that text is still in unicode\n","tokens1 = nltk.word_tokenize(text_content1)\n","print(tokens1[3:8])\n","\n","tokens2 = nltk.word_tokenize(text_content2)\n","print(tokens2[:5])  #['Smokers', 'need', 'to', 'quit', 'cigarettes']\n","print(len(tokens2))\n","tokens2_2 = re.findall(r'\\w+', text_content2)  # also can use re\n","print(len(tokens2_2))\n","\n","\n","pattern = r'\\w+'\n","tokens2_3 = nltk.regexp_tokenize(text_content2, pattern) # same as above\n","print(len(tokens2_3))\n","\n","\n","input_text2 = nltk.Text(tokens2)   # nltk TEXT Object\n","print(type(input_text2))  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08qB55mbAG7_"},"source":["# Raw Text Test"]},{"cell_type":"code","metadata":{"id":"qLDnFarZmi1v"},"source":["from urllib import request\n","url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n","\n","html_content = request.urlopen(url).read()\n","\n","from bs4 import BeautifulSoup\n","soup = BeautifulSoup(html_content, 'html.parser')\n","\n","#soup.prettify()\n","\n","n_links = soup.find_all('a') #attrs={'class':'external text'}\n","print(len(n_links))\n","\n","\n","table = soup.find(attrs={'class':'wikitable'})\n","rows = table.find_all('tr')\n","\n","rows = rows[1:]\n","\n","for row in rows:\n","  cols = row.find_all('td')\n","  first_col = cols[0]\n","  tet = first_col.get_text()\n","  print(tet)\n","  \n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsTmhYbABddz","executionInfo":{"status":"ok","timestamp":1569309954595,"user_tz":-330,"elapsed":958,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"0debf8ae-9237-4d3b-8904-ae4ccf12627b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["s = 'Python is cool!!!'\n","print(re.findall(r'\\w+', s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Python', 'is', 'cool']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UqRKoWhSLRrQ"},"source":["# Bigrams\n","\n","Bigrams represent a set of two consecutive words appearing in a text.\n","\n","bigrams function is called on tokenized words, as shown in the following example, to obtain bigrams."]},{"cell_type":"code","metadata":{"id":"fPC1rt1MJlks","executionInfo":{"status":"ok","timestamp":1569314675532,"user_tz":-330,"elapsed":1198,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"3ad47bca-7e9f-45c1-9c46-90ba1b89d1d3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import nltk\n","from nltk.corpus import genesis\n","\n","s = 'Python is an awesome language.'\n","tokens = nltk.word_tokenize(s)\n","list(nltk.bigrams(tokens))\n","\n","\n","eng_tokens = genesis.words('english-kjv.txt')\n","eng_bigrams = nltk.bigrams(eng_tokens)\n","filtered_bigrams = [ (w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5 ]\n","\n","\n","#eng_bifreq = nltk.FreqDist(filtered_bigrams)\n","#print(eng_bifreq.most_common(3))\n","\n","eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n","print(eng_cfd['living'].most_common(2))   #[('creature', 7), ('thing', 4)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7V9eBnMML1D0"},"source":["def generate(cfd, word, n=5):\n","    n_words = []\n","    for i in range(n):\n","      n_words.append(word)\n","      word = cfd[word].max()\n","    return n_words\n","  \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NA2Uais2Oair"},"source":["generate(eng_cfd, 'living')\n","list(nltk.ngrams(tokens, 4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZ-U9Df6QXCV"},"source":["Similar to Bigrams, Trigrams refers to set of all three consecutive words appearing in text.\n","\n","\n","nltk also provides the function ngrams. It can be used to determine a set of all possible n consecutive words appearing in a text."]},{"cell_type":"markdown","metadata":{"id":"3xhbJmF1Q2JR"},"source":["**Collocations**\n","Collocations are expressions of multiple words which commonly co-occur. For example, the top ten bigram collocations in Genesis are listed below, as measured using Pointwise Mutual Information."]},{"cell_type":"code","metadata":{"id":"lNcpEsP7Qg6i","executionInfo":{"status":"ok","timestamp":1569311720606,"user_tz":-330,"elapsed":894,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"be541011-0913-40ef-e5a4-d091a7743850","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["tokens = genesis.words('english-kjv.txt')\n","gen_text = nltk.Text(tokens)\n","gen_text.collocations()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n","spake unto; thou art; LORD God; every living; God hath; begat sons;\n","seven years; shalt thou; little ones; living creature; creeping thing;\n","savoury meat; thirty years; every beast\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SHlh4-Vlp2eN"},"source":["# Bigrams Test"]},{"cell_type":"code","metadata":{"id":"e2a78iGzRICv","executionInfo":{"status":"ok","timestamp":1569318412610,"user_tz":-330,"elapsed":8317,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"f0b4aa82-3d7a-4ca8-eaeb-0e0821f44ded","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import nltk\n","from nltk.corpus import brown\n","\n","news_words = brown.words(categories='news')\n","#print(news_words)\n","lc_news_words = [word.lower() for word in news_words]\n","len_news_words = [len(word) for word in lc_news_words]\n","#print(len_news_words)\n","news_len_bigrams = nltk.bigrams(len_news_words)\n","cfd_news = nltk.ConditionalFreqDist(news_len_bigrams)\n","#cfd_news.plot()\n","print(cfd_news[4][6])\n","\n","lc_news_bigrams = nltk.bigrams(lc_news_words)\n","bigrams = list(lc_news_bigrams)\n","#print(bigrams[1][0].isalpha())\n","lc_news = []\n","for bi in bigrams:\n","  if bi[0].isalpha() & bi[1].isalpha():\n","    lc_news.append(bi)\n","    \n","from nltk.corpus import stopwords\n","\n","stop_words = stopwords.words()\n","lc_stop_words = [word.lower() for word in stop_words]\n","#print(lc_stop_words)\n","\n","lc_news_alpha_nonstop_bigrams = [bis for bis in lc_news if bis[1] not in lc_stop_words and bis[0] not in lc_stop_words]\n","print(len(lc_news_alpha_nonstop_bigrams))\n","    \n","#print(lc_news[1:76])\n","print(len(bigrams))\n","print(len(lc_news))\n","\n","#lc_news_alpha_bigrams = [bigram for bigram in bigrams if bigram[0].isalpha() & bigram[1].isalpha()]\n","#print(list(lc_news_bigrams))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1084\n","17132\n","100553\n","70137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GRf6yQv3RxWa","executionInfo":{"status":"ok","timestamp":1569319707698,"user_tz":-330,"elapsed":899,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"095f8db0-9af7-4498-a757-29fde0d2055f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from nltk.book import text6\n","words = [word for word in text6]\n","bigams = nltk.bigrams(words)\n","cfd6 = nltk.FreqDist(bigams)\n","print(cfd6[('clop', 'clop')])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f9g_l3moVGAt"},"source":["# POS TAGGING\n","\n","The method of categorizing words into their parts of speech and then labeling them respectively is called POS Tagging"]},{"cell_type":"code","metadata":{"id":"yJtOL-QwN3y3"},"source":["import nltk\n","text = 'Python is awesome.'\n","words = nltk.word_tokenize(text)\n","nltk.pos_tag(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BeLj13aDUSXr"},"source":["text = 'Python/NN is/VB awesome/JJ ./.'\n","[ nltk.tag.str2tuple(word) for word in text.split() ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZRjJDR5UZjy"},"source":["from nltk.corpus import brown\n","brown_tagged = brown.tagged_words()\n","brown_tagged[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESxzCyqJUcKB"},"source":["default_tagger = nltk.DefaultTagger('NN')\n","default_tagger.tag(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWeIPAg4Wcj7"},"source":["defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}\n","\n","baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n","baseline_tagger.tag(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqWO2U0yXL2E"},"source":["**You can build a unigram tagger through a process known as training.\n","\n","Then use the tagger to tag words in a test set and evaluate the performance.**"]},{"cell_type":"code","metadata":{"id":"cpu2ofdQWrxz"},"source":["from nltk.corpus import brown\n","brown_tagged_sents = brown.tagged_sents(categories='government')\n","brown_sents = brown.sents(categories='government')\n","len(brown_sents)  #3032\n","train_size = int(len(brown_sents)*0.8)\n","train_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BYCPdaoXcNN"},"source":["train_sents = brown_tagged_sents[:train_size]\n","test_sents = brown_tagged_sents[train_size:]\n","unigram_tagger = nltk.UnigramTagger(train_sents)\n","unigram_tagger.evaluate(test_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAMbgZOzXiQs"},"source":["unigram_tagger.tag(brown_sents[3000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Wp1XNM_bEC8"},"source":["# Tagging Test"]},{"cell_type":"code","metadata":{"id":"Tp6FigYPXs7L","executionInfo":{"status":"ok","timestamp":1569399438489,"user_tz":-330,"elapsed":6049,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"16191f58-3679-4a84-849e-cefc65a85308","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from nltk.corpus import brown\n","import nltk\n","brown_tagged_words = brown.tagged_words()\n","brown_tagged_trigrams = list(nltk.trigrams(brown_tagged_words))\n","#print(brown_tagged_trigrams[:2][0][1])\n","#print(len(brown_tagged_trigrams))\n","pos = []\n","\n","for i in range(len(brown_tagged_trigrams)):\n","  t1 = brown_tagged_trigrams[i][0][1]\n","  t2 = brown_tagged_trigrams[i][1][1]\n","  t3 = brown_tagged_trigrams[i][2][1]\n","  top = (t1,t2,t3)\n","  pos.append(top)\n","  \n","brown_trigram_pos_tags_freq = nltk.FreqDist(pos)\n","print(brown_trigram_pos_tags_freq[('JJ','NN','IN')])\n","\n","#print(pos[:4])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rMyY8A0lcHMM","executionInfo":{"status":"ok","timestamp":1569400866812,"user_tz":-330,"elapsed":8470,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"a752b1e5-0949-4dd9-95a0-6049e8a5e315","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["brown_tagged_sents = brown.tagged_sents()\n","total_size = len(brown_tagged_sents)\n","\n","train_size = int(total_size*0.8)\n","train_sents = brown_tagged_sents[:train_size]\n","test_sents = brown_tagged_sents[train_size:]\n","\n","unigram_tagger = nltk.UnigramTagger(train_sents)\n","tag_performance = unigram_tagger.evaluate(test_sents)\n","print(tag_performance)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8773754310202373\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i-BMZKyKf0CL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"odvkA0nWSKIM","executionInfo":{"status":"ok","timestamp":1569413590113,"user_tz":-330,"elapsed":2170,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"4ebb7675-e6b1-4848-a75d-612798a7f09d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cfd = nltk.ConditionalFreqDist(\n","           (target, fileid[:4])\n","           for fileid in inaugural.fileids()\n","           for w in inaugural.words(fileid)\n","           for target in ['america', 'citizen']\n","           if w.lower().startswith(target))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BFeijMl7wMzp"},"source":["# Stemming\n","\n","Stemming is a process of stripping affixes from words.\n","\n","With stemming, the words playing, played and play will be treated as single word, i.e. play."]},{"cell_type":"code","metadata":{"id":"09xIdx4Rux3z","executionInfo":{"status":"ok","timestamp":1569406379589,"user_tz":-330,"elapsed":6110,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"e9e5c87e-ed9f-4e2e-a418-80b3a85a5d07","colab":{"base_uri":"https://localhost:8080/"}},"source":["from nltk import PorterStemmer\n","porter = nltk.PorterStemmer()\n","porter.stem('builders')\n","porter.stem('ceremony')\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ceremoni'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"NKC9RM9pwa4m","executionInfo":{"status":"ok","timestamp":1569406303507,"user_tz":-330,"elapsed":1161,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"fb1457e7-ff88-4f3b-e50b-db35c826e31e","colab":{"base_uri":"https://localhost:8080/"}},"source":["from nltk import LancasterStemmer\n","lancaster = LancasterStemmer()\n","lancaster.stem('builders')\n","lancaster.stem('lying')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'lying'"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"Nw4IVGtlwrKk","executionInfo":{"status":"ok","timestamp":1569320129714,"user_tz":-330,"elapsed":1242,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"81b28613-524c-4910-880a-c2d3988f186e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from nltk import PorterStemmer\n","from nltk.book import text1\n","\n","lc_words = [word.lower() for word in text1]\n","porter = PorterStemmer()\n","p_stem_words = [porter.stem(word) for word in set(lc_words) ]\n","len(set(p_stem_words))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10927"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"nKnFDXiFxSzF"},"source":["Lemma is a lexical entry in a lexical resource such as word dictionary.\n","\n","You can find multiple Lemma's with the same spelling. These are known as homonyms.\n","\n","For example, consider the two Lemma's listed below, which are homonyms.\n","\n","1. saw [verb] - Past tense of see\n","2. saw [noun] - Cutting instrument\n","\n","nltk comes with WordNetLemmatizer. This lemmatizer removes affixes only if the resulting word is found in lexical resource, Wordnet."]},{"cell_type":"code","metadata":{"id":"sbLDF9ojw3Ji","executionInfo":{"status":"ok","timestamp":1569320299530,"user_tz":-330,"elapsed":3234,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"a4ae347e-9595-466f-8897-24f982789290","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["wnl = nltk.WordNetLemmatizer()\n","wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]\n","len(set(wnl_stem_words))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15168"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"HkXqTUDFST-_"},"source":["import nltk\n","nltk.download('book')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gNrvl5ayI2e"},"source":["# Stemming Test\n","\n","\n","Ok this was the hardest test that bugged me for two days,\n","i reported it twice and finally i received an answer on stackoverflow, instead to doing the set i should have done !=. That was the big error and i finally cleared it.\n"]},{"cell_type":"code","metadata":{"id":"vt1cgT4ix18L","executionInfo":{"status":"ok","timestamp":1569485858218,"user_tz":-330,"elapsed":98711,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"931bf80b-0713-433e-f78e-666ce110ebd2","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import nltk\n","from nltk.corpus import brown\n","\n","humor_words = brown.words(categories='humor',)\n","lc_humor_words = [word.lower() for word in humor_words]\n","lc_humor_uniq_words = list(set(lc_humor_words))\n","\n","from nltk.corpus import words\n","\n","wordlist_words = words.words()\n","wordlist_uniq_words = list(set(wordlist_words))\n","print(len(lc_humor_uniq_words))\n","print(len(wordlist_uniq_words))\n","\n","from nltk import PorterStemmer\n","porter = nltk.PorterStemmer()\n","\n","from nltk import LancasterStemmer\n","lancaster = nltk.LancasterStemmer()\n","\n","p_stemmed = [porter.stem(word) for word in lc_humor_uniq_words]\n","l_stemmed = [lancaster.stem(word) for word in lc_humor_uniq_words]\n","\n","p_stemmed_in_wordlist = [word for word in p_stemmed if word in wordlist_uniq_words]\n","l_stemmed_in_wordlist = [word for word in l_stemmed if word in wordlist_uniq_words]\n","\n","print(len(p_stemmed_in_wordlist))\n","print(len(l_stemmed_in_wordlist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4755\n","235892\n","2802\n","2704\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u5nzuBz7Ep2t"},"source":["Filter the words from lc_humor_uniq_words which have the same length as its corresponding stemmed word present in p_stemmed, and also contains at least one different character from the corresponding stemmed word. Store the result in the list p_stemmed_diff.\n","\n","Filter the words from lc_humor_uniq_words which have the same length as its corresponding stemmed word, present in l_stemmed, and also contains at least one different character from the corresponding stemmed word. Store the result in list l_stemmed_diff."]},{"cell_type":"code","metadata":{"id":"Dg6sNksqJBvE","executionInfo":{"status":"ok","timestamp":1569410693954,"user_tz":-330,"elapsed":834,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"b12d0eb0-30dd-4643-e9dd-b497fab46580","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(len(p_stemmed))\n","print(len(l_stemmed))\n","print(len(lc_humor_uniq_words))\n","print(lc_humor_uniq_words[0:15])\n","print(p_stemmed[0:15])\n","print(l_stemmed[0:15])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4755\n","4755\n","4755\n","['screens', 'housebreakers', 'paris', 'deigned', 'preferred', 'newspaper', 'waxed', 'stringy', 'litigant', 'dens', 'themselves', 'charm', 'impassioned', 'courtiers', 'all']\n","['screen', 'housebreak', 'pari', 'deign', 'prefer', 'newspap', 'wax', 'stringi', 'litig', 'den', 'themselv', 'charm', 'impass', 'courtier', 'all']\n","['screens', 'housebreak', 'par', 'deign', 'prefer', 'newspap', 'wax', 'stringy', 'litig', 'den', 'themselv', 'charm', 'impass', 'courty', 'al']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CQZ7cuJ016qv","executionInfo":{"status":"ok","timestamp":1569486464604,"user_tz":-330,"elapsed":966,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"25e394da-174c-486b-a567-519074782544","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["\n","p_stemmed_diff = []\n","import itertools\n","\n","for word, stem in zip(lc_humor_uniq_words, p_stemmed):\n","  if len(word)==len(stem) and word != stem:\n","        p_stemmed_diff.append(word)\n","\n","l_stemmed_diff = []\n","for word1, stem1 in zip(lc_humor_uniq_words, l_stemmed):\n","  if len(word1)==len(stem1) and word1 != stem1:\n","        l_stemmed_diff.append(word1)\n","\n","print(len(p_stemmed_diff))  # gives 252\n","print(len(l_stemmed_diff))  # gives 3\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["257\n","3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JDO9cEDsUXfO","executionInfo":{"status":"ok","timestamp":1569487985055,"user_tz":-330,"elapsed":938,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"ac38eaf6-0aaa-4623-e2d9-3dd074b18a71","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["p_stemmed_diff2 = []\n","import itertools\n","\n","for word, stem in zip(lc_humor_uniq_words, p_stemmed):\n","  if len(word)==len(stem) and len(set(word) - set(stem)) > 0:\n","        p_stemmed_diff2.append(word)\n","\n","\n","diff5words = [word for word in p_stemmed_diff if word not in p_stemmed_diff2]  \n","print(diff5words)\n","stem5words = [porter.stem(word) for word in s2]  \n","print(stem5words)\n","set5words = [set(word) for word in diff5words]\n","set5stems = [set(stem) for stem in stem5words]\n","print(set5words)\n","print(set5stems)\n","\n","set('anabody') - set('anabodi')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['pityingly', 'anybody', 'yearly', 'mystery', 'everybody']\n","['pityingli', 'anybodi', 'yearli', 'mysteri', 'everybodi']\n","[{'y', 'p', 'i', 'g', 'l', 'n', 't'}, {'y', 'b', 'd', 'o', 'n', 'a'}, {'y', 'r', 'e', 'l', 'a'}, {'y', 'r', 'e', 's', 'm', 't'}, {'y', 'b', 'd', 'r', 'e', 'o', 'v'}]\n","[{'y', 'p', 'i', 'g', 'l', 'n', 't'}, {'y', 'b', 'd', 'i', 'o', 'n', 'a'}, {'y', 'r', 'i', 'e', 'l', 'a'}, {'y', 'r', 'i', 'e', 's', 'm', 't'}, {'y', 'b', 'd', 'r', 'i', 'e', 'o', 'v'}]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'y'}"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"QCO3KVIBHCT7","executionInfo":{"status":"ok","timestamp":1569410775892,"user_tz":-330,"elapsed":829,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"167c84bb-e74c-4008-efc7-153f9b96d5ce","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(p_stemmed_diff[:15])\n","print(l_stemmed_diff[:15])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['stringy', 'seemingly', 'dearly', 'comedy', 'pleasantly', 'fascinatingly', 'tawdry', 'apply', 'quality', 'clarity', 'unexpectedly', 'study', 'blindly', 'triumphantly', 'company']\n","['except', 'belief', 'relief']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Db1QtgySCbnq","executionInfo":{"status":"ok","timestamp":1569419778785,"user_tz":-330,"elapsed":960,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"360f777d-d121-43dd-a406-84c2ce65dee0","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["p1 = lc_humor_uniq_words[1:500]\n","p2 = p_stemmed[1:500]\n","\n","l1 = lc_humor_uniq_words[1:5000]\n","l2 = l_stemmed[1:5000]\n","p = []\n","\n","\n","p_stemmed_diff = []\n","for word, stem in zip(p_stemmed, lc_humor_uniq_words):\n","  if len(word)==len(stem) and len((set(stem) - set(word))) > 0:\n","        p_stemmed_diff.append(word)\n","\n","          \n","l_stemmed_diff = [] \n","for word1, stem1 in zip(l_stemmed, lc_humor_uniq_words):\n","  if len(word1)==len(stem1) and len((set(stem1) - set(word))) > 0:\n","        l_stemmed_diff.append(word1)\n","\n","         \n","print(len(p_stemmed_diff))\n","print(len(l_stemmed_diff))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["252\n","1585\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rq3N5lEWP1Na","executionInfo":{"status":"ok","timestamp":1569419820164,"user_tz":-330,"elapsed":909,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"d366ceef-08d9-4928-d82e-d2c007e00bcb","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(p_stemmed_diff[0:10])\n","print(l_stemmed_diff[0:10])\n","print(lc_humor_uniq_words[0:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['stringi', 'seemingli', 'dearli', 'comedi', 'pleasantli', 'fascinatingli', 'tawdri', 'appli', 'qualiti', 'clariti']\n","['screens', 'stringy', 'charm', 'rid', 'albert', 'f.d.r.', 'joss', 'tailor-made', 'threats', 'highest']\n","['screens', 'housebreakers', 'paris', 'deigned', 'preferred', 'newspaper', 'waxed', 'stringy', 'litigant', 'dens']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AVPEL3Do-__6","executionInfo":{"status":"ok","timestamp":1569412150756,"user_tz":-330,"elapsed":883,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"e17bd3b2-9e91-44e8-88ab-3323b494491d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a = [1,2,4,5,4,5,6,6,7]\n","b= [5,6,67,7,8,8,9,9,12,4]\n","\n","set(a)-set(b)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{1, 2}"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"id":"40QAI0WGcjIi"},"source":["# Reporting with Feedback\n","\n","**\n","hi,\n","can someone\n","\n","Please explain or elaborate the final(4th) task in NLP with Python |6| Stemming Hands-On. I have written the code right way and it gives the required output but it is not accepted.\n","\n","Here is the feedback i have written along with problem statement and my solution.\n","\n","\"\"\"\n","\n","**Filter the words from lc_humor_uniq_words which have the same length as its corresponding stemmed word present in p_stemmed, and also contains at least one different character from the corresponding stemmed word. Store the result in the list p_stemmed_diff.\n","\n","Filter the words from lc_humor_uniq_words which have the same length as its corresponding stemmed word, present in l_stemmed, and also contains at least one different character from the corresponding stemmed word. Store the result in list l_stemmed_diff.**\n","\n","\n","Above condition statement is perfectly satisfied by the following code block.\n","\n","###\n","p_stemmed_diff = []\n","for word, stem in zip(lc_humor_uniq_words, p_stemmed):\n","  if len(word)==len(stem) and len((set(word) - set(stem))) >\n","0:\n","        p_stemmed_diff.append(word)\n","\n","l_stemmed_diff = []\n","for word1, stem1 in zip(lc_humor_uniq_words, l_stemmed):\n","  if len(word1)==len(stem1) and len((set(word1)-set(stem1))) > 0:\n","        l_stemmed_diff.append(word1)\n","\n","print(len(p_stemmed_diff))  # gives 252\n","print(len(l_stemmed_diff))  # gives 3\n","\n","###\n","\n","It gives the output of 252,3 witch is not accepted in the test.\n","Please help me resolve the issue.\n","\n","The final(4th) test in Stemming hands-On has a bug or something is wrong, am able to clear all previous tests.\n","Please inform me if there are any issues with test or am not getting the task statement right way.\n","please help\n","\n","\"\"\"\n","\n","according to condition the code should work and output should be accepted, could you please explain what am i missing here.\n","\n","Thank You\n","\n","**"]}]}
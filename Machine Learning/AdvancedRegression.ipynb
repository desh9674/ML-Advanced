{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AdvancedRegression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TuoPs2PV1Gv_","colab_type":"text"},"source":["#Limitations of Linear Models\n","Linear Models are the most widely used models in Statistics.\n","\n","But they come with their own limitations.\n","\n","Not proficient in handling Binary Data\n","\n","Not Accurate When count data(number of footfalls, number of pages visited etc ..) is involved.\n","\n","Some variable have a constraint of being only strictly positive\n","\n","To fix some of these problems we go can go for Transformation.\n","\n","In some scenarios Transformation minimises interpretability so we have to look for other alternatives.\n","\n","#GLM\n","To overcome the some limitations of Linear Models , we can go for Generalized Linear Models(GLMs).\n","\n","In GLMs the modeling is done on the scale in which the data was recorded.\n","\n","GLMs honor the known assumptions of the data\n","\n","GLMs comprise of 3 components\n","\n","Random Component that explains the data distribution that describes Randomness / Errors.\n","\n","Systematic Component consists of linear predictors (the covariate and the coefficient)\n","\n","Link function connects the mean of the response to Predictors\n","The First Equation describes the Random Component, here it is the Gaussian Distribution\n","\n","The second equation is the systematic component which has the covariates and the coefficients . This is the Linear Predictor\n","\n","The third equation links the random component to the Link Function\n","\n","The above set of equations are a generic representation of the Generalized Linear Model.\n","Types of Generalized Models\n","Logistic Regression used for predicting Binary Outcomes.\n","\n","Poisson Regression\n","\n","used for predicting count data (# of footfalls, # of hits on a website)\n","\n","#Binary Classification\n","You are going to check the certainty of your outcome in the range (0 to 1)\n","\n","In mathematical terms you are interested in knowing the probability P(y = 1) given a set of independent variables x1,x2,x3,...xk\n","\n","For this kind of a scenario Logistic Regression will come to your rescue.\n","\n","It is used for performing classification where the outcome is dichotomous [(yes/no) or (1/0) or (True/False)]\n","\n","#Understanding the Logistic Function\n","Positive values of the coefficients predict class 1\n","\n","Positive values coefficients increase linear regression piece there by increasing the probability of y = 1\n","\n","Negative values of the coefficients predict class 0\n","\n","Negative values coefficients decrease linear regression piece there by decreasing the probability of y = 1\n","Regression Coefficients\n","The coefficients β0 , β1 and β2 are selected in such a way that\n","\n","Predict high probability for a given case\n","\n","Predict low probability for the opposite case\n","\n","#Odds Ratio\n","Odds = p(y=1) / p(y=0)\n","\n","Odds > 1 if y = 1 is more likely\n","\n","Odds < 1 if y = 0 is more likely\n","\n","Odds = 1 if outcome is equally likely\n","Odds      = e^(β0 + β1x1 + ... βkxk) \n","\n","log(Odds) = β0 + β1x1 + ... βkxk\n","\n","\n","\n","This is called logit and looks like linear regression.\n","Bigger the logit bigger the P(y=1)\n","\n","+ve beta values increase logit increasing the odds\n","\n","-ve beta values decrease logit decreasing the odds\n","\n","#Baseline Method\n","For Logistic Regression / Binary Classification the baseline method is to predict the most frequent outcome.\n","\n","The output is a probability value. To separate the 1 and 0 you have to identify a threshold value.\n","\n","Values above the threshold will be marked 1 and below will be marked 0.\n","\n","Choosing the right threshold value is important.\n","\n","ROC stands for Receiver Operator Characteristics\n","\n","It is a graphical way of identifying how the model has been fit\n","\n","The true positive rate represents the vertical axis\n","\n","The false positive rate represents the horizontal axis\n","ROC Curver captures all the threshold values\n","\n","It also helps to ...\n","\n","Choose the best threshold for the best trade off\n","\n","Get the Cost of failing to detect the false positives\n","\n","Get the Cost of raising the false alarm"]},{"cell_type":"markdown","metadata":{"id":"QFQ3VzfQ2Rhx","colab_type":"text"},"source":["#Data and Code\n","The code below is a simple demonstration of how GLMs are implemented in Python\n","\n","A dataset is created with scores a team got and Won or lost that respective game\n","\n","This is to illustrate how the score is helping us predict the binary outcome win/loose\n","```\n","import pandas as pd\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","from __future__ import print_function\n","Scores = [(200,1),(100,0),(150,1),(320,1),(270,1),(134,0),(322,1),(140,0),(210,0),(199,0)]\n","Labels = ['Score','Win']\n","df = pd.DataFrame.from_records(Scores, columns=Labels)\n","glm_binom = sm.GLM(df.Win, df.Score, family=sm.families.Binomial())\n","res = glm_binom.fit()\n","print(res.summary())\n","```\n","\n","Sample Output\n","\n","The value of the score coef tells us how it is able to tell us to what extent it is able to predict the likilihood of winning a game .\n","\n","The rest of the values are a standard outcome of a regression equation.\n","\n","```\n","from sklearn.datasets import make_classification\n","X, y = make_classification(n_samples=100, n_features=2,\n","                           n_informative=2, n_redundant=0,\n","                           n_clusters_per_class=1,\n","                           class_sep = 2.0, random_state=101)\n","\n","array([[-1.04910781,  2.85665467],\n","       [-0.95081668, -2.92962994],\n","       [-3.40154264, -1.14982839],\n","       [-1.53659935, -2.28580115],\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n","            linewidth=0, edgecolor=None)\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n","\n","#Splitting the Data\n","from sklearn.cross_validation import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y.astype(float),test_size=0.33, random_state=101)\n","\n","\n","#Logistic Regression Model Fit\n","from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression()\n","clf.fit(X_train, y_train.astype(int))\n","y_clf = clf.predict(X_test)\n","print(classification_report(y_test, y_clf))\n","\n","Results of Model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","clf = LogisticRegression()\n","clf.fit(X_train, y_train.astype(int))\n","y_clf = clf.predict(X_test)\n","print(classification_report(y_test, y_clf))\n","precision    recall  f1-score   support\n","        0.0       1.00      0.93      0.97        15\n","        1.0       0.95      1.00      0.97        18\n","avg / total       0.97      0.97      0.97        33\n","\n","\n","\n","#ROC Curve\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","# Compute fpr, tpr, thresholds and roc auc\n","fpr, tpr, thresholds = roc_curve(y_test, y_clf)\n","#roc_auc = auc(y_test, y_clf)\n","# Plot ROC curve\n","#plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.0])\n","plt.xlabel('False Positive Rate or (1 - Specifity)')\n","plt.ylabel('True Positive Rate or (Sensitivity)')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_test, y_clf)\n","\n","\n","array([[14,  1],\n","\n","       [ 0, 18]])\n","\n","```\n","\n","Sensitivity and Specificity\n","Sensitivity for the model is 100%\n","\n","Specificity for the model is 94%\n","\n","Based on the numbers we can interpret that the model is able to clearly separate the data into 2 classes.\n","\n","The model is also able to designate the individual numbers that do not belong to a specific class as negative.\n","\n"]},{"cell_type":"code","metadata":{"id":"_JCvZW340W3_","colab_type":"code","outputId":"d0474964-b8ea-4868-fd77-aeaf282b9342","executionInfo":{"status":"ok","timestamp":1570632888948,"user_tz":-330,"elapsed":1126,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"source":["from sklearn import datasets\n","iris = datasets.load_iris()\n","iris_X = iris.data\n","iris_y = iris.target\n","#print(iris.feature_names)\n","#print(iris.target_names)\n","\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(iris_X, iris_y,test_size=0.33, random_state=101)\n","###Start code\n","from sklearn.linear_model import LogisticRegression\n","model = LogisticRegression()\n","model.fit(X_train, y_train) \n","#fit the model\n","###End code(approx 3 lines)\n","\n","y_pred = model.predict(X_test)\n","\n","###Start code here\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import recall_score as rec\n","from sklearn.metrics import precision_score  as pe\n","rep = classification_report(y_test,y_pred, output_dict=True)\n","print(classification_report(y_test, y_pred))\n","###End code(approx 2 to 3 lines)\n","\n","\n","\n","from sklearn.metrics import precision_recall_fscore_support as score\n","precision,recall,fscore,support=score(y_test,y_pred,average='macro')\n","print(precision)\n","print(recall)\n","\n","### WEIGHTED AVERAGE IS WHAT WE NEED TO PASS THE TEST"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        15\n","           1       1.00      0.91      0.95        22\n","           2       0.87      1.00      0.93        13\n","\n","    accuracy                           0.96        50\n","   macro avg       0.96      0.97      0.96        50\n","weighted avg       0.97      0.96      0.96        50\n","\n","0.9555555555555556\n","0.9696969696969697\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"WJGT-NsfaVOo","colab_type":"text"},"source":["One of the underlying assumptions of Linear Regression is that the error terms follow a normal distribution\n","\n","When the error terms do not follow normal distribution , we go for other types of Regression\n","\n","When we try to model count data(number of footfalls, traffic in a website), we go for Poisson Regression\n","\n","Poisson Regression is used to model count data\n","\n","Number of foot falls , number of call drops etc ...\n","\n","In mathematical terms , Poisson Regression is used to model the logarithm of the count data\n","Dependent Variable Y represents count or sometimes Y/t is used signifying the rate\n","\n","Independent variables are categorical or continuous variables depending on the dataset\n","\n","Link Function : g(μ)=β0+β1x1+β2x2+…+βkxk = xTiβ\n","\n","Random component: Response Y has a Poisson distribution that is yi∼Poisson(μi) for i=1,...,N where the expected count of yi is E(Y)=μ.\n","\n","Systematic component: Any set of X = (X1, X2, … Xk) are independent variables.\n","#Link Function\n","Identity link: μ=β0+β1x1\n","In some ocassions the identity link function is used in Poisson regression. Here the random component is the Poisson distribution.\n","\n","Natural log link: log(μ)=β0+β1x1\n","The Poisson regression model for counts is occassionally referred to as a “Poisson loglinear model”.\n","\n","For simplicity, with a single dependent variable, we can write: log(μ)=α+βx. This is equivalent to:μ=exp(α+βx)=exp(α)exp(βx)\n","\n","Interpreting Parameters\n","Interpreting the estimated parameter.\n","\n","exp(α) = effect on the mean of Y, that is mean, when X = 0\n","\n","exp(β) = with every unit increase in X, the predictor variable has **multiplicative effect **of exp(β) on the mean of Y, that is μ\n","\n","If β = 0, then exp(β) = 1, and the expected count, μ = E(y) = exp(α), and Y and X are not related.\n","\n","If β > 0, then exp(β) > 1, and the expected count μ = E(y) is exp(β) times larger than when X = 0\n","\n","If β < 0, then exp(β) < 1, and the expected count μ = E(y) is exp(β) times smaller than when X = 0\n","\n","The set of equations mentioned in the above cards can also be applicable for rate data. Y/t\n","\n","Y is the count data and t is the time"]},{"cell_type":"code","metadata":{"id":"2bGEzz5F4MsH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"266a6d78-be74-40fe-fd57-a409a0992144","executionInfo":{"status":"ok","timestamp":1570790936130,"user_tz":-330,"elapsed":4016,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["import numpy as np\n","\n","import pandas as pd\n","\n","import statsmodels.api as sm\n","dataset = pd.DataFrame({'A':np.random.rand(100)*1000, \n","\n","                        'B':np.random.rand(100)*100,  \n","\n","                        'C':np.random.rand(100)*10, \n","\n","                        'target':np.random.randint(0, 5, 100)})\n","\n","#Split the Variables\n","\n","X = dataset[['A','B','C']]\n","\n","X['constant'] = 1\n","\n","y = dataset['target']\n","\n","size = 1e5\n","\n","nbeta = 3\n","\n","#Model Fitting\n","\n","fam = sm.families.Poisson()\n","\n","pois_glm = sm.GLM(y,X, family=fam)\n","\n","pois_res = pois_glm.fit()\n","\n","pois_res.summary()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<table class=\"simpletable\">\n","<caption>Generalized Linear Model Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>  <td>   100</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>    96</td> \n","</tr>\n","<tr>\n","  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     3</td> \n","</tr>\n","<tr>\n","  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -166.64</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>            <td>Fri, 11 Oct 2019</td> <th>  Deviance:          </th> <td>  141.06</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                <td>10:48:55</td>     <th>  Pearson chi2:      </th>  <td>  112.</td> \n","</tr>\n","<tr>\n","  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>A</th>        <td>    0.0004</td> <td>    0.000</td> <td>    1.304</td> <td> 0.192</td> <td>   -0.000</td> <td>    0.001</td>\n","</tr>\n","<tr>\n","  <th>B</th>        <td>   -0.0058</td> <td>    0.003</td> <td>   -2.139</td> <td> 0.032</td> <td>   -0.011</td> <td>   -0.000</td>\n","</tr>\n","<tr>\n","  <th>C</th>        <td>   -0.0374</td> <td>    0.026</td> <td>   -1.454</td> <td> 0.146</td> <td>   -0.088</td> <td>    0.013</td>\n","</tr>\n","<tr>\n","  <th>constant</th> <td>    0.7727</td> <td>    0.270</td> <td>    2.864</td> <td> 0.004</td> <td>    0.244</td> <td>    1.301</td>\n","</tr>\n","</table>"],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                 Generalized Linear Model Regression Results                  \n","==============================================================================\n","Dep. Variable:                 target   No. Observations:                  100\n","Model:                            GLM   Df Residuals:                       96\n","Model Family:                 Poisson   Df Model:                            3\n","Link Function:                    log   Scale:                          1.0000\n","Method:                          IRLS   Log-Likelihood:                -166.64\n","Date:                Fri, 11 Oct 2019   Deviance:                       141.06\n","Time:                        10:48:55   Pearson chi2:                     112.\n","No. Iterations:                     5                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","A              0.0004      0.000      1.304      0.192      -0.000       0.001\n","B             -0.0058      0.003     -2.139      0.032      -0.011      -0.000\n","C             -0.0374      0.026     -1.454      0.146      -0.088       0.013\n","constant       0.7727      0.270      2.864      0.004       0.244       1.301\n","==============================================================================\n","\"\"\""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"8k58W6hmbwpj","colab_type":"text"},"source":["On viewing the results and the coefficient values, we can say to what extent each coef is explaining the log of count data i.e the dependent variable.\n","\n","The rest of the values are what a Regression Output shows.\n","\n","\n","#Bayesian Vs Linear Regression\n","Bayesian Regression is similar to Linear Regression in many ways\n","\n","In Linear Regression the output is number / value\n","\n","In Bayesian the output is also a value but it also returns the entire probability distribution\n","\n","How is the Probability Distribution constructed?\n","\n","Here, the predicted value is returned and the variance value is also returned.\n","\n","With value as the mean and the variance value as the standard deviation the probability distribution can be constructed\n","```\n","Bayesian Regression in Python\n","regr = linear_model.BayesianRidge()\n","regr.fit(X, y)\n","Out:\n","BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, \n","              copy_X=True, fit_intercept=True, lambda_1=1e-06, \n","              lambda_2=1e-06, n_iter=300, normalize=False, \n","              tol=0.001, verbose=False)\n","\n","```\n","Pros and Cons of BR\n","Pro\n","\n","It is Robust to Gaussian Noise\n","\n","Works well if the number of features and observations in the dataset are comparable\n","\n","Cons\n","\n","It is really time-consuming\n","#CART Algorithm\n","Classification and Regression Trees are a set of non-linear learning algorithms which can be used for numerical as well as categorical features\n","\n","Here the tree has a set of nodes that split the branch into children\n","\n","In turn each of the branches can go into another node or just stay as a leaf along with the forecasted value or the predicted class\n","Why Trees ?\n","Performing the prediction task is quick\n","\n","The principal task is traversal along the the tree from the root node to the leaf nodes and at each point check if the respective feature is above or below the threshold\n","\n","The concept of variance reduction is used in this algorithm\n","\n","In each of the given nodes a search is performed along all the features across all levels in that feature\n","\n","The combination that contains the best variance is marked and selected as the best\n","```\n","Regression Trees with Python\n","In:\n","from sklearn.tree import DecisionTreeRegressor\n","regr = DecisionTreeRegressor(random_state=101)\n","regr.fit(X_train, y_train)\n","```\n","mean_absolute_error(y_test, regr.predict(X_test))\n","The syntax is similar to applying any regression model using scikit learn.\n","\n","#Pros and Cons of Trees\n","Pro ...\n","\n","Trees are the go to algorithms for modeling non-linear behavior\n","\n","They can be used for both categorical and numeric datatypes without performing any kind of normalization\n","\n","The training time , Prediction time are fast\n","\n","They leave a very small memory fingerprint\n","\n","Cons\n","\n","It belongs to a class of Greedy Algorithms , does not optimize the entire solution , it just optimizes specific choices\n","\n","If there are significant number of features, it does not perform well\n","\n","The leaf nodes can be very specific sometime leading to overfitting. In that case those nodes can be pruned.\n","\n","#Bagging and Boosting\n","Bagging and Boosting are techniques that are used for combining multiple models to improve overall accuracy.\n","\n","The final combination is a non linear model containing a set of linear models.\n","\n","Bootstrap Aggregation is abbreviated as Bagging.\n","\n","The main objective of this technique is to reduce the overall variance by aggregating the models.\n","\n","How is Bagging Done ?\n","Each model is trained on the on the selected set of features with replacement\n","\n","At the end of training , during prediction , each of the models perform their respective prediction , the results are all taken , averaged and then the ensemble prediction is performed.\n","\n","Bagging Tip\n","\n","The training and the prediction happens at individual model level. This gives flexibility to parallelize the operation on multiple CPUs.\n","\n","#Bagging in Python\n","\n","```\n","from sklearn.ensemble import BaggingRegressor\n","bagging = BaggingRegressor(SGDRegressor(), n_jobs=-1,\n","                           n_estimators=1000, random_state=101,\n","                           max_features=0.8)\n","bagging.fit(X_train, y_train)\n","mean_absolute_error(y_test, bagging.predict(X_test))\n","from sklearn.ensemble import RandomForestRegressor\n","regr = RandomForestRegressor(n_estimators=100, \n","                             n_jobs=-1, random_state=101)\n","regr.fit(X_train, y_train)\n","mean_absolute_error(y_test, regr.predict(X_test))\n","```\n","\n","Boosting\n","Boosting is another way of combining multiple learning models\n","\n","The objective of boosting is to reduce the prediction bias\n","\n","In boosting the models are in a sequence , cascaded with each other , the output of one is the input of another\n","\n","Boosting Algorithm\n","During training , the output of one model is predicted\n","\n","The error is calculated based on the actual value\n","\n","This error is multiplied with the learning rate\n","\n","New model is trained on that error set and inserted at final stage of the cascaded and trained models\n","\n","The output value from one stage is the value predicted combined with the learning rate times by the output prediction from the current stage\n","\n","#Boosting Sample Code\n","```\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","regr = GradientBoostingRegressor(n_estimators=500, \n","\n","                                 learning_rate=0.01, \n","\n","                                 random_state=101)\n","\n","regr.fit(X_train, y_train)\n","```\n","\n","Pros and Cons\n","Pros\n","\n","We can build very good and robust models combining weak models\n","\n","They support stochastic learning\n","\n","The robustness in the solution is created by the stochastic or random nature of the model\n","\n","Cons\n","\n","Time taken for training is very high . There is a high memory footprint\n","\n","The steps in model building can be tricky because of the stochastic nature\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sTlw9Zw7eh7D","colab_type":"text"},"source":["#Application\n","In this course so far you have seen different types of regression models , now you will learn in what kind scenarios are all these models applied.\n","\n","Some of the application areas include\n","\n","Prediction Problems\n","\n","Binary and Multi Class Classification\n","\n","Time Series Analysis\n","\n","Ranking Problems\n","\n","Consider a dataset from the Music Industry.\n","\n","The descriptors of a particular song are given and the year the song was produced is given.\n","\n","Can this data be modeled as a Regression Problem to predict the year given the descriptors ?\n","\n","#Problem Approach\n","For the question raised in the previous card, the answer is yes we can predict the year of production based on the descriptors .\n","\n","The features should be identified based on the relevance to the context\n","\n","Once the features are extracted a model can be trained with Features as inputs and year of production as output\n","\n","The model can be evaluated using Mean Absolute Error between actual and predicted values\n","\n","The ultimate objective would be to minimize the error\n","\n","#Classification Problem\n","The previous problem can also be modeled as a Multi Class Classification problem.\n","\n","The features and the descriptors still remain the same.\n","\n","The output will belong to one of the classes from the range of years provided.\n","\n","Mean absolute error can be used for validating the accuracy of the prediction.\n","\n","#Ranking Problem\n","Consider a dataset with some features related to a car along with a price.\n","\n","Insurance companies would want to assess if the car is riskier or not to sell / buy on a given scale.\n","\n","How do you think you will design this problem ?\n","\n","Ranking Problem Approach\n","The above problem can be modeled as a regression problem where we are predicting the risk on a scale .\n","\n","The methodology to asses the prediction will be different .\n","\n","In this scenario , we can go for label ranking loss , a metric that indicates the strength ranking\n","\n","Mean Absolute and Mean Standard Errors are not applicable in this scenario.\n","\n","Another way to measure the prediction accuracy is by Label Ranking Average Precision.\n","\n","\n","#Time Series Problem\n","So far you have seen problems where the features and the target variables are different.\n","\n","In scenario where you try to analysis stock prices or currency fluctuations or Support ticket trend over a period of time the variables themselves can be the features and the targets .\n","\n","These problems fall under Time Series Analysis.\n","\n","In time series analysis , the data at time t+k can be the target and data at time t can be the feature. The concept of auto regression is applied in these scenarios."]}]}
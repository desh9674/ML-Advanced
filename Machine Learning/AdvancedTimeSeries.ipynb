{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AdvancedTimeSeries.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Zyrwcb_-Xy_q","colab_type":"text"},"source":["#What will you learn ?\n","You will learn about\n","\n","Auto Correlation and Partial auto correlation - help you in choosing the right model for forecasting.\n","\n","Modeling Using Auto Regressing - understand how you can build this model and use it for Forecasting.\n","\n","Moving average Smoothing - Your time series data has a lot of noise. You will learn how to smoothen the noise and use the data for developing models.\n","\n","ARMA and ARIMA - very powerful models for Time Series Forecasting\n","\n","Exponential Smoothing - another important concept you will learn.\n","\n","Finally, you will also learn about multivariate Time Series Modeling.\n","\n","For the practical aspects, you will go through code snippets in Python.\n","\n","You will also get an opportunity to try a few hands-on exercises for familiarizing with model building and forecasting.\n","\n","#Auto Correlation Function\n","Auto Correlation is a very important step in Time Series Model Building\n","\n","In this step , the data is correlated with its lag values to see how well the current value is related to the previous values\n","\n","Auto Correlation Function is used for quantifying this relationship\n","\n","In Time Series, the same data point is observed at different time intervals, hence the correlation of the current value with the previous values is known as ***Auto Correlation or Serial Correlation***.\n","\n","#ACF Explanation\n","ACF Explanation\n","Consider the above time series\n","\n","You can observe the data point at different time intervals\n","\n","If you take a lag of 1 time step and correlate the lag values with the actual values , you get the auto correlation of lag 1\n","\n","This process can be repeated for multiple lags\n","\n","#ACF Plot\n","ACF Plot\n","The ACF plot is a very useful graph while performing Time Series Modeling\n","\n","It tell the correlation value for each lag\n","\n","An ideal ACF plot will decay exponentially\n","\n","Depending on the ACF plot value , you can decide to stop at any given lag\n","\n","#Plotting ACF using Python\n","Plotting ACF using Python\n","Getting the Time Series Data\n","\n","import pandas_datareader as pdr\n","appleData = pdr.get_data_yahoo('AAPL')\n","The above data set has the stock prices of APPLE Inc.\n","\n","It is a Series of Open, High, Low , Close, Adj stock prices for a given data.\n","\n","Plotting the ACF Values for various lags\n","```\n","from matplotlib import pyplot\n","from statsmodels.graphics.tsaplots import plot_acf\n","plot_acf(appleData.Close,lags=100)\n","pyplot.show()\n","To plot the PACF we are considering only the Close column in the dataset.\n","```\n","The ACF Plot has lag number in the x axis and the correlation value for the specific lag in y axis.\n","#ACF Properties\n","ACF value lies between - 1 and + 1\n","\n","ACF is unitless\n","\n","It helps the analyst in getting how each lag value is correlated\n","Fitting ACF is the first step during Model Building\n","\n","Depending on the pattern you can decide what should be the order of the Auto Regression process\n","\n","#PACF\n","Partial Auto Correlation is another important step in the Time Series Modeling Process\n","\n","The partial auto correlation at any given lag k is the correlation obtained after cancelling the effect of correlations due to terms at shorter lags.\n","\n","In simple terms , in partial auto correlation , the effects due to intermediate terms are nullified to determine the correlation\n","\n","#Determining the PAC value\n","If you have a time series that is represented by [yt , yt-1 , yt-2 , .... yt-s]\n","\n","If you want to determine the Partial Auto Correlation between yt and yt-s then you have to nullify the effect of all intermediate terms to get the PACF value\n","\n","#Significance of PACF\n","Significance of PACF\n","The PACF values when plotted can help in determining the order of the Auto Regressive Process.\n","\n","<PACF Picture>\n","\n","The number of significant correlations will determine the order of the order of the AR process.\n","\n","The picture above shows a sample PACF plot.\n","\n","Top Left Plot has one significant correlation meaning the order of the AR process is 1.\n","\n","Top Right Plot has two and so on ..\n","\n","PACF using Python\n","Getting the Time Series Data\n","```\n","import pandas_datareader as pdr\n","appleData = pdr.get_data_yahoo('AAPL')\n","Plotting the PACF Values for various lags\n","\n","from matplotlib import pyplot\n","from statsmodels.graphics.tsaplots import plot_pacf\n","plot_pacf(appleData.Close,lags=100)\n","pyplot.show()\n","```\n","\n","The PACF plot has lag on X axis and correlation value on Y axis.\n","\n","Reading the PACF Plot we will understand the number of significant lags to determine the order of AR Process.\n","\n","From the plot in the previous card , it is evident that the order of AR will be 2 for the time series.\n","\n","nstalling Statsmodels Package\n","The following exercise will require you to install ***statsmodels package***. Installation instructions below.\n","\n","In your playground, you have to do the following installation to try out your code.\n","\n","Once you land in the online playground, go the terminal and enter the following command pip install --user statsmodels.\n","\n","If your installation is successful, you should get the following message\n","\n","\n","Installing collected packages: patsy, statsmodels\n","\n","Successfully installed patsy-0.4.1 statsmodels-0.8.0\n"]},{"cell_type":"code","metadata":{"id":"IrwuRH2gXkre","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2H7CLmh5ZClv","colab_type":"text"},"source":["##Auto Regression\n","Auto Regression\n","Regression is the process of predicting one variable from another .\n","\n","In Time Series , the data points collected are from the same observation.\n","\n","Here the past observations are used for predicting the future values.\n","\n","This process of regressing the past values to get the future values is known as Auto Regression.\n","\n","#Auto Regression Equation\n","Auto Regression Equation\n","The outcome of the model fitting will be determining the coef values.\n","\n","Once the coef values are determined they can be used to predict the future values based on the current and past data.\n","\n","#Assumptions for Model Fitting\n","The lag values should help in predicting the current and the future values\n","\n","Stronger the correlation between the current and the past values, more robust the predictions from Auto Regression\n","\n","#Getting the Apple stock prices\n","```\n","import pandas_datareader as pdr\n","appleData = pdr.get_data_yahoo('AAPL')\n","Viewing the Time Series\n","\n","import matplotlib.pyplot as plt\n","appleData.Close.plot()\n","plt.show()\n","\n","\n","#splitting\n","from statsmodels.tsa.ar_model import AR\n","\n","from sklearn.metrics import mean_squared_error\n","\n","\n","X = appleData.Close.values\n","\n","\n","\n","train, test = X[1:len(X)-10], X[len(X)-10:]\n","\n","\n","#Viewing the correlogram with 250 lags\n","\n","\n","from statsmodels.graphics.tsaplots import plot_acf\n","\n","plot_acf(appleData.Close,lags = 250)\n","\n","plt.show()\n","\n","\n","#Model Fitting\n","\n","model = AR(train)\n","\n","model_fit = model.fit()\n","\n","print('Lag: %s' % model_fit.k_ar)\n","\n","print('Coefficients: %s' % model_fit.params)\n","\n","#We are fitting the model and and viewing the coefficients.\n","\n","#Coef Values\n","\n","\n","Lag: 25\n","\n","Coefficients: [ 0.11762701  1.01419227 -0.03097132 -0.01778476  0.04704682 -0.0164039\n","\n","  0.00131838  0.0240413  -0.05058969  0.05770135 -0.00680828 -0.06627654\n","\n","  0.03677982  0.03879211  0.00913336 -0.05083633  0.00953855  0.02372498\n","\n"," -0.03278349  0.02695213  0.03216126 -0.04961039 -0.05640927  0.04916215\n","\n","  0.03326709 -0.02598511]\n","\n","\n","\n","\n","\n","#Forecasting with AR\n","#Forecasting with AR\n","\n","predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n","\n","for i in range(len(predictions)):\n","\n","    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n","\n","error = mean_squared_error(test, predictions)\n","\n","print('Test MSE: %.3f' % error)\n","\n","# plot results\n","\n","plt.plot(test)\n","\n","plt.plot(predictions, color='red')\n","\n","plt.show()\n","\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"BRWibnvtWAyK","colab_type":"text"},"source":["#Moving Average Smoothing\n","Moving Average Smoothing\n","Moving Average is a smoothing technique for reducing the noise in the Time Series Data\n","\n","It is used for exposing the signal that is in-between the noise\n","\n","The values are averaged at different time windows\n","#Centered Moving Average\n","Centered Moving Average\n","In Centered Moving average , the average is calculated for time steps before and after the given time step\n","\n","cma(t) = average(obs(t-1), obs(t), obs(t+1))\n","#Trailing Moving Average\n","Trailing Moving Average uses historical observations for determining the future observation.\n","```\n","trail_moving_average(t) = average(obs(t-2), obs(t-1), obs(t))\n","\n","\n","Moving Average with Python\n","import pandas_datareader as pdr\n","appleData = pdr.get_data_yahoo('AAPL')\n","from pandas import Series\n","seriesData = Series(appleData.Close.values)\n","rolling = seriesData.rolling(window=3)\n","rolling_mean = rolling.mean()\n","rolling_mean.head(10)\n","In the above code , we are keeping the window size of 3 and moving across the time series to calculate the arithmetic mean.\n","\n","Results below. The first two observations are Nan.\n","\n","0          NaN\n","1          NaN\n","2    30.434285\n","3    30.445714\n","4    30.282380\n","5    30.168095\n","6    30.127143\n","7    29.990953\n","8    29.927619\n","9    29.895238\n","dtype: float64\n","```\n","\n","#Visualizing Moving Average Results\n","\n","```\n","seriesData.head(100).plot()\n","\n","rolling_mean.head(100).plot(color='red')\n","\n","plt.show()\n","```\n","The Actual data is in Blue and the rolling mean is in Red .\n","\n","You can see how the data is smoother now .\n","\n","\n","#Moving Average for Prediction\n","Moving Average for Prediction\n","Moving Average can be used for prediction\n","\n","It is not a very effective way to predict but can still be used to get a naive estimate\n","\n","The assumption we make is that the trend and seasonal components are nullified\n","\n","This can be used in a Walk Forward Manner\n","\n","Prediction using Python\n","```\n","from numpy import mean\n","\n","\n","\n","X = appleData.Close.values\n","\n","window = 3\n","\n","history = [X[i] for i in range(window)]\n","\n","test = [X[i] for i in range(window, len(X))]\n","\n","predictions = list()\n","\n","# walk forward over time steps in test\n","\n","for t in range(len(test)):\n","\n","\tlength = len(history)\n","\n","\tyhat = mean([history[i] for i in range(length-window,length)])\n","\n","\tobs = test[t]\n","\n","\tpredictions.append(yhat)\n","\n","\thistory.append(obs)\n","\n","#\tprint('predicted=%f, expected=%f' % (yhat, obs))\n","\n","error = mean_squared_error(test, predictions)\n","\n","print('Test MSE: %.3f' % error)\n","\n","\n","# plot\n","\n","plt.plot(test)\n","\n","plt.plot(predictions, color='red')\n","\n","plt.show()\n","\n","# zoom plot\n","\n","plt.plot(test[0:100])\n","\n","plt.plot(predictions[0:100], color='red')\n","\n","plt.show()\n","\n","```\n","In the above code , we are forecasting using Moving Average process. We are traversing along the time series to create the forecast values.\n","\n","Finally we are getting the Mean Square Error.\n","\n","9 of 12"]},{"cell_type":"code","metadata":{"id":"zsDAsDa3ZsZ2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"07a7b3cb-e56a-459a-9767-277d09333ece","executionInfo":{"status":"ok","timestamp":1570608692486,"user_tz":-330,"elapsed":1316,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["import pandas as pd\n","from pandas import Series\n","timeSeries = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,\n","          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,\n","          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,\n","          18,8,17,21,31,34,44,38,31,30,26,32]\n","ts = Series(timeSeries)\n","from matplotlib import pyplot \n","from statsmodels.tsa.stattools import acf\n","from statsmodels.tsa.stattools import pacf\n","\n","acf_corr = acf(ts,nlags=5, unbiased=True,)\n","###End code(approx 2 lines)\n","print(acf_corr)\n","\n","from statsmodels.graphics.tsaplots import pacf\n","\n","pacf_corr = pacf(ts,nlags=5)\n","###End code(approx 2 lines)\n","print(pacf_corr)\n","\n","acf_lag_1 = acf_corr[0]   # acf at lag 0 == 1\n","acf_lag_2 = acf_corr[1]  # acf at lag 1\n","pacf_lag_3 = pacf_corr[2] # acf at lag 3\n","print(acf_lag_1,pacf_lag_3)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[ 1.          0.72120588  0.42688471  0.09746397 -0.18409287 -0.31332648]\n","[ 1.          0.72120588 -0.19433338 -0.28172395 -0.18857053  0.02711725]\n","1.0 -0.19433337567125356\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/stattools.py:541: FutureWarning: fft=True will become the default in a future version of statsmodels. To suppress this warning, explicitly set fft=False.\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s9LiCElng1cn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"e54b1c9c-aa62-4b06-b9f9-3cb1681663a1","executionInfo":{"status":"ok","timestamp":1570609142580,"user_tz":-330,"elapsed":1320,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["import pandas as pd\n","from pandas import Series\n","timeSeries = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,\n","          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,\n","          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,\n","          18,8,17,21,31,34,44,38,31,30,26,32]\n","train, test = timeSeries[1:len(timeSeries)-10], timeSeries[len(timeSeries)-10:]\n","\n","###Start code here\n","from statsmodels.tsa.ar_model import AR\n","model = AR(train)\n","model_fit = model.fit()\n","###End code(approx 3 lines)\n","print('Lag: %s' % model_fit.k_ar)\n","print('Coefficients: %s' % model_fit.params)\n","\n","from sklearn.metrics import mean_squared_error\n","###Start code here\n","predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n","###End code\n","for i in range(len(predictions)):\n","    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n","error = mean_squared_error(test, predictions)\n","print('Test MSE: %.3f' % error)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Lag: 11\n","Coefficients: [17.61119882  0.60959022 -0.46145298  0.34147052 -0.5535399   0.50364582\n"," -0.57076599  0.40686544 -0.48981335  0.36377344 -0.51010253  0.64166238]\n","predicted=13.925993, expected=17.000000\n","predicted=20.279285, expected=21.000000\n","predicted=26.556221, expected=31.000000\n","predicted=34.709756, expected=34.000000\n","predicted=39.324673, expected=44.000000\n","predicted=28.433625, expected=38.000000\n","predicted=27.219552, expected=31.000000\n","predicted=25.737600, expected=30.000000\n","predicted=23.488579, expected=26.000000\n","predicted=27.009755, expected=32.000000\n","Test MSE: 20.726\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dHiM_N1sms99","colab_type":"text"},"source":["#Moving Average Smoothing\n","Moving Average Smoothing\n","Moving Average is a smoothing technique for reducing the noise in the Time Series Data\n","\n","It is used for exposing the signal that is in-between the noise\n","\n","The values are averaged at different time windows\n","\n","Centered Moving Average\n","Centered Moving Average\n","In Centered Moving average , the average is calculated for time steps before and after the given time step\n","\n","cma(t) = average(obs(t-1), obs(t), obs(t+1))\n","\n","Centered Moving Average Properties\n","This method requires the know how of the future values to calculate the average.\n","\n","This technique is used for detrending and removing any seasonal effects on Time Series Data.\n","\n","This is not very useful for forecasting.\n","\n","#Trailing Moving Average\n","Trailing Moving Average uses historical observations for determining the future observation.\n","\n","trail_moving_average(t) = average(obs(t-2), obs(t-1), obs(t))\n","\n","Moving Average with Python\n","```\n","import pandas_datareader as pdr\n","appleData = pdr.get_data_yahoo('AAPL')\n","from pandas import Series\n","seriesData = Series(appleData.Close.values)\n","rolling = seriesData.rolling(window=3)\n","rolling_mean = rolling.mean()\n","rolling_mean.head(10)\n","In the above code , we are keeping the window size of 3 and moving across the time series to calculate the arithmetic mean.\n","\n","Results below. The first two observations are Nan.\n","\n","0          NaN\n","1          NaN\n","2    30.434285\n","3    30.445714\n","4    30.282380\n","5    30.168095\n","6    30.127143\n","7    29.990953\n","8    29.927619\n","9    29.895238\n","dtype: float64\n","seriesData.head(100).plot()\n","\n","rolling_mean.head(100).plot(color='red')\n","\n","plt.show()\n","\n","```\n","\n","Moving Average can be used for prediction\n","\n","It is not a very effective way to predict but can still be used to get a naive estimate\n","\n","The assumption we make is that the trend and seasonal components are nullified\n","\n","This can be used in a Walk Forward Manner\n","\n","#Prediction using Python\n","```\n","from numpy import mean\n","\n","\n","\n","X = appleData.Close.values\n","\n","window = 3\n","\n","history = [X[i] for i in range(window)]\n","\n","test = [X[i] for i in range(window, len(X))]\n","\n","predictions = list()\n","\n","# walk forward over time steps in test\n","\n","for t in range(len(test)):\n","\n","\tlength = len(history)\n","\n","\tyhat = mean([history[i] for i in range(length-window,length)])\n","\n","\tobs = test[t]\n","\n","\tpredictions.append(yhat)\n","\n","\thistory.append(obs)\n","\n","#\tprint('predicted=%f, expected=%f' % (yhat, obs))\n","\n","error = mean_squared_error(test, predictions)\n","\n","print('Test MSE: %.3f' % error)\n","\n","# plot\n","\n","plt.plot(test)\n","\n","plt.plot(predictions, color='red')\n","\n","plt.show()\n","\n","# zoom plot\n","\n","plt.plot(test[0:100])\n","\n","plt.plot(predictions[0:100], color='red')\n","\n","plt.show()\n","\n","```\n","In the above code , we are forecasting using Moving Average process. We are traversing along the time series to create the forecast values.\n","\n","Finally we are getting the Mean Square Error."]},{"cell_type":"markdown","metadata":{"id":"bAI_AeYptrcV","colab_type":"text"},"source":["#ARMA\n","ARMA\n","The above equation represents a typical ARMA process.\n","\n","Alpha represent all the Auto Regression Terms and Beta represents all the Moving Average Terms.\n","\n","ARMA stands for Auto Regressive Moving Average.\n","\n","It is a combination of AR and MA represented using the above equation.\n","\n","Statisticians Box and Jenkins suggested an approach for identifying , estimating and fitting time series models.\n","\n","The steps are\n","\n","Identification\n","\n","Estimation\n","\n","Diagnostic Checking\n","\n","In the following cards , you will learn these steps in detail.\n","\n","This is the First Step in the process\n","\n","Check if the Time series is Stationary or Not\n","\n","Get the parameters for ARMA model\n","\n","Stationarity Check\n","\n","Differencing\n","\n","Unit Root Method\n","\n","Configuring AR and MA\n","\n","ACF and PACF plot will help in getting the p and q values for the model\n","\n","Some Observable Patterns\n","\n","If the ACF is trailing after a particular lag value and shows a very hard cut-off in PACF after a specific lag value , the process is AR. Value of p is the lag value\n","\n","If the the PACF is trailing off after a specific lag value and is having a very hard cut-off in the ACF after a particular lag value , the model is MA . Value of q is the lag value.\n","\n","Estimation involves all the steps to minimize the loss from errors\n","\n","Diagnosting checking involves investigation of\n","\n","Over-fitting\n","\n","Residual Errors\n","\n","#Model Representation\n","The standard representation of an ARIMA model is ARIMA(p,d,q)\n","\n","p the number of lag observations also called as lag order\n","\n","d number of observations that are differenced, degree of difference\n","\n","q order of moving average or the size of the moving average window\n","\n","#ARIMA Process\n","Data is prepared by the degree of differencing to make it stationary\n","\n","A linear regression model is prepared based on the specified type and the number of terms\n","\n","One assumption is that the process that generated the observations is also an ARIMA process\n","\n","\n","```\n","import pandas_datareader as pdr\n","\n","appleData = pdr.get_data_yahoo('AAPL')\n","\n","appleCloseTs = appleData.Close\n","\n","Viewing the autocorrelation plot\n","\n","\n","autocorrelation_plot(appleCloseTs)\n","\n","plt.show()\n","\n","from pandas import DataFrame\n","\n","model = ARIMA(appleCloseTs, order=(5,1,0))\n","\n","model_fit = model.fit(disp=0)\n","\n","print(model_fit.summary())\n","\n","# plot residual errors\n","\n","residuals = DataFrame(model_fit.resid)\n","\n","residuals.plot()\n","\n","plt.show()\n","\n","residuals.plot(kind='kde')\n","\n","plt.show()\n","\n","print(residuals.describe())\n","\n","from statsmodels.tsa.arima_model import ARIMA\n","\n","from sklearn.metrics import mean_squared_error\n","\n"," \n","\n","X = appleCloseTs.values\n","\n","size = int(len(X) * 0.66)\n","\n","train, test = X[0:size], X[size:len(X)]\n","\n","history = [x for x in train]\n","\n","predictions = list()\n","\n","for t in range(len(test)):\n","\n","\tmodel = ARIMA(history, order=(5,1,0))\n","\n","\tmodel_fit = model.fit(disp=0)\n","\n","\toutput = model_fit.forecast()\n","\n","\tyhat = output[0]\n","\n","\tpredictions.append(yhat)\n","\n","\tobs = test[t]\n","\n","\thistory.append(obs)\n","\n","\t#print('predicted=%f, expected=%f' % (yhat, obs))\n","```\n","Here we are splitting the data into training and testing.\n","Fitting an ARIMA model and doing the forecasting.\n","We are fitting an ARIMA model with 5 lags , number of observations differenced is 1 and zero order for the moving window.\n","\n"]},{"cell_type":"code","metadata":{"id":"s_UH-Te8nVeo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from pandas import Series\n","timeSeries  = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,\n","          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,\n","          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,\n","          18,8,17,21,31,34,44,38,31,30,26,32]\n","ts = Series(timeSeries)\n","X = ts.values\n","\n","###Start code here\n","from statsmodels.tsa.arima_model import ARIMA                         #import ARIMA\n","X = X.astype('float64')\n","size = int(len(X) * 0.80)\n","train, test =  X[0:size], X[size:len(X)]\n","history = [x for x in train]\n","predictions = list()\n","for t in range(len(test)):\n","    model = ARIMA(history, order=(5,1,0))\n","    model_fit =  model.fit(disp=0)\n","    output = model_fit.forecast()\n","##End code\n","    yhat = output[0]\n","    predictions.append(yhat)\n","    obs = test[t]\n","    history.append(obs)\n","    print('predicted=%f, expected=%f' % (yhat, obs))\n","\n","from sklearn.metrics import mean_squared_error\n","error = mean_squared_error(test, predictions)\n","print(\"MSE = \", error)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mc5mlt_hzUU2","colab_type":"text"},"source":["#What is VAR ?\n","Vector Auto Regression is a multivariate generalization of a uni variate auto regressive time series model\n","\n","Multivariate linear time series models\n","\n","They are designed to capture the collective dynamics of multiple time series\n","\n","```\n","import numpy as np\n","\n","import pandas\n","\n","import statsmodels.api as sm\n","\n","from statsmodels.tsa.api import VAR, DynamicVAR\n","\n","mdata = sm.datasets.macrodata.load_pandas().data\n","\n","dates = mdata[['year', 'quarter']].astype(int).astype(str)\n","\n","quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n","\n","from statsmodels.tsa.base.datetools import dates_from_str\n","\n","quarterly = dates_from_str(quarterly)\n","\n","mdata = mdata[['realgdp','realcons','realinv']]\n","\n","mdata.index = pandas.DatetimeIndex(quarterly)\n","\n","data = np.log(mdata).diff().dropna()\n","\n","model = VAR(data)\n","\n","results = model.fit(2)\n","\n","results.summary()\n","results.plot()\n","\n","plt.show()\n","\n","\n","lag_order = results.k_ar\n","\n","results.forecast(data.values[-lag_order:], 5)\n","\n","\n","array([[ 0.00502587,  0.0053712 ,  0.0051154 ],\n","\n","       [ 0.00593683,  0.00784779, -0.00302473],\n","\n","       [ 0.00662889,  0.00764349,  0.00393308],\n","\n","       [ 0.00731516,  0.00797044,  0.00657495],\n","\n","       [ 0.00732726,  0.00808811,  0.00649793]])\n","\n","\n","results.plot_forecast(10)\n","\n","plt.show()\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"Qyfz2-gUzqap","colab_type":"text"},"source":["#Weighted Smoothing\n","In time series forecasting , while predicting the future value , all the past values may not equally contribute\n","\n","Value at lag 1 will contribute more than the remaining values , hence it is a good approach to assign weights that decay to each of the values that are a time step apart and use that to predict the future value\n","\n","#Weighted Smoothing Limitation\n","One limitation to the approach suggested in the previous card is if the sum of the weights are greater than 1 then the prediction accuracy might not be good.\n","\n","To overcome this limitation , we go for an approach where the sum of the weights is 1\n","\n","```\n","import matplotlib.pyplot as plt \n","\n","import pandas as pd\n","\n","from pandas import Series \n","\n","\n","\n","timeSeries = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,\n","\n","          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,\n","\n","          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,\n","\n","          18,8,17,21,31,34,44,38,31,30,26,32]\n","\n","ts = Series(timeSeries)\n","\n","\n","\n","The below python function returns the values of a time series after smoothing\n","\n","\n","def exp_smth(ts, alpha):\n","\n","    result = [ts[0]] # first value is same as series\n","\n","    for n in range(1, len(ts)):\n","\n","        result.append(alpha * ts[n] + (1 - alpha) * result[n-1])\n","\n","    return result\n","\n","\n","# >>> exp_smth(series,0.1)\n","\n","# >>> exp_smth(series,0.9)\n","```\n","Significance of Exponential Smoothing\n","Helps in smoothing all the noise from the time series\n","\n","For different alpha values , the smoothing will differ\n","\n","Higher values recreate the time series with some smoothing\n","\n","Forecasts only one value at a time\n","Extension\n","The process described can be extended to predict multiple values\n","\n","Depending on the trend of the time series either additive or multiplicative , the process of smoothing differs\n","\n","All these process are collectively called Holt-Winters Method"]},{"cell_type":"code","metadata":{"id":"p7d0uIReixED","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ClusteringFresco.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sRSI5062l2qp","colab_type":"text"},"source":["The Jaccard index is used to compare elements of two sets to identify which of the members are shared and not shared. The Jaccard Distance is a measure of how different the two given sets are.\n","\n","Jaccard Distance = 1-(Jaccard Index)\n","\n","Eucledian Distance is the shortest distance between the two given points in Eucledian Space.\n","Cosine distance of two given vectors u and v is the angular cosine between the given vectors.\n","Manhattan distance is calculated on a strictly horizontal or vertical path.\n","\n","#Hierarchical Clustering Explained\n","Hierarchical Clustering Explained\n","Begin by allotting each item to a cluster. If you are having N items, you are now having N clusters, where each of them contains one item. Now, let us make the similarities (distances) between the clusters the same as the similarities (distances) between the items they include.\n","Discover the most identical or closest pair of clusters, merge them into one cluster, thereby reducing one cluster.\n","Calculate the similarities (distances) between each of the old clusters and the new cluster.\n","Repeat step 2 and step 3 until all items are finally clustered into one cluster with size N.\n","Source: https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html\n","\n","#Dendogram\n","Dendogram\n","A dendrogram is a branching diagram that represents the relationships of similarity among a group of entities\n","Each branch is called a clade\n","The terminal end of each clade is called a leaf\n","There is no limit to the number of leaves in a clade\n","The arrangement of the clades tells us which leaves are most similar to each other\n","The height of the branch points indicates how similar or different they are from each other\n","The greater the height, the greater the difference between the points\n","\n","Disadvantages of Agglomerative Clustering\n","Disadvantages for agglomerative hierarchical clustering\n","\n","If data points are wrongly grouped at the inception, they cannot be reallocated.\n","If different similarity measures are utilized to calculate the similarity between clusters, it may result in different results altogether.\n","\n","Tips for Hierarchical Clustering\n","There is no particular size that fits all solutions to determine how many clusters you need. It depends on what you intend to do with them. For a better solution, look at the basic characteristics of the given clusters at successive steps and make a decision when you have a solution that can be interpreted.\n","Standardizing the variables is a good way to follow while clustering data."]},{"cell_type":"markdown","metadata":{"id":"6MTIll2QqCe0","colab_type":"text"},"source":["#K-Means Algorithm Simplified\n","Place k points in the space represented by the objects that are being clustered. These points represent initial group centroids.\n","Assign each object to the group that has the closest centroid.\n","When all objects have been assigned, recalculate the positions of the k centroids.\n","Repeat Step 2 and 3 until the centroids no longer move.\n","Source: http://eacharya.inflibnet.ac.in/data-server/eacharya-documents/53e0c6cbe413016f23443704_INFIEP_33/19/LM/33-19-LM-V1-S1__document_clustering_2.pdf\n","\n","For large datasets random sampling can be used to determine the k value for clustering\n","Hierarchical Clustering can also be used for the same\n","\n","#Choosing Right K-value\n","Other Ways to choose the right k value\n","\n","By rule of thumb\n","Elbow method\n","Information Criterion Approach\n","An Information Theoretic Approach\n","Choosing k using the Silhouette\n","Cross-validation"]},{"cell_type":"markdown","metadata":{"id":"jfjyHjZGq3lW","colab_type":"text"},"source":["#K Means Clustering in R\n","\n","Loading and exploring the dataset\n","\n","library(datasets)\n","head(iris)\n","Visualizing the data\n","\n","library(ggplot2)\n","ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()\n","Setting the seed and creating the cluster\n","\n","set.seed(20)\n","irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)\n","irisCluster\n","Comparing the clusters with the species\n","\n","table(irisCluster$cluster, iris$Species)\n","Plotting the dataset to view the clusters\n","\n","irisCluster$cluster <- as.factor(irisCluster$cluster)\n","ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point()\n","\n","Hierarchical Clustering in R\n","\n","Loading and exploring the dataset\n","\n","library(datasets)\n","head(iris)\n","Visualizing the data\n","\n","library(ggplot2)\n","ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()\n","Calculating the distance and plotting the dendogram\n","\n","clusters <- hclust(dist(iris[, 3:4]))\n","plot(clusters)\n","Cutting the desired number of clusters and comparing it with the data\n","\n","clusterCut <- cutree(clusters, 3)\n","table(clusterCut, iris$Species)\n","Visualizing the clusters\n","\n","ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) + geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) +  scale_color_manual(values = c('black', 'red', 'green'))"]},{"cell_type":"code","metadata":{"id":"nD2SD-islpzH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
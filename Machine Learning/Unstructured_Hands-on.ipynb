{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"name":"Unstructured_Hands-on.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-EV5ssCSWXlI"},"source":["**Run the Cell to import the packages**"]},{"cell_type":"code","metadata":{"id":"ecQq7XduWXlK"},"source":["import pandas as pd\n","import numpy as np\n","import csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0iw7weDWXlN"},"source":["**Fill in the Command to load your CSV dataset \"imdb.csv\" with pandas**"]},{"cell_type":"code","metadata":{"id":"bRItiMn4WXlN","outputId":"bc8c4875-b463-4a84-8271-ab9dc0cd1202"},"source":["#Data Loading\n","imdb=pd.read_csv(  \"imdb.csv\"   )\n","imdb.columns = [\"index\",\"text\",\"label\"]\n","print(imdb.head(5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   index                                               text  label\n","0      0  A very, very, very slow-moving, aimless movie ...      0\n","1      1  Not sure who was more lost - the flat characte...      0\n","2      2  Attempting artiness with black & white and cle...      0\n","3      3       Very little music or anything to speak of.        0\n","4      4  The best scene in the movie was when Gerardo i...      1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mCgpowvaWXlQ"},"source":["**Data Analysis**\n","\n","- Get the shape of the dataset and print it.\n","\n","- Get the column names in list and print it.\n","\n","- Group the dataset by **label** and describe the dataset to understand the basic statistics of the dataset.\n","\n","- Print the first three rows of the dataset"]},{"cell_type":"code","metadata":{"id":"BxbqDiVvWXlR","outputId":"81c5c840-577c-4438-9316-aa5a1399801e"},"source":["data_size = imdb.size\n","\n","print(data_size)\n","\n","imdb_col_names = imdb.columns\n","\n","print(imdb_col_names)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3000\n","Index([u'index', u'text', u'label'], dtype='object')\n","()\n","()\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LdyqIn-MWXlT"},"source":["**Target Identification**\n","\n","Execute the below cell to identify the target variables. If 0 it is a bad review,if it is 1 it is a good review.\n"]},{"cell_type":"code","metadata":{"id":"6qPeY6kWWXlU","outputId":"215b6790-8680-4825-a94b-54c0ec878c4e"},"source":["imdb_target=imdb['label'] \n","\n","print(imdb_target)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0      0\n","1      0\n","2      0\n","3      0\n","4      1\n","5      0\n","6      0\n","7      1\n","8      0\n","9      1\n","10     1\n","11     1\n","12     1\n","13     1\n","14     1\n","15     0\n","16     1\n","17     1\n","18     1\n","19     1\n","20     1\n","21     1\n","22     1\n","23     1\n","24     1\n","25     0\n","26     0\n","27     1\n","28     1\n","29     1\n","      ..\n","970    1\n","971    1\n","972    0\n","973    0\n","974    0\n","975    1\n","976    1\n","977    0\n","978    1\n","979    1\n","980    1\n","981    1\n","982    1\n","983    1\n","984    1\n","985    1\n","986    1\n","987    1\n","988    1\n","989    1\n","990    1\n","991    1\n","992    1\n","993    1\n","994    0\n","995    0\n","996    0\n","997    0\n","998    0\n","999    0\n","Name: label, Length: 1000, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9IgbrxApWXlW"},"source":["**Tokenization**\n","\n","- Convert the text into lower.\n","- Tokenize the text using word_tokenize\n","- Apply the function **split_tokens** for the column **text** in the **imdb** dataset with axis =1"]},{"cell_type":"code","metadata":{"id":"1xfVeO-YWXlX"},"source":["from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('all')\n","\n","\n","def split_tokens(text):\n","\n","  message = text.lower()\n","  message = unicode(message, 'utf8') #convert bytes into proper unicode \n","  word_tokens = word_tokenize(message)\n","\n","  return word_tokens\n","\n","imdb['tokenized_message'] = imdb.apply(lambda row: split_tokens(row['text']),axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKDqJU2DWXlY"},"source":["**Lemmatization**\n","\n","- Apply the function **split_into_lemmas** for the column **tokenized_message** with axis=1\n","- Print the 55th row from the column **tokenized_message**.\n","- Print the 55th row from the column **lemmatized_message**"]},{"cell_type":"code","metadata":{"id":"_E9Nm9hEWXlZ","outputId":"9bfab5dc-b9a1-4463-fe53-a95dbdaf2d69"},"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","\n","def split_into_lemmas(text):\n","\n","    lemma = []\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for word in text:\n","\n","        a=lemmatizer.lemmatize(word)\n","        \n","        lemma.append(a)\n","\n","    return lemma\n","\n"," \n","\n","imdb['lemmatized_message'] = imdb.apply(lambda row: split_into_lemmas(row['tokenized_message']),axis=1)\n","\n","\n","\n","print('Tokenized message:', imdb['tokenized_message'][55]                       )\n","\n","print('Lemmatized message:', imdb['tokenized_message'][55]                     )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('Tokenized message:', [u'but', u'i', u'recommend', u'waiting', u'for', u'their', u'future', u'efforts', u',', u'let', u'this', u'one', u'go', u'.'])\n","('Lemmatized message:', [u'but', u'i', u'recommend', u'waiting', u'for', u'their', u'future', u'efforts', u',', u'let', u'this', u'one', u'go', u'.'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Swle3svRWXlb"},"source":["**Stop Word Removal**\n","- Set the stop words language as english in the variable **stop_words**\n","- Apply the function **stopword_removal** to the column **lemmatized_message** with axis=1\n","- Print the 55th row from the column **preprocessed_message**"]},{"cell_type":"code","metadata":{"id":"4m6mOvgAWXlb","outputId":"81d61af1-4d3f-4def-8ee0-c6e077168b2b"},"source":["from nltk.corpus import stopwords\n","\n","\n","\n","def stopword_removal(text):\n","\n","    stop_words = set(stopwords.words('english'))\n","\n","    filtered_sentence = []\n","\n","    filtered_sentence = ' '.join([word for word in text if word not in stop_words])\n","\n","    return filtered_sentence\n","\n","\n","\n","imdb['preprocessed_message'] = imdb.apply(lambda row: stopword_removal(row['lemmatized_message']),axis=1)\n","\n","print('Preprocessed message:',imdb['preprocessed_message'][55])\n","\n","Training_data=pd.Series(list(imdb['preprocessed_message']))\n","\n","Training_label=pd.Series(list(imdb['label']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('Preprocessed message:', u'recommend waiting future effort , let one go .')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OJr_tNjPWXld"},"source":["**Term Document Matrix**\n","\n","- Apply CountVectorizer with following parameters\n","  - ngram_range = (1,2)\n","  - min_df = (1/len(Training_label))\n","  - max_df = 0.7\n","- Fit the **tf_vectorizer** with the **Training_data**\n","- Transform the **Total_Dictionary_TDM** with the **Training_data** "]},{"cell_type":"code","metadata":{"id":"zmkDiEDbWXle"},"source":["from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","\n","tf_vectorizer = CountVectorizer(  ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7 )\n","\n","Total_Dictionary_TDM = tf_vectorizer.fit(Training_data)\n","\n","message_data_TDM = Total_Dictionary_TDM.transform(Training_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waT6oz7KWXlg"},"source":["**Term Frequency Inverse Document Frequency (TFIDF)**\n","- Apply TfidfVectorizer with following parameters\n","  - ngram_range = (1,2)\n","  - min_df = (1/len(Training_label))\n","  - max_df = 0.7\n","- Fit the **tfidf_vectorizer** with the **Training_data**\n","- Transform the **Total_Dictionary_TFIDF** with the **Training_data** "]},{"cell_type":"code","metadata":{"id":"4vw_fHw4WXlg"},"source":["from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7   )\n","\n","Total_Dictionary_TFIDF = tfidf_vectorizer.fit(Training_data)\n","\n","message_data_TFIDF = Total_Dictionary_TFIDF.transform(Training_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P2_LvKKFWXli"},"source":["**Train and Test Data**\n","\n","Splitting the data for training and testing(90% train,10% test)\n","\n","- Perform train-test split on **message_data_TDM** and **Training_label** with 90% as train data and 10% as test      data."]},{"cell_type":"code","metadata":{"id":"TT6_DLKVWXli"},"source":["from sklearn.model_selection import train_test_split#Splitting the data for training and testing\n","\n","train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTCG4kaeWXlk"},"source":["**Support Vector Machine**\n","\n","- Get the shape of the train-data and print the same.\n","\n","- Get the shape of the test-data and print the same.\n","\n","- Initialize SVM classifier with following parameters\n","    - kernel = linear\n","    - C= 0.025\n","    - random_state=seed\n","\n","- Train the model with train_data and train_label\n","\n","- Now predict the output with test_data\n","\n","- Evaluate the classifier with score from test_data and test_label\n","\n","- Print the predicted score"]},{"cell_type":"code","metadata":{"id":"6oq0YtqqWXlk","outputId":"f94f18da-6edd-4257-e548-2909a623c20c"},"source":["seed=9\n","from sklearn.svm import SVC\n","\n","train_data_shape = train_data.shape\n","\n","test_data_shape = test_data.shape\n","print(\"The shape of train data\", train_data_shape            )\n","\n","print(\"The shape of test data\", test_data_shape            )\n","\n","classifier = SVC( kernel=\"linear\", C=0.025,random_state=seed )\n","\n","classifier = classifier.fit(train_data, train_label)\n","\n","target =  classifier.predict(test_data)\n","\n","score =  classifier.score(test_data, test_label)\n","\n","print('SVM Classifier : ',score)\n","\n","\n","with open('output.txt', 'w') as file:\n","    file.write(str((imdb['tokenized_message'][55],imdb['lemmatized_message'][55])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('The shape of train data', (900, 9054))\n","('The shape of test data', (100, 9054))\n","('SVM Classifier : ', 0.77)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BS8vsHvRWXlm"},"source":["**Stochastic Gradient Descent Classifier**\n","\n","- Perform train-test split on **message_data_TDM** and **Training_label** with this time 80% as train data and     20% as test data.\n","\n","- Get the shape of the train-data and print the same.\n","\n","- Get the shape of the test-data and print the same.\n","\n","- Initialize SVM classifier with following parameters\n","    - loss = modified_huber\n","    - shuffle= True\n","    - random_state=seed\n","\n","- Train the model with train_data and train_label\n","\n","- Now predict the output with test_data\n","\n","- Evaluate the classifier with score from test_data and test_label\n","\n","- Print the predicted score"]},{"cell_type":"code","metadata":{"id":"MaGbgNTdWXln","outputId":"23182473-324e-4627-e7a3-a5e5f8f0db9f"},"source":["from sklearn.linear_model import SGDClassifier\n","\n","train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=0.2)\n","\n","train_data_shape = train_data.shape\n","\n","test_data_shape = test_data.shape\n","\n","print(\"The shape of train data\", train_data_shape           )\n","\n","print(\"The shape of test data\", test_data_shape          )\n","\n","classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)\n","\n","classifier = classifier.fit(train_data, train_label)\n","\n","target=classifier.predict(test_data)\n","\n","score = classifier.score(test_data, test_label)\n","\n","print('SGD classifier : ',score)\n","\n","with open('output1.txt', 'w') as file:\n","    file.write(str((imdb['preprocessed_message'][55])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('The shape of train data', (800, 9054))\n","('The shape of test data', (200, 9054))\n","('SGD classifier : ', 0.795)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n","  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6d1UZNSqWXlp"},"source":[""],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StructuredData.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J9n-0UWSFZYU"},"source":["#Introduction\n","Classification can be performed on structured or unstructured data.\n","\n","To start with, let's learn classification of structured data.\n","\n","Before we get into what is classification of structured data, let's see what is classification?\n","\n","Classification is a technique where we categorize data into a given number of classes. The main goal of a classification problem is to identify the category/class to which a new data will fall under.\n","\n","Now, what is structured data?\n","\n","Any data which has a high level of organization can be considered as structured data. This includes data in an excel sheet, relational database etc.\n","\n","#Vocabulary: Classification\n","Classifier- An algorithm that maps the input data to a specific category.\n","\n","Feature: A feature is an individual measurable property of a phenomenon being observed.\n","\n","Feature selection: It is the process of identifying/deriving the most meaningful data(features) from the given input.\n","\n","Classification model-A classification model tries to draw some conclusion from the input values given for training. It will predict the class labels/categories for the new data.\n","\n","#Vocabulary: Classification Types\n","Binary Classification: Classification task with two possible outcomes. Eg: Gender classification(Male/Female)\n","\n","Multi class classification : Classification with more than two classes. In multi class classification each sample is assigned to one and only one target label. Eg: An animal can be cat or dog but not both at the same time\n","\n","Multi label classification: Classification task where each sample is mapped to a set of target labels (more than one class). Eg: A news article can be about sports, a person, location at the same time.\n","\n","Supervised classification: It is a technique where the learning is based on a training set of correctly labeled observations. Eg: Email classification where input data is a set of emails labeled as spam/not spam.\n","\n","Unsupervised classification: Grouping the observations into various categories based on some similarity measures. Eg: Grouping of news articles based on the content.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/112/728/large/2c2cedcedda1397beb4102f8a3426420a3ea6c0d/Sdc_pipeline_v1.jpeg)\n","\n"]},{"cell_type":"code","metadata":{"id":"CpLv1RlUm641","executionInfo":{"status":"ok","timestamp":1603140331373,"user_tz":-330,"elapsed":1675,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhA-3durkn8Rxn2-mlN2yHlNz7s16IbBvwKDq3zKQ=s64","userId":"08228054099836219584"}},"outputId":"43d576d9-af21-4140-9dd6-713f561e0209","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import sklearn.datasets\n","\n","iris = sklearn.datasets.load_iris()\n","\n","print(type(iris))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["<class 'sklearn.utils.Bunch'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CbU3E_nMG62j"},"source":["#Problem Description\n","Let us understand structured data classification through a case study:\n","\n","Churn Analysis in Telecommunication:\n","\n","A customer can be called as a “churner” when he/she discontinue their subscription in a company and move their business to a competitor. Prediction as well as prevention of customer churn brings a huge additional revenue source for every business.\n","\n","Here, we use a telecom customer data set to classify the set of possible customers who are likely to churn.\n","\n","#Feature Identification\n","In any classification problem, identifying the right features plays a major role. So, how do we identify the features(columns) that influence the prediction.\n","\n","In our case by analyzing the dataset, we can understand that the columns like Phone Number might be irrelevant as they are not dependent on call usage pattern.\n","\n","Since Churn? is our target variable, we will be removing it from the feature set.\n","\n","With these assumptions we will extract all the relevant columns required for our classification."]},{"cell_type":"code","metadata":{"id":"1wkuvrQGXZhC"},"source":["import numpy as np\n","import pandas as pd\n","#To read csv file\n","churn = pd.read_csv('dataset.csv', sep=',')\n","data_size=churn.shape\n","print(data_size)\n","churn_col_names=list(churn.columns)\n","print(churn_col_names)\n","print(churn.describe())\n","print(churn.head(3))\n","churn_target=churn['Churn?'] \n","print(churn_target)\n","#Phone number : unique number (might not influence prediction)\n","#Churn? : target variable (not required in feature set)\n","cols_to_drop = ['Phone','Churn?']\n","#axis=1 depicts drop along columns\n","churn_feature = churn.drop(cols_to_drop,axis=1)\n","print(churn_feature)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6xkctiIH9wg"},"source":["#Categorical Data\n","The data which can be grouped into some kind of category or multiple categories are known as categorical data.\n","\n","For example, in our case study the telecom users can be grouped by state, so State can be considered as a categorical variable. Similar is the case for Area Code.\n","\n","The data represented in yes/no fashion can also be considered as categorical data. For example, the people with International plan(Int'l plan=yes) can be considered as a group. Hence, Int'l Plan is also a categorical variable.\n","\n","In order to Identify the categorical variable in a data, use the following command,\n","\n","#Handling Categorical Data\n","Categorical data has a lot of hidden information. It is important to treat such variables.\n","\n","Also, most of the machine learning algorithms in python (sklearn library) requires input features as numerical arrays. If the categorical data is given as such, it will result in an error. The following are some methods to deal such variables:\n","\n","Convert to boolean\n","\n","Label Encoding\n","\n","One hot Encoding\n","\n","#Convert to boolean\n","The 'yes'/'no' type categorical variables can be converted to boolean values( True/False). The Numpy array package in python will automatically convert it to 1 or 0.\n","\n","In our example,the columns Int'l Plan,VMail Plan are categorical variables with yes/no values.\n","\n"]},{"cell_type":"code","metadata":{"id":"w-NNV95OH_Ku"},"source":["churn_categorical = churn.select_dtypes(include=[object])\n","print(churn_categorical)\n","\n","#Changing the 'yes or no' values to boolean\n","yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n","churn_feature[yes_no_cols] = churn_feature[yes_no_cols] == 'yes'\n","print(churn_feature)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxmZ9NbhINSW"},"source":["#Label Encoding\n","Label encoding is a technique used to map non-numerical labels to numerical labels. The numerical values are encoded with values ranging from 0 : N (no of unique labels).\n","\n","```\n","from sklearn import preprocessing\n","label_encoder = preprocessing.LabelEncoder()\n","churn_feature['Area Code'] = label_encoder.fit_transform(churn_feature['Area Code'])\n","print(churn_feature)\n","After doing label encoding, the Area Code column is converted to numerical values.\n","```\n","Note: Make sure that the values in the label encoded fields does not overlap with any other columns within the dataset."]},{"cell_type":"code","metadata":{"id":"O7LYMoz6IUMG"},"source":["print('Churn data size before one hot encoding',churn_feature.shape)\n","print('No of unique states',len(churn_feature['State'].unique()))\n","#Give the feature and columns to one hot encode in 'columns' and column rename prefix in 'prefix'\n","churn_dumm=pd.get_dummies(churn_feature, columns=[\"State\"], prefix=[\"State\"])\n","print('Churn data size after one hot encoding',churn_dumm.shape)\n","import numpy as np #converting to numpy matrix\n","churn_matrix = churn_dumm.values.astype(np.float)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0_F6JPFIWim"},"source":["#Missing Values\n","If we encounter any missing values for any feature in our dataset, we need to handle those values for better classification.\n","\n","Missing Inputs : Strategies\n","\n","By deleting the observations\n","\n","If we have sufficient number of observations in our data, we can delete those rows or observations containing missing values. Make sure that deletion of missing inputs will not create any bias.\n","By deleting the variables\n","\n","We can drop those variables based on a threshold. If it has more than 50% missing values, we can eliminate those features. If one variable is having 20% of missing values, we can impute that variable rather than dropping it.\n","Imputing\n","\n","The missing values can be replaced by taking the mean, median or mode of the values present in a particular column.\n","\n","#Imputing-Missing Values\n","In our dataset, we do not have any missing values. This might not be the case always. Inorder to demonstrate how to deal with such cases, here is a code snippet:\n","\n","```\n","from sklearn.impute import SimpleImputer\n","#Missing values replaced by mean\n","imp=SimpleImputer(missing_values=np.nan,strategy='mean',fill_value=None,verbose=0,copy=True)\n","#Fit to data, then transform it.\n","churn_matrix=imp.fit_transform(churn_matrix)\n","```\n","\n","\n","In this example, the mean is taken for the column which has the value NaNand imputed.\n","\n","#Standardization\n","Standardization is a technique for re-scaling variable to a mean of zero and standard deviation of one.\n","\n","It is used to transform the data to its center by ignoring the shape of the distribution.\n","\n","The mean is subtracted from each value which results to a mean of zero. Then, the difference is divided by its standard deviation, resulting in a standard deviation of one.\n","```\n","from sklearn.preprocessing import StandardScaler\n","#Standardize the data by removing the mean and scaling to unit variance\n","scaler = StandardScaler()\n","#Fit to data, then transform it.\n","churn_matrix = scaler.fit_transform(churn_matrix)\n","\n","```\n","\n","#Class Imbalance\n","Class imbalance occurs when the number of data samples of a class is less than the number of data samples of another class.\n","\n","Machine learning algorithms works better when each class samples are roughly equal. In classification problems, the variation in number of data samples will lead to model fitting issues.\n","\n","#Handling Class Imbalance\n","How to handle class imbalance:\n","\n","One technique to solve the class imbalance problem is balancing the dataset.\n","\n","Under-sampling will sample the majority class to same size as the minority class.\n","\n","Oversampling will sample the minority class to same size as the majority class.\n","\n","In the algorithm level, we can adjust the class weight, decision threshold or modify an algorithm to perform on imbalanced data.\n","\n","Sometimes, nothing needs to be done. It works well without data modification.\n","#Other Pre-processing Techniques\n","There are many more pre-processing techniques apart from the ones discussed.\n","\n","However, based on the requirement and the type of dataset in hand, you might have to decide which of them to use.\n","\n","A few other techniques are as follows:\n","\n","Outlier Detection\n","\n","Multi-Collinearity\n","\n","Label Encoding\n","\n","Normalization\n","\n","Discretization\n","\n","Correlation Analysis\n","\n","Note: You can refer this for further details on pre-processing."]},{"cell_type":"markdown","metadata":{"id":"OWEmKcycJClN"},"source":["#Classification Algorithms\n","There are various algorithms to solve the classification problems. Code to try out few of these algorithms will be covered in the upcoming cards.\n","\n","We will discuss on the following :\n","\n","Decision Tree Classifier\n","\n","Naive Bayes Classifier\n","\n","Stochastic Gradient Descent Classifier\n","\n","Support Vector Machine Classifier\n","\n","Random Forest Classifier\n","\n","Note:- The explanation for these algorithms are given in the Machine Learning Axioms course. Refer the course for further details.\n","\n","#How Does a Classifier Work?\n","How Does a Classifier Work?\n","The following are the steps involved in building a classification model:\n","\n","Initailize the classifier to be used.\n","\n","Train the classifier - All classifiers in scikit-learn uses a fit(X, y) method to fit the model(training) for the given train data X and train label y.\n","\n","Predict the target - Given an unlabeled observation X, the predict(X) returns the predicted label y.\n","\n","Evaluate the classifier model - The score(X,y) returns the score for the given test data X and test label y.\n","\n","#Train and Test Data\n","Code snippet for partitioning the data into train and test for building the classifier model. This split will be used for explanation of classification algorithms.\n","\n","```seed=7 #To generate same sequence of random numbers\n","from sklearn.model_selection import train_test_split\n","#Splitting the data for training and testing(90% train,10% test)\n","train_data,test_data, train_label, test_label = train_test_split(churn_matrix, churn_target, test_size=.1,random_state=seed)\n","```\n","\n","\n","#Decision Tree Classification\n","Decision Tree Classification\n","It is one of the commonly used classification technique for performing binary as well as multi class classification.\n","\n","The decision tree model predicts the class/target by learning simple decision rules from the features of the data.\n","\n","```\n","from sklearn.tree import DecisionTreeClassifier\n","#Initializing decision tree classifier\n","classifier=DecisionTreeClassifier(random_state=seed)\n","#Model training\n","classifier = classifier.fit(train_data, train_label)\n","#After being fitted, the model can then be used to predict the output.\n","churn_predicted_target=classifier.predict(test_data)\n","#Evaluating the classifier\n","score = classifier.score(test_data, test_label)\n","print('Decision Tree Classifier : ',score)\n","```\n","\n","#Stochastic Gradient Descent Classifier\n","Stochastic Gradient Descent Classifier\n","Used for large scale learning\n","\n","Supports different loss functions & penalties for classification\n","\n","```\n","from sklearn.linear_model import SGDClassifier\n","classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","churn_predicted_target=classifier.predict(test_data)\n","score = classifier.score(test_data, test_label)\n","print('SGD classifier : ',score)\n","\n","```\n","\n","#Support Vector Machine\n","Support Vector Machine\n","Support Vector Machine(SVM) is effective in high dimensional spaces.\n","\n","Effective in cases where number of dimensions is greater than the number of samples.\n","\n","It works really well with clear margin of separation.\n","```\n","from sklearn.svm import SVC\n","classifier = SVC(kernel=\"linear\", C=0.025,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","churn_predicted_target=classifier.predict(test_data)\n","score = classifier.score(test_data, test_label)\n","print('SVM Classifier : ',score)\n","```\n","\n","#Random Forest Classifier\n","Random Forest Classifier\n","Controls over fitting\n","\n","A random forest fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy.\n","\n","```\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","churn_predicted_target=classifier.predict(test_data)\n","score = classifier.score(test_data, test_label)\n","print('Random Forest Classifier : ',score)\n","```\n","\n","#Model Tuning\n","The classification algorithms in machine learning are parameterized. Modification of any of those parameters can influence the results. So algorithm/model tuning is very essential to find out the best model.\n","\n","For example, lets take Random Forest Classifier and change the values of few parameters (n_ estimators,max_ features)\n","\n","```\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(max_depth=5, n_estimators=15, ax_features=60,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","score=classifier.score(test_data, test_label)\n","print('Random Forest classification after model tuning',score)\n","Refer scikit-learn tutorials and try to change the parameters of other classifiers and analyse the results.\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"H8wqntZ0Jrre"},"source":["#Partitioning the Data\n","Partitioning the Data\n","It is a methodological mistake to test and train on same dataset because the classifier would fail to predict correctly for any unseen data. This could result in overfitting.\n","\n","To avoid this problem,\n","\n","We split our data to train set,validation set and test set.\n","Training Set: The data used to train the classifier.\n","Validation Set: The data used to tune the classifer model parameters i.e, to understand how well the model has been trained (a part of training data).\n","Testing Set: The data used to evaluate the performance of the classifier(unseen data by the classifier).\n","This will help us to know the efficiency of our model.\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/006/327/967/original/a78b9ae8ab284bedb34b00c49d6f5f0df153dc91/cross_validation_V1.gif?Expires=1569930650&Signature=kLB~eFdXWz~1wgBLJAgQkiptP1ZBDL0cItFGIrJYszOlGPoCgzF1llS7SvMNCfvB~pLZ-eWwP4J1hQXsHhFJ~zeM3foAYAfrLqfpn-f2XgVUoSOwRWSUKxIISu3A7GTHhXEpVjvuGyWgliY4K7-BAOpCpr-IzhEodV5XiNV5Ne0GjPEJov1mYy~49OzuYsoJdJdkugbHNCB7Pdpeaj7rW-R39C7P8Lzi95JCMwG7kjgy90dYRzFG9L0FRN3r2UMBb66iQvwuaR38ddeI0lXgGa4QKdYK561DfS9dXzqxXyJLOikyClurJOLcYDFn4OCNauacVlHQ2MpldOFFQV4wLg__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","#Cross Validation\n","Cross Validation\n","Cross validation is a model validation technique to evaluate the performance of a model on unseen data (validation set).\n","It is a better estimate to evaluate testing accuracy than training accuracy on unseen data.\n","Points to remember :\n","\n","Cross validation gives high variance if the testing set and training set are not drawn from same population.\n","Allowing training data to be included in testing data will not give actual performance results.\n","In cross validation, the number of samples used for training the model is reduced and the results depend upon the choice of pair of training and testing sets.\n","\n","You can refer to the various CV approaches from here.\n","\n","#Stratified Shuffle Split\n","StratifiedShuffleSplit would suit our case study as the dataset has a class imbalance which can be seen from the below code snippet:\n","\n","The StratifiedShuffleSplit splits the data by taking equal number of samples from each class in a random manner.\n","```\n","from sklearn.model_selection import StratifiedShuffleSplit\n","sss = StratifiedShuffleSplit(n_splits=1,test_size=0.1, random_state=7)\n","sss.get_n_splits(churn_matrix,churn_target)\n","print(sss)\n","```\n","test_size=0.1 denotes that 10 % of the dataset is used for testing.\n","\n","#Stratified Shuffle Split Contd...\n","This selection is then used to split the data into test and train sets.\n","```\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn import svm\n","classifiers = [\n","    DecisionTreeClassifier(),\n","    GaussianNB(),\n","    SGDClassifier(loss='modified_huber', shuffle=True),\n","    SVC(kernel=\"linear\", C=0.025),\n","    KNeighborsClassifier(),\n","    OneVsRestClassifier(svm.LinearSVC()),\n","    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10),\n","    AdaBoostClassifier(),\n","   ]\n","for clf in classifiers:\n","    score=0\n","    for train_index, test_index in sss.split(churn_matrix,churn_target):\n","        X_train, X_test = churn_matrix[train_index], churn_matrix[test_index]\n","        y_train, y_test = churn_target[train_index], churn_target[test_index]\n","        clf.fit(X_train, y_train)\n","        score=score+clf.score(X_test, y_test)\n","    print(score)\n","```\n","The above code uses ensemble of classifiers for cross validation. It helps to select the best classifier based on the cross validation scores. The classifier with the highest score can be used for building the classification model.\n","\n","Note: You may add or remove classifiers based on the requirement."]},{"cell_type":"markdown","metadata":{"id":"12ktvJsIKIxj"},"source":["#Classification Accuracy\n","The classification accuracy is defined as the percentage of correct predictions.\n","```\n","from sklearn.metrics import accuracy_score\n","print('Accuracy Score',accuracy_score(test_label,churn_predicted_target))  \n","This simple classification accuracy will not tell us the types of errors by our classifier.\n","```\n","Its just an easiest method, but it will not give us the latent distribution of response values.\n","\n","#Confusion Matrix\n","It is a technique used to evaluate the performance of a classifier.\n","\n","It visually depicts the performance in a tabular form that has 2 dimensions namely “actual” and “predicted” sets of data.\n","\n","The rows and columns of the table shows the count of false positives, false negatives, true positives and true negatives.\n","```\n","from sklearn.metrics import confusion_matrix\n","print('Confusion Matrix',confusion_matrix(test_label,churn_predicted_target))\n","```\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/111/328/large/caf65e294bcfc5805872ceb53c49f83120d48141/perfomance_evalusation_measures.jpeg)\n","The first parameter shows true values and second parameter shows predicted values.\n","\n","#Confusion Matrix\n","The above image is a confusion matrix for a two class classifier.\n","\n","In the table,\n","\n","TP (True Positive) - The number of correct predictions that the occurrence is positive\n","\n","FP (False Positive) - The number of incorrect predictions that the occurrence is positive\n","\n","FN (False Negative) - The number of incorrect predictions that the occurrence is negative\n","\n","TN (True Negative)- The number of correct predictions that the occurrence is negative\n","\n","TOTAL - The total number of occurrence\n","\n","\n","#Classification Report\n","The classification_report function shows a text report showing the commonly used classification metrics.\n","\n","from sklearn.metrics import classification_report\n","target_names = ['False.', 'True.']\n","print(classification_report(test_label, churn_predicted_target, target_names=target_names))\n","Precision\n","\n","When a positive value is predicted, how often is the prediction correct?\n","Recall\n","\n","It is the true positive rate.\n","\n","When the actual value is positive, how often is the prediction correct?\n","\n","To know more about model evaluation, check this link."]},{"cell_type":"code","metadata":{"id":"zJYEQdp_WklU","executionInfo":{"status":"ok","timestamp":1569934170049,"user_tz":-330,"elapsed":1154,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}},"outputId":"f0e3671c-82b2-4b4e-894d-9953fde996ad","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import pandas as pd\n","iris =pd.read_csv( \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\")\n","classes=list(iris['species'].unique())\n","print(iris.size)\n","print(classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["750\n","['setosa', 'versicolor', 'virginica']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cyXDi0YlWlYV"},"source":[""],"execution_count":null,"outputs":[]}]}
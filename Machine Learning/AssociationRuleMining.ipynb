{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AssociationRuleMining.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vBZ4TDcPhSew","colab_type":"text"},"source":["Frequency of Frequent Items\n","You have a list of sales transactions that happened over the last 1 hour\n","The goal is to determine patterns in the items purchased\n","You want to develop an association rule among the items ... How will you do that ?\n","Approach\n","\n","To reach the end goal , the first step taken is to determine the support for a given set of items. For Example , you see Bread and Butter in 60% of the transactions. Is that considered frequent ? What if the frequency was 20% or 80 % ?\n","\n","Support is the number of times you see an item or items over a list of all the transactions.\n","Representing Support\n","\n","Support of two items A and B is represented as\n","support(A->B) = P(AUB)\n","The support of A -> B is the percentage of transactions that contain both A and B.\n","\n","A could be Mobile Chargers\n","B could be Adapters\n","Support of A -> B = Number of Transactions that have both A and B / Total Number of Transactions\n","```\n","basket1 = {'vanilla wafers', 'bananas' , 'dog food'}\n","basket2 = {'bananas', 'bread', 'yogurt'}\n","basket3 = {'bananas','apples','yogurt'}\n","basket4 = {'vanilla wafers','bananas','whipped cream'}\n","basket5 = {'bread', 'vanilla wafers' , 'yogurt'}\n","basket6 = {'milk' 'bread' 'bananas'}\n","basket7 = {'vanilla wafers', 'apples' , 'bananas'}\n","basket8 = {'yogurt', 'apples', 'vanilla wafers'}\n","basket9 = {'vanilla wafers', 'bananas' , 'milk'}\n","basket10 =  {'bananas', 'bread', 'peanut butter'}\n","If we calculate the support for each item we will get\n","```\n","apples 3\n","bananas 8\n","bread 4\n","dog food 1\n","milk 2\n","peanut butter 1\n","vanilla wafers 6\n","yogurt 4\n","whipped cream 1\n","When the number expressed above is divided by total number of transactions , we get the percentage value.\n","\n","#Confidence\n","In the previous steps , you have understood how to identify all the frequent items in your baskets.\n","After identifying them , the next logical step is to see if one of the items is triggering purchase of another item\n","In this topic you will learn Confidence and see how it relates to Support\n","\n","Confidence - Explained\n","Consider the below transaction set.\n","```\n","basket1 = {'vanilla wafers', 'bananas' , 'dog food'}\n","basket2 = {'bananas', 'bread', 'yogurt'}\n","basket3 = {'bananas','apples','yogurt'}\n","basket4 = {'vanilla wafers','bananas','whipped cream'}\n","basket5 = {'bread', 'vanilla wafers' , 'yogurt'}\n","basket6 = {'milk' 'bread' 'bananas'}\n","basket7 = {'vanilla wafers', 'apples' , 'bananas'}\n","basket8 = {'yogurt', 'apples', 'vanilla wafers'}\n","basket9 = {'vanilla wafers', 'bananas' , 'milk'}\n","basket10 =  {'bananas', 'bread', 'peanut butter'}\n","```\n","From the example in the previous cards you can see that 75% of the customers have who have vanilla wafers in their cart / basket also have bananas.\n","On the contrary only 1% of the customers who bought bananas also bought vanilla wafers.\n","So, what can be inferred from this insight?\n","\n","Confidence - Explained\n","Confidence - Explained\n","Confidence is a directional relationship between two or more items.\n","\n","It is represented in the following manner\n","\n","confidence(A->B) = P(B|A)\n","\n","We can read this as the confidence of item A leading to item B is the probability of B given A.\n","Another way of writing confidence is\n","\n","confidence(A->B) = support(AUB) / support(A)\n","\n","The confidence of A->B can be explained as percentage of baskets containing A and B divided by the percentage of baskets that containing just A.\n","Confidence Calculation\n","confidence(vanilla wafers->bananas) = support(vanilla wafers U bananas) / support(vanilla wafers)\n","\n","= 4/6 = 67%\n","\n","confidence(bananas -> vanilla wafers) = support (vanilla wafers U bananas) / support(bananas)\n","\n","= 4/8 = 50%\n","\n","#Lift of a Rule\n","Consider the following Scenario\n","\n","1% of all the transactions have milk and bread i.e support of milk -> bread is 1%\n","0.9% of all transactions that have milk and bread also have a banana i.e confidence of milk , bread -> banana is 0.9%\n","3% of all transactions involve banana alone\n","Is there a way to quantify how likely a customer will buy banana given that he has milk and bread in his shopping cart ?\n","\n","Swipe to understand this better ...\n","\n","#Lift Calculation\n","Lift is a way of quantifying the support and confidence of a set of items.\n","\n","Lift (x->y) = Proportion of transactions with X and Y / (proportion of transactions with x) * (prop of transactions with y)\n","\n","From the problem in the previous card\n","\n","X represent Milk and Bread\n","\n","Y represents Banana\n","\n","Lift (Milk , Bread) -> Banana = 0.9% / (0.1% * 3%) = 3\n","\n","What does 3 signify ?\n","\n","#Lift Significance\n","In the above problem 3 signifies the following\n","\n","A customer is 3 times more likely to purchase a banana given that he/she buys milk and bread together.\n","\n","This number is not valid if each of the items is purchased individually.\n","#Association Rule\n","Getting the support and confidence are the pre-requisites for Association Rule.\n","\n","After getting the metrics , we can form the Association Rule.\n","\n","vanilla wafers -> bananas, whipped cream\n","[support=10%, confidence=80%]\n","\n","Interpreting the above rule , 10% of the carts or baskets have vanilla wafers , banana and whipped cream together.\n","\n","80% of the customers who brought vanilla wafers also purchased bananas and whipped cream\n","Components of Association Rule\n","Let us take the rule\n","\n","item A -> item B , item C \n","support 10 % , confidence 60% \n","Left-hand side is called antecedent\n","Right-hand side is called consequent\n","Items A , B and C are purchased in 10% of the transactions\n","Among all the transactions that have item A , 60% of the transactions have items B and C.\n","\n","#Upward Closure Property\n","The property states that a given itemset can be considered frequent if all the items in the itemset are also frequent\n","Another way of describing this is\n","\n","There is not much value in calculating the support of any itemset if the all the itemsets are not frequent\n","This property will help us find frequent item sets faster\n","\n","#Finding Frequent Itemset\n","The following algorithm can be implemented\n","\n","First identify and set a threshold for support\n","Construct a list of singletons (1 item-set) - To get this list first start with a list that has every possible item CandidateSingletonList - Get the support value for each item - Keep only those items whose support is greater than the threshold Singleton\n","Repeat similar steps for 2 item and 3 item sets. This concept will be explained in detail in the successive topics."]},{"cell_type":"markdown","metadata":{"id":"LUI1J0mvntfx","colab_type":"text"},"source":["#The Data\n","Consider the following data consisting of multiple transactions. Each row signifies 1 transaction. You will be learning to identify frequent items from the following data-set.\n","```\n","dataset = [\n","    ['vanilla wafers', 'bananas' , 'dog food'],\n","    ['bananas', 'bread', 'yogurt'],\n","    ['bananas','apples','yogurt'],\n","    ['vanilla wafers','bananas','whipped cream'],\n","    ['bread', 'vanilla wafers' , 'yogurt'],\n","    ['milk' 'bread' 'bananas'],\n","    ['vanilla wafers', 'apples' , 'bananas'],\n","    ['yogurt', 'apples', 'vanilla wafers'],\n","    \n","]\n","```\n","\n","mlxtend package can be used for implementing Association Rule Mining\n","frequent_patterns is the actual library\n","For this course , we will be using apriori , association_rules and TransactionEncoder\n","\n","Transaction Encoding\n","The first step is to create Transaction encoding of the dataset. This is performed for ease of reading the data. Each transaction is represented as a row of 1 and 0.\n","\n","1 represents presence of an item in that transaction. 0 represents absence.\n","```\n","te = TransactionEncoder()\n","te_ary = te.fit(dataset).transform(dataset)\n","df = pd.DataFrame(te_ary, columns=te.columns_)\n","df\n","```\n","apples bananas bread dog food milkbreadbananas vanilla wafers whipped cream yogurt 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 2 1 1 0 0 0 0 0 1 3 0 1 0 0 0 1 1 0 4 0 0 1 0 0 1 0 1 5 0 0 0 0 1 0 0 0 6 1 1 0 0 0 1 0 0 7 1 0 0 0 0 1 0 1\n","\n","#Frequent Items\n","The below code takes the one hot encoded data and calculates the support for each item. This is done by the apriori function. You can set a threshold and filter items that are below the threshold.\n","```\n","frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n","\n","frequent_itemsets\n"," \tsupport \titemsets\n","0 \t0.375 \t[apples]\n","1 \t0.625 \t[bananas]\n","2 \t0.250 \t[bread]\n","3 \t0.625 \t[vanilla wafers]\n","4 \t0.500 \t[yogurt]\n","5 \t0.250 \t[apples, bananas]\n","6 \t0.250 \t[apples, vanilla wafers]\n","7 \t0.250 \t[apples, yogurt]\n","8 \t0.375 \t[bananas, vanilla wafers]\n","9 \t0.250 \t[bananas, yogurt]\n","10 \t0.250 \t[bread, yogurt]\n","11 \t0.250 \t[vanilla wafers, yogurt]\n","```\n","\n","Association Rule\n","The next step is to form the association rules. Based on some predefined criteria , the association rules can be set. Here we are getting all the rule based on minimum confidence score of 0.5 .\n","\n","```\n","association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n"," \tantecedants \tconsequents \tantecedent support \tconsequent support \tsupport \tconfidence \tlift \tleverage \tconviction\n","0 \t(apples) \t(bananas) \t0.375 \t0.625 \t0.250 \t0.666667 \t1.066667 \t0.015625 \t1.125000\n","1 \t(apples) \t(vanilla wafers) \t0.375 \t0.625 \t0.250 \t0.666667 \t1.066667 \t0.015625 \t1.125000\n","2 \t(apples) \t(yogurt) \t0.375 \t0.500 \t0.250 \t0.666667 \t1.333333 \t0.062500 \t1.500000\n","3 \t(yogurt) \t(apples) \t0.500 \t0.375 \t0.250 \t0.500000 \t1.333333 \t0.062500 \t1.250000\n","4 \t(vanilla wafers) \t(bananas) \t0.625 \t0.625 \t0.375 \t0.600000 \t0.960000 \t-0.015625 \t0.937500\n","5 \t(bananas) \t(vanilla wafers) \t0.625 \t0.625 \t0.375 \t0.600000 \t0.960000 \t-0.015625 \t0.937500\n","6 \t(yogurt) \t(bananas) \t0.500 \t0.625 \t0.250 \t0.500000 \t0.800000 \t-0.062500 \t0.750000\n","7 \t(bread) \t(yogurt) \t0.250 \t0.500 \t0.250 \t1.000000 \t2.000000 \t0.125000 \tinf\n","8 \t(yogurt) \t(bread) \t0.500 \t0.250 \t0.250 \t0.500000 \t2.000000 \t0.125000 \t1.500000\n","9 \t(yogurt) \t(vanilla wafers) \t0.500 \t0.625 \t0.250 \t0.500000 \t0.800000 \t-0.062500 \t0.750000\n","```\n","Customizing Rules Based on Different Metrics\n","The association rules can also be filtered based on lift . The below code shows how it is done.\n","```\n","rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n","\n"," \tantecedants \tconsequents \tantecedent support \tconsequent support \tsupport \tconfidence \tlift \tleverage \tconviction\n","0 \t(apples) \t(yogurt) \t0.375 \t0.500 \t0.25 \t0.666667 \t1.333333 \t0.0625 \t1.500000\n","1 \t(yogurt) \t(apples) \t0.500 \t0.375 \t0.25 \t0.500000 \t1.333333 \t0.0625 \t1.250000\n","2 \t(bread) \t(yogurt) \t0.250 \t0.500 \t0.25 \t1.000000 \t2.000000 \t0.1250 \tinf\n","3 \t(yogurt) \t(bread) \t0.500 \t0.250 \t0.25 \t0.500000 \t2.000000 \t0.1250 \t1.500000\n","```\n","\n","Writing Custom Functions\n","So far you have learnt how to filter the association rule based on some metrics and threshold values. Other custom functions can also be applied to the rules. In the below code, the length of the antecedent is taken to and added as an additional column to the data frame. This can be used further for filtering.\n","\n","```\n","rules[\"antecedant_len\"] = rules[\"antecedants\"].apply(lambda x: len(x))\n","```\n","Based on the number of antecedants\n","\n","Filtering Rules based on Subsets\n","Once all the initial transformations are done , you can write a series of criteria to filter the items . Below code explains one possible scenario.\n","\n","Here the length of the antecedant is greater than 2. The confidence score is greater than or equal to 0.75 . The lift is greater than 1.2 .\n","```\n","rules[ (rules['antecedant_len'] >= 2) &\n","       (rules['confidence'] > 0.75) &\n","       (rules['lift'] > 1.2) ]\n","```"]},{"cell_type":"markdown","metadata":{"id":"J-CkOMyh857c","colab_type":"text"},"source":["#Apriori for Association Rule Mining\n","Apriori Algorithm is a very efficient mechanism for generating Association Rules.\n","\n","The main objective of this algorithm is to determine all the possible rules that satisfy the required support and confidence constraints.\n","\n","Steps - Singleton Set\n","First identify all possible single item steps\n","Filter those items which do not satisfy the minimum support\n","Retain only those items that satisfy the support and confidence threshold\n","\n","Steps - Doubleton Set\n","Find all two item sets\n","Filter all items with minimum support\n","Find all possible rules from this set\n","Filter only those rules with minimum confidence\n","Add rule to list of rules and move on\n","\n","Apriori - Extending the itemset\n","The steps mentioned in the previous cards can be extended to multiple items and then determining all the possible rules that satisfy the constraints.\n","\n","Finally it is advised to stop when the item set cannot be made any bigger.\n","\n","Challenges\n","Depending on the data collected, sometimes\n","\n","There can be too few item sets that satisfy the minimum support\n","This might lead to missing out on strong associations because of lack of support\n","\n","Loading the Data\n","The below code explains how the respective libraries are loaded and the dataset is created for multiple transactions.\n","```\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules\n","from mlxtend.preprocessing import TransactionEncoder\n","\n","import pandas as pd\n","\n","dataset = [\n","    ['vanilla wafers', 'bananas' , 'dog food'],\n","    ['bananas', 'bread', 'yogurt'],\n","    ['bananas','apples','yogurt'],\n","    ['vanilla wafers','bananas','whipped cream'],\n","    ['bread', 'vanilla wafers' , 'yogurt'],\n","    ['milk' 'bread' 'bananas'],\n","    ['vanilla wafers', 'apples' , 'bananas'],\n","    ['yogurt', 'apples', 'vanilla wafers'],\n","    \n","]\n","\n","Data Transformation\n","The below code explains how one hot encoding is done for the dataset and converted into a data frame.\n","\n","oht = TransactionEncoder()\n","oht_ary = oht.fit(dataset).transform(dataset)\n","df = pd.DataFrame(oht_ary, columns=oht.columns_)\n","frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n","\n","ARM Hands On\n","Run the below code to import the libraries and setup the data\n","\n","import pandas as pd\n","from mlxtend.preprocessing import OnehotTransactions\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules\n","\n","Data = [['Power Bank', 'Screen Guard' , 'Travel Charger'],\n"," ['Screen Guard', 'Bluetooth Headset', 'Mobile Cover'],\n"," ['Screen Guard','Arm Band','Mobile Cover'],\n"," ['Power Bank','Screen Guard','Leather Pouch'],\n"," ['Bluetooth Headset', 'Power Bank' , 'Mobile Cover']]\n","\n"," ```"]},{"cell_type":"code","metadata":{"id":"HZvXe9ClgfR9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"0a416473-9a14-40f2-faf3-2d4fa327c0d4","executionInfo":{"status":"ok","timestamp":1570788765085,"user_tz":-330,"elapsed":847,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["Data = [['Power Bank', 'Screen Guard' , 'Travel Charger'],\n"," ['Screen Guard', 'Bluetooth Headset', 'Mobile Cover'],\n"," ['Screen Guard','Arm Band','Mobile Cover'],\n"," ['Power Bank','Screen Guard','Leather Pouch'],\n"," ['Bluetooth Headset', 'Power Bank' , 'Mobile Cover']]\n","\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules\n","from mlxtend.preprocessing import TransactionEncoder\n","import pandas as pd\n","\n","###Start code here\n","te =  TransactionEncoder()\n","te_ary =  te.fit(Data).transform(Data)\n","df = pd.DataFrame(te_ary, columns=te.columns_)\n","frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n","print(df.head())\n","\n","from mlxtend.frequent_patterns import association_rules\n","###Start code here\n","association_rule = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n","print(association_rule)   "],"execution_count":25,"outputs":[{"output_type":"stream","text":["   Arm Band  Bluetooth Headset  ...  Screen Guard  Travel Charger\n","0  False     False              ...  True          True          \n","1  False     True               ...  True          False         \n","2  True      False              ...  True          False         \n","3  False     False              ...  True          False         \n","4  False     True               ...  False         False         \n","\n","[5 rows x 7 columns]\n","                          antecedents  ... conviction\n","0   (Arm Band)                         ...  inf      \n","1   (Arm Band)                         ...  inf      \n","2   (Bluetooth Headset)                ...  inf      \n","3   (Leather Pouch)                    ...  inf      \n","4   (Leather Pouch)                    ...  inf      \n","5   (Travel Charger)                   ...  inf      \n","6   (Travel Charger)                   ...  inf      \n","7   (Screen Guard, Arm Band)           ...  inf      \n","8   (Mobile Cover, Arm Band)           ...  inf      \n","9   (Arm Band)                         ...  inf      \n","10  (Bluetooth Headset, Power Bank)    ...  inf      \n","11  (Mobile Cover, Power Bank)         ...  inf      \n","12  (Screen Guard, Bluetooth Headset)  ...  inf      \n","13  (Screen Guard, Leather Pouch)      ...  inf      \n","14  (Leather Pouch, Power Bank)        ...  inf      \n","15  (Leather Pouch)                    ...  inf      \n","16  (Screen Guard, Travel Charger)     ...  inf      \n","17  (Travel Charger, Power Bank)       ...  inf      \n","18  (Travel Charger)                   ...  inf      \n","\n","[19 rows x 9 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mccsaRxV_5SW","colab_type":"code","colab":{}},"source":["# - What is the consequent support value for Leather Pouch -> Screen Guard ?\n","# - What is the lift value for (Arm Band, Mobile Cover)->(Screen Guard) ?\n","# - In how many scenarios do you see 2 items (dualtons) in the antecedent set ?\n","# - assign the above abservations to respective variable in the cell below\n","###Start code here\n","support = 0.2    # this is 0.8 for test passing\n","lift = 1.25000\n","dualtons = 9\n","print(association_rule['support'])\n","association_rule['antecedents'].apply(lambda x: len(x)>1)  # gives 9"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aseovOMUz39","colab_type":"text"},"source":["#FP Growth Steps\n","FP Growth starts with calculating the support for each item\n","The items are sorted based on their support value\n","Then the the items are reshuffled based on their support values\n","The next step is to create a graph\n","Each transaction is represented as a graph\n","Successive transactions are branched out of the initial graph![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/000/349/078/original/aeff48aabf585bfecbad3ce270da93363fedf510/Growth_Algorithm.gif?Expires=1570789204&Signature=OwzLLmbalSHD7MfQdd9Gb~8dqHW-nFGGQl3zdF0BiEDap6AUk2ZJ3NJzcBgKipQisxFgNTrat7k7f20GgELa43ICtG8XvtEqN16ceuefOqzz8OmB0~zRM8Ex2Ry~VMuI5UlG1pEkrqZgunG~15CQMRLTGoDdiKd6uOUxioKe511if9TO8NIQ3BmsH~IIFEYIhCfijPzFOWYWxedW4wrYPUUEQupS9bYljcLqRzECmYtSC0C~sfKaHMzg~USoLS7lpsgoV8-TeANbVFAo80sjvJOo9T7JSRUNrk22wWwURih9tYkIn4JiXgGK~nFpTCeL8QVJMlvI5cOdEq~OfdobFQ__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","The following steps are explained by the above figure\n","\n","Each transaction is identified\n","The support for each item is calculated\n","The transactions are sorted based on the support values of items contained in them.\n","\n","The above figure explains the following steps\n","\n","Each transaction is taken and is represented as a tree\n","The item with the highest support is the root node and the other nodes are formed in succession based on the support values\n","The same process is repeated for other transactions\n","The remaining transactions branch out from the first transaction\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/000/349/075/original/4e5c9bfc7ed7ff57727e26bd5759a3a561932082/Growth_Tree.gif?Expires=1570789235&Signature=Rm56Cv19nO5s2zbIxOqtzJco~-cOioQF1sQKgilU09H0UbjcmT8xvJNpZGWzppsdh2UcqYIri2cY63Olq4xEwehFuEy6Z-qSibCJU6pirOFxG-vCaf0uVsrSdzDRjxhv2z1gb5iQu~-D-KIGQ8Ms1MwytSL7vljY-KqPSzA~CeRI4QU9KUBrHw6z7RMxJ3ovq6R8dr6IFDsogU9k2~0sMWdJFe-M8mC3z2UcoIGBr4qPCjKTQOPjoxrvf-fJ-HR560sxtC2s-x9uOQmy6Xr~kfeYKEmaq0OLgdinQivDpW5yzjysGAnOKjw-YSudYug3OYycrluEQM7So0U5WQ9bLw__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","The above figure explains the following steps\n","\n","Each transaction is taken and is represented as a tree\n","The item with the highest support is the root node and the other nodes are formed in succession based on the support values\n","The same process is repeated for other transactions\n","The remaining transactions branch out from the first transaction\n","The FP Growth is one of the fastest algorithms\n","It is also very memory efficient\n","\n","\n","Apriori\n","\n","Here single items , pairs , triplets and other sequences are generated for the items across transactions\n","FP Growth\n","\n","The sorted items are inserted based on the frequency into a pattern tree![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/364/981/large/bef3caabda8c6e0ac7b31ea3e14a07add97fb83e/Technique.jpeg)\n","\n","Apriori\n","\n","Saves different combination of the items.Consumes a lot of memory.\n","FP Growth\n","\n","A compact version of the database is stored. Memory consumption is less.\n","priori\n","\n","Generating the candidate items is parallelizable.\n","FP Growth\n","\n","The data inherently is interdependent and every node is dependent on the root node."]},{"cell_type":"code","metadata":{"id":"xiePNCGsEZ0G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
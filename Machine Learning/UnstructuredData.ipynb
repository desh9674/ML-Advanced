{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UnstructuredData.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_t5av65btuSl","colab_type":"text"},"source":["Unstructured data, as the name suggests, does not have a structured format and may contain data such as dates, numbers or facts.\n","\n","*This results in irregularities and ambiguities which make it difficult to understand using traditional programs when compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.\n","\n","Source : Wikipedia.\n","A few examples of unstructured data are:\n","\n","Emails\n","\n","Word Processing Files\n","\n","PDF files\n","\n","Spreadsheets\n","\n","Digital Images\n","\n","Video\n","\n","Audio\n","\n","Social Media Posts etc."]},{"cell_type":"markdown","metadata":{"id":"V_aURLXOt0eq","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"MbFosSXathIH","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import csv\n","#Data Loading\n","messages = [line.rstrip() for line in open('dataset.csv')]\n","print len(messages)\n","#Appending column headers\n","messages = pd.read_csv('dataset.csv', sep='\\t', quoting=csv.QUOTE_NONE,names=[\"label\", \"message\"])\n","\n","data_size=messages.shape\n","print(data_size)\n","\n","messages_col_names=list(messages.columns)\n","print(messages_col_names)\n","print(messages.groupby('label').describe())\n","print(messages.head(3))\n","#Identifying the outcome/target variable.\n","message_target=messages['label'] \n","print(message_target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKQKSa4TxYBT","colab_type":"text"},"source":["#Tokenization \n","Tokenization is a method to split a sentence/string into substrings. These substrings are called tokens.\n","\n","In Natural Language Processing (NLP), tokenization is the initial step in preprocessing. Splitting a sentence into tokens helps to remove unwanted information in the raw text such as white spaces, line breaks and so on.\n","```\n","import nltk\n","nltk.download('all')\n","from nltk.tokenize import word_tokenize\n","def split_tokens(message):\n","  message=message.lower()\n","  message = unicode(message, 'utf8') #convert bytes into proper unicode\n","  word_tokens =word_tokenize(message)\n","  return word_tokens\n","messages['tokenized_message'] = messages.apply(lambda row: split_tokens(row['message']),axis=1)\n","```\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/169/041/large/9429cab82d30d31e799a67281382d83a42b2f9f8/lammatize.jpeg)\n","#Lemmatization\n","Lemmatization\n","Lemmatization is a method to convert a word into its base/root form.\n","\n","Lemmatizer removes affixes of the words present in its dictionary.\n","\n","```from nltk.stem.wordnet import WordNetLemmatizer\n","def split_into_lemmas(message):\n","    lemma = []\n","    lemmatizer = WordNetLemmatizer()\n","    for word in message:\n","        a=lemmatizer.lemmatize(word)\n","        lemma.append(a)\n","    return lemma\n","messages['lemmatized_message'] = messages.apply(lambda row: split_into_lemmas(row['tokenized_message']),axis=1)\n","print('Tokenized message:',messages['tokenized_message'][11])\n","print('Lemmatized message:',messages['lemmatized_message'][11])\n","```\n","\n","#Stop Word Removal\n","Stop Word Removal\n","Stop words are commons words that do not add any relevance for classification (For eg. “the”, “a”, “an”, “in” etc.). Hence, it is essential to remove these words.\n","```\n","from nltk.corpus import stopwords\n","def stopword_removal(message):\n","    stop_words = set(stopwords.words('english'))\n","    filtered_sentence = []\n","    filtered_sentence = ' '.join([word for word in message if word not in stop_words])\n","    return filtered_sentence\n","messages['preprocessed_message'] = messages.apply(lambda row: stopword_removal(row['lemmatized_message']),axis=1)\n","Training_data=pd.Series(list(messages['preprocessed_message']))\n","Training_label=pd.Series(list(messages['label']))\n","```\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/167/877/large/0cef24abdee4953a24ac4f53af51b59aff55dccb/stop_words_removal.jpeg)\n","\n","#Why Feature Extraction is important?\n","To perform machine learning on text documents, you first need to turn the text content into numerical feature vectors.\n","\n","In Python, you have a few packages defined under sklearn.\n","\n","We will be looking into a few specific ones used for unstructured data.\n","\n","\n","#Bag Of Words(BOW)\n","\n","Bag of Words (BOW) is one of the most widely used methods for generating features in Natural Language Processing.\n","\n","Representing/Transforming a text into a bag of words helps to identify various measures to characterize the text.\n","\n","Predominantly used for calculating the term(word) frequency or the number of times a term occurs in a document/sentence.\n","\n","It can be used as a feature for training the classifier.\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/173/375/large/ea3a2586a5fcfff917fb8bff373bb06f70f71f71/bags_of_words.jpeg)\n","\n","\n","#Term Document Matrix\n","Term Document Matrix\n","The Term Document Matrix (TDM) is a matrix that contains the frequency of occurrence of terms in a collection of documents.\n","-In a TDM, the rows represent terms and columns represent the documents\n","```\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","tf_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7)\n","Total_Dictionary_TDM = tf_vectorizer.fit(Training_data)\n","message_data_TDM = Total_Dictionary_TDM.transform(Training_data)\n","```\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/167/821/large/2f3e54bd014f3a596c1cc0dae54240472dde9b59/Term_Document.jpeg)\n","\n","#Term Frequency Inverse Document Frequency (TFIDF)\n","Term Frequency Inverse Document Frequency (TFIDF)\n","In a Term Frequency Inverse Document Frequency (TFIDF) matrix, the term importance is expressed by Inverse Document Frequency (IDF).\n","\n","IDF diminishes the weight of the most commonly occurring words and increases the weightage of rare words.\n","```\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),min_df = (1/len(Training_label)), max_df = 0.7)\n","Total_Dictionary_TFIDF = tfidf_vectorizer.fit(Training_data)\n","message_data_TFIDF = Total_Dictionary_TFIDF.transform(Training_data)\n","Let's take the TDM matrix for further evaluation. You can also try out the same using TFIDF matrix.\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"srbJIUhwzlyz","colab_type":"text"},"source":["#How Does a Classifier Work?\n","How Does a Classifier Work?\n","The following are the steps involved in building a classification model:\n","\n","Initialize the classifier to be used.\n","\n","Train the classifier - All classifiers in scikit-learn uses a fit(X, y) method to fit the model(training) for the given train data X and train label y.\n","\n","Predict the target - Given an unlabeled observation X, the predict(X) returns the predicted label y.\n","\n","Evaluate the classifier model - The score(X,y) returns the score for the given test data X and test label y.\n","\n","#Train and Test Data\n","The code snippet provided here is for partitioning the data into train and test for building the classifier model. This split will be used to explain classification algorithms.\n","```\n","from sklearn.model_selection import train_test_split#Splitting the data for training and testing\n","train_data,test_data, train_label, test_label = train_test_split(message_data_TDM, Training_label, test_size=.1)\n","```\n","\n","#Decision Tree Classification\n","Decision Tree Classification\n","It is one of the commonly used classification techniques for performing binary as well as multi-class classification.\n","\n","The decision tree model predicts the class/target by learning simple decision rules from the features of the data.\n","\n","```\n","from sklearn.tree import DecisionTreeClassifier#Creating a decision classifier model\n","classifier=DecisionTreeClassifier() #Model training\n","classifier = classifier.fit(train_data, train_label) #After being fitted, the model can then be used to predict the output.\n","message_predicted_target = classifier.predict(test_data)\n","score = classifier.score(test_data, test_label)\n","print('Decision Tree Classifier : ',score)\n","```\n","\n","#Stochastic Gradient Descent Classifier\n","Stochastic Gradient Descent Classifier\n","It is used for large scale learning\n","\n","It supports different loss functions & penalties for classification\n","```\n","seed=7\n","from sklearn.linear_model import SGDClassifier\n","classifier =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","score = classifier.score(test_data, test_label)\n","print('SGD classifier : ',score)\n","```\n","\n","#Support Vector Machine\n","Support Vector Machine\n","Support Vector Machine(SVM) is effective in high-dimensional spaces.\n","\n","It is effective in cases where the number of dimensions is greater than the number of samples.\n","\n","It works well with a clear margin of separation.\n","```\n","from sklearn.svm import SVC\n","classifier = SVC(kernel=\"linear\", C=0.025,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","score = classifier.score(test_data, test_label)\n","print('SVM Classifier : ',score)\n","```\n","\n","#andom Forest Classifier\n","Random Forest Classifier\n","Controls over fitting\n","\n","Here, a random forest fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy.\n","```\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","score = classifier.score(test_data, test_label)\n","print('Random Forest Classifier : ',score)\n","7 of 8\n","```\n","\n","#Model Tuning\n","The classification algorithms in machine learning are parameterized. Modifying any of those parameters can influence the results. So algorithm/model tuning is essential to find out the best model.\n","\n","For example, let's take the Random Forest Classifier and change the values of a few parameters (n_ estimators,max_ features)\n","\n","```\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(max_depth=5, n_estimators=15, max_features=60,random_state=seed)\n","classifier = classifier.fit(train_data, train_label)\n","score=classifier.score(test_data, test_label)\n","print('Random Forest classification after model tuning',score)\n","Refer scikit-learn tutorials and try to change the parameters of other classifiers and analyze the results.\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s6-0kkEB0h5i","colab_type":"text"},"source":["#Partitioning the Data\n","It is a methodological mistake to test and train on the same dataset. This is because the classifier would fail to predict correctly for any unseen data. This could result in overfitting.\n","\n","To avoid this problem,\n","\n","Split the data to train set, validation set and test set.\n","\n","Training Set: The data used to train the classifier.\n","\n","Validation Set: The data used to tune the classifier model parameters i.e., to understand how well the model has been trained (a part of training data).\n","\n","Testing Set: The data used to evaluate the performance of the classifier (unseen data by the classifier).\n","\n","This will help you know the efficiency of your model.\n","\n","#Cross Validation\n","Cross validation is a model validation technique to evaluate the performance of a model on unseen data (validation set).\n","\n","It is a better estimate to evaluate testing accuracy than training accuracy on unseen data.\n","\n","Points to remember:\n","\n","Cross validation gives high variance if the testing set and training set are not drawn from same population.\n","\n","Allowing training data to be included in testing data will not give actual performance results.\n","\n","In cross validation, the number of samples used for training the model is reduced and the results depend on the choice of the pair of training and testing sets.\n","\n","You can refer to the various CV approaches here.\n","\n","#Stratified Shuffle Split\n","The StratifiedShuffleSplit splits the data by taking an equal number of samples from each class in a random manner.\n","\n","StratifiedShuffleSplit would suit our case study as the dataset has a class imbalance which can be seen from the following code snippet:\n","```\n","seed=7\n","from sklearn.model_selection import StratifiedShuffleSplit\n","###cross validation with 10% sample size\n","sss = StratifiedShuffleSplit(n_splits=1,test_size=0.1, random_state=seed)\n","sss.get_n_splits(message_data_TDM,Training_label)\n","print(sss)\n","test_size=0.1 denotes that 10 % of the dataset is used for testing.\n","```\n","\n","#Stratified Shuffle Split Contd...\n","This selection is then used to split the data into test and train sets.\n","```\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn import svm\n","classifiers = [\n","    DecisionTreeClassifier(),\n","    SGDClassifier(loss='modified_huber', shuffle=True),\n","    SVC(kernel=\"linear\", C=0.025),\n","    KNeighborsClassifier(),\n","    OneVsRestClassifier(svm.LinearSVC()),\n","    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=10),\n","   ]\n","for clf in classifiers:\n","    score=0\n","    for train_index, test_index in sss.split(message_data_TDM,Training_label):\n","       X_train, X_test = message_data_TDM [train_index], message_data_TDM [test_index]\n","        y_train, y_test = Training_label[train_index], Training_label[test_index]\n","        clf.fit(X_train, y_train)\n","        score=score+clf.score(X_test, y_test)\n","    print(score)\n","\n"," ```   \n","The above code uses ensemble of classifiers for cross validation. It helps to select the best classifier based on the cross validation scores. The classifier with the highest score can be used for building the classification model.\n","\n","Note: You may add or remove classifiers based on the requirement."]},{"cell_type":"markdown","metadata":{"id":"fS8W37nH07n6","colab_type":"text"},"source":["#Classification Accuracy\n","The classification accuracy is defined as the percentage of correct predictions.\n","\n","```\n","from sklearn.metrics import accuracy_score\n","print('Accuracy Score',accuracy_score(test_label,message_predicted_target))  \n","classifier = classifier.fit(train_data, train_label)\n","score=classifier.score(test_data, test_label)\n","test_label.value_counts()\n","This simple classification accuracy will not tell us the types of errors by our classifier.\n","```\n","It is just an easier method, but it will not give us the latent distribution of response values.\n","\n","#Confusion Matrix\n","It is a technique to evaluate the performance of a classifier.\n","\n","It depicts the performance in a tabular form that has 2 dimensions namely “actual” and “predicted” sets of data.\n","\n","The rows and columns of the table show the count of false positives, false negatives, true positives and true negatives.\n","\n","from sklearn.metrics import confusion_matrix\n","print('Confusion Matrix',confusion_matrix(test_label,message_predicted_target))\n","The first parameter shows true values and the second parameter shows predicted values."]},{"cell_type":"code","metadata":{"id":"6pPSZNnF1OHu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
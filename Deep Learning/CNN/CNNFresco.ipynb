{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNNFresco.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RFMhLVuEAybu","colab_type":"text"},"source":["Image recognition is one of the prominent applications of CNN. It is used to classify images, identify faces, patterns and other aspects of visual data.\n","\n","CNN works on the underlying assumption that data is represented in rows and columns. Since image data is a matrix of the pixel data, CNN works best on image data.![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/762/large/75db4a9386de766ff67ac9b1602da174e0ba1b34/Image_recognitionfla.jpeg)\n","\n","CNN in Speech Recognition\n","Any audio data can be represented as a spectrogram which is analogous to image data. As a result, the same techniques of image recognition can be applied to these spectrograms to work with audio data.\n","\n","CNN is used for speech-to-text translation and real-time speech translation to other languages.\n","\n","CNN in NLP\n","Any text data can be represented as a matrix where each row represents a word or character, and the column represents the documents.\n","\n","The matrix could be a word embeddings like DTM, Doc2Vec or GloVe.\n","\n","This matrix representation is similar to image data upon which CNN is used.\n","\n","Sentiment Analysis, Spam Detection or Topic Categorization are common areas of NLP where CNN is used.\n","\n","\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/771/large/cc9cddeb42c1c6b26aea67e6b418ad773da645ec/GrayScale.jpeg)\n","\n","\n","A digital colour image comprises three channels of red, blue and green.\n","Each channel is a matrix of pixel intensities ranging from 0 to 255 where 255 is the dark intensity, and 0 is the bright intensity.\n","A grayscale image is one which has only one channel where each pixel value represents different shades of gray.\n","\n","\n","\n","\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/617/025/original/ab8e1b986bab08885c011f3c6947aff5335a7a7c/one_d_rep.gif?Expires=1577496295&Signature=T~5Qs8x7I9WOj5pgQ-KYa10IfTymv0Fu4Xy0SPCP~OYoqeEzHrrC-tMKjYbk8OQKheNCb~H93egCA4vxi8CfQiyA0owajMfdp3-Hq~jx9c1DgHa5s1f-rtWZYE7tGsG6hToxN4wDtbylXUK4ULXULXxel~lXEF~~ziQ21pgEF~AJ1blRO8REZw0bGNkPBr0CUiDCWxla1AAAYYhCn5eTkS9sysX-CRZfW2YrS-TCcPxePxUE60s0gv4uc~82xCyFgWpM8HhdAfDsU9n~ymQZ1z4hbe79~KuvTNETdr8KZjwo1cyl~ztgZqQ0pvP12UyMy~7beqLgAqtuhXezFYqU7Q__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","\n","One approach to train the model to classify an image is to\n","\n","flatten the matrix into a one-dimensional vector (by resizing) and then\n","\n","feed it to a fully connected neural network.\n","\n","\n","#1D Representation - Drawback\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/787/original/5975036f112600a1c1620d1dc0d48e1c170ad00a/limitation.gif?Expires=1577496375&Signature=Dmh0H5hUGyG8sOBkB8MYARhfUu~s1LiDk7GJqXEeMMdwPCEooexkfYhwYs8oDjpQ77IPQQJopaC~s~T3udPerhmWrNQ-Mecd95f-IuX8d6VK1GnZGuJUislPEfEZNlI5DTSWv~W6bsMbLyYGvpaRBRxZV-HGzrtGUuJ5sGOGFneHYX5rJ1TF-E4gmtP~8R~u0ZfDoCuTfXW6doJ357iZnZ0kja4wdbsNd4VV1E6rgvH9eTOOoTkfV1w60GENz9Ih9K6q6f9vWvg8d2Zg1uqS1fghiQFN0zZBms-q2nsf8QYvKEUYxT8gD2zJG7pphsaJM9tlqPU1vFehtSKL~HdklA__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","\n","But what if the object is at a different position within the image?\n","\n","If you have trained the model with images where the object of interest is at one particular position; say exactly at the center then model fails to predict the same object in test set if it is positioned at the corner of the image.\n","\n","\n","\n","\n","\n","\n","Brute Force Approach\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/785/original/4127376f5f88d9a1282bbd2c7dd7fa825b8886d7/Brute_force.gif?Expires=1577496452&Signature=UJSir~UDTlOp5Pa6PfFp8CKKIBX~p~KJLcVF8bazVdZdLCfURFVYIoaGjeUhP~428HvFYpmjdmznSku7HWzL2nu~pvQuXotOvWpuqaELYc-hqzqPn6e5YU-u39ncwOOTzI8x8ob2cKBW-8-BjRXDJitX-puWInoyONJA5F29c7nC8dexNHAa72y6T1908uQ-sHOcC-pIb1vfWtFy-aPxlAuhaL9tCg9arIOA6SXL6YxrDj-~ZVpYeA9iFY0~fYlFNHETKpmnc5vjwM7myEBLlZUcMpHCKyOegz9R~qZY5-IH8t38i2XglaWcmfl7L08Na3~kNg6KO37Moh3M3e7Nxw__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","Brute Force Approach\n","In this approach, the network is trained similarly to the previous approach, but during prediction, the test image is scanned frame by frame and data from each frame is fed to the network for its prediction.\n","\n","This works well if the object is at a different position but we need to use the window size of a different size if in case the object doesn't fit within one window.\n","\n","\n","\n","\n","#Brute Force Limitations\n","Brute force method works good provided the test image fits into the filter, which is used to scan the whole image.\n","\n","If the filter size doesn't match with the image, then it fails to identify the image.\n","\n","One thing you can do is to use filters of different sizes to scan the image, but this turns out to be computationally expensive.\n","\n","\n","#CNN to the Rescue\n","Convolutional neural networks overcome the limitations of previous approaches since their predictions are spatially invariant and independent of the size of the images.\n","\n","They also greatly reduce the number of parameters required to train the network.\n","\n","You will be learning the logic and practical implementation of CNN in the rest of the chapters\n"]},{"cell_type":"markdown","metadata":{"id":"SLJoqAa7HdB4","colab_type":"text"},"source":["#list of Operations\n","Following are the common operations performed on an image when it flows through a CNN network:\n","\n","Striding\n","\n","Convolution\n","\n","Pooling\n","\n","Flattening\n","\n","Feeding to a fully connected network\n","\n","Let's look briefly about the actions performed by these steps in this topic, and you will be learning the technical implementation in the later topics.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/764/large/2ab49ea32a23dd829216a742a07285b812d8f8d9/cat.jpeg)\n","\n","\n","#Striding - Generate Overlapping Frames\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/798/original/87271e9fae54c7208803dc7ab7535b59be7b7ff4/strided_image.gif?Expires=1577496651&Signature=TwsTDE02SceKf9w2RNXL9t7f7-3b~bWQmiKnGis8sF3bblkC87he-e-nh6rnEy3HSQqEXfKq3S0~erXmcXxv3OrDCdS5rxm6EDMbId9E6crsXRKuOCL~kIzOIwgprPpYpMR-k7bVAJyc83c2GOc~lqQXjq1l27AEgzphZfhO8CghCxAoMV4LToq7stiUsUc~x6-MBoomLY8yO7GrvpAS0zcF88fF-waGQXMrTGx4hdZORRQ73zyu~tRGeCDNEfXD7KqHsWKrkH-GKUnLBT8nUY-tR9n9M7CuBmS9nKmfgzBNu9hAp-mllMLVjXfgRSeIwP8oQ~UmLydD2JMj9GqfgA__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","In this step, you break down the images into overlapping frames using a sliding window.\n","\n","The window size is a hyperparameter that needs to be chosen.\n","\n","\n","#Convolution\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/796/original/02ca5c1e51e621fa8232e034e4267b87912c9ec1/convolution.gif?Expires=1577496724&Signature=OmF-0vE-DQ9j59vZWs9u~lngxgFzU8NEeNskKeBLMH1iRTdMmTx5vNXtZ5JsnH0Hu2D9LzG1kNQO8Xv7CfqyZC4~8PF0ObHU5zG3uavpcMGVzguROMRRyQNooV-XjAHmpoMZCCWLJ~nUlPpCRpYygXsnOCrqzv80NKS65Of-xW81cQ2zV3IeQk4~sk34Iqcd5meAssbgH2Kgb5GSCuKxFDjoFcZ5lXCT6sYhY8J33Nx~nOFfCt-qiq7UiECexDhiQ7WY3F3mLo6HVNrRYWIX-LPVSJUwKgBaLB51sVY1s5O4PL6niws3MdumhoLYlvm-E0Qcm~Qry9PjsBfqHgQyIQ__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","\n","Pass data from each frame into a smaller neural network (known as a feature vector) which performs a convolution operation.\n","\n","To keep track of the arrangement of original frames we store the result of each convolution in a grid preserving the same position as in the original image.\n","\n","Note: Observe that we are using the same neural network to perform convolution on all the frames.\n","\n","\n","#Pooling\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/793/original/569bf5a1a7077caf06b16095f4e27853bb8ab7c1/sampling.gif?Expires=1577496821&Signature=hm6m9r-lzLNvf0CDNOHzvYQoW12328ZxxltlM3VBG6h7f~BE9J2ZqRhZ~u15ffulaQ2ECPYui0p7aAKKMkVJviBfvaj5BgQs5X55NQ2-~~kTIZcuVCGuLzy5kNUTofKXrkYTWyySWl15K8ciJiOJbFUkGQP~EBbtpCa-cumhodWTq8puxY-Wo9wxIQhH~ee6yZFTs82XVhUb0cUvHNtdWZ~xsC8FRVFRamBpgWT8KrjhxHYsvbbDNHqamjG0OvwFa5zi3S4qYcMnfDNqhcMmdWNu5Zx1hNA73T5~rsfFwC0G4EbDk5UdoMwl2Bg7UxbFoxe4jU-3Qu~~WfqCIWQgGA__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","Convolution extracts the most notable features from the image, but the resulting matrix is still pretty big.\n","\n","In pooling, we look at nn x nn slice of convolved output (22 x 22 in this case) and select the maximum value (max pooling) or the average of average pooling.\n","\n","Why Pooling?\n","\n","Pooling reduces the input dimension yet preserving the essential features.\n","\n","This what makes the CNN spacially invariant when classifying the images.\n","\n","Pooling also prevents overfitting since it provides generalization by extracting only the notable features.\n","\n","#Flattening\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/795/original/37cbe9e919828d2e0a887ae457c370464e15ab01/flattening.gif?Expires=1577496894&Signature=jaZqVOxRgLWlq0qBbJb-sYd-VgUcGeHbWbftpdHRplMt3MP5bA~7jkwNgeqDL27Nk8OxNLTDhGA42Kflpk8j-cM9wUW7N8seKLzMZXvPFI6OExr2a6Gt6nlm85bfqvObG-Tw9tSJ8GAgS0-ZTQ0QgswHpi~DYS-pyREnppqg2HW0puqyyKU6nJQoPUrJqZKQJoDb6nSnLE76TH20TiLWFAMxKGq3WTF41EXKaZBSqm3vIYGqRsTJKL4amDNU5JwzltupU-uoa2qA91C-9rPyFdEpVxLgWb4G51UYaRc6YlrDSWGJ-eSgsT5BtX~BLF4YHNerwfdNODBknWBwqM1yGg__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","\n","Now we have successfully managed to reduce a giant image into a fairly small array.\n","\n","The next step is to flatten (resizing) this array into one dimension to feed it into the fully connected neural network.\n","\n","#Prediction\n","\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/794/original/6968335e74a481800ba3b7d14e84c4f675da6ca0/predection.gif?Expires=1577496947&Signature=zlqEh8l6VKRam9RvBTWfFAA-QCkxC4X22Q2Btt0td-FJ7Hx2Ene2715knkNx5qY66RQQkgWFc9C0OyezzV7HvNycWsTJWCjR4hXB3pAokZTq8-YmnJ-IIshCtfJfdpNVCGmDc8UWHlL3l8N9NZiXoduzkzrAoLiJP4byKras3cdU6TAEYR7GrASsoge0qQ55rbrbPMR1NPLaLdvZu4JqRMm3huDpKUGDAPZKPIQXDxR1Gki7rnTqlO8FG5fwHDtMhCjVfACQJBK7YZkV71fp2skLa~x3P~fRKojhCQvrgYwhoSkBC4k-~0QWzZyPQ~6nH6DJv-zIXq2jTtx65xLZrg__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)\n","\n","\n","The flattened array is just a bunch of numbers ready to be fed into a fully connected neural network.\n","\n","The final network is the same as what you have learned in the previous courses.\n","\n","So you can think CNN as a series of operations to be performed on an N-dimensional data before feeding it into a fully connected neural network.\n","\n","#Pipeline\n","\n","![alt text](https://docs-secure-cdn.fresco.me/system/attachments/files/002/615/797/original/8e4a9eea8e2a562e1cb6df402c232ff12ecb62ae/stories_so_for.gif?Expires=1577497018&Signature=Mn4TSaJ5tm-FW5qiGbHB3FLlZmnPyZ6YnvRiDSv1xxP4avpNkf3zVyxDCVU-fqp4EVVB2RM~n7kxtg5rnZ7PLh~wNdzG5XeSWW~WohCOooBzAidkWvcKcJerMmE~aA5MqIfsNzco99gClV6PYF219OWdUtcv3p5JSiTc7Ja~DqfuKllJVLF0xh7KMU~H0Nycv0FcS1FP-4nqBTnqYAPaL70OyHQew6R2i3o88eXPNUWvHIQcRfqm~xQ6x3nk62y1QvgGof6BOsLau777xpoMa6JWkOJribs5xustb43Bi1gCHH-JlFTKnWCUkX4G~AviA9zOU8gZL10upqjwWXg5DQ__&Key-Pair-Id=APKAJUTRVJCFRZY3Z43A)"]},{"cell_type":"markdown","metadata":{"id":"jyilQBXOL8p0","colab_type":"text"},"source":["Convolution\n","Convolution in CNN is an element-wise multiplication between two matrices and adding all the elements of the resulting matrix to output a scalar number.\n","\n","Formula:\n","\n","\\small \\begin{matrix}output = \\\\ a_1 \\times w_1 + & a_2 \\times w_2 + & a_3 \\times w_3 + \\\\ a_4 \\times w_4 + & a_5 \\times w_5 + & a_6 \\times w_6 + \\\\ a_7 \\times w_7 + & a_8 \\times w_8 + & a_9 \\times w_9 \\end{matrix}\n","​output=\n","​a\n","​1\n","​​ ×w\n","​1\n","​​ +\n","​a\n","​4\n","​​ ×w\n","​4\n","​​ +\n","​a\n","​7\n","​​ ×w\n","​7\n","​​ +\n","​​ \n","​a\n","​2\n","​​ ×w\n","​2\n","​​ +\n","​a\n","​5\n","​​ ×w\n","​5\n","​​ +\n","​a\n","​8\n","​​ ×w\n","​8\n","​​ +\n","​​ \n","​a\n","​3\n","​​ ×w\n","​3\n","​​ +\n","​a\n","​6\n","​​ ×w\n","​6\n","​​ +\n","​a\n","​9\n","​​ ×w\n","​9\n","​​ \n","​​ \n","\n","if its a 3\\times33×3 matrices convolution\n","\n","Convolution for N-D Array\n","Convolution for N-D Array\n","The previous example shows convolution between two 2D arrays; the same thing can be extended to the 3D array.\n","\n","You can visualize it as a convolution between two matrices that looks like boxes where corresponding elements are multiplied and added together to obtain a scalar.\n","\n","When you deal with images, you use this kind of convolution.\n","\n","#Observe the GIF to understand how the strided convolution is performed.\n","\n","We slide the orange matrix (known as the filter) over the original matrix (in green) by one row (or column), and for each position, convolution is performed.\n","\n","The output (known as feature map) of each convolution is stored in another matrix where the position of each element is preserved.\n","\n","```\n","A = np.random.randint(0,10, size= (3,3,3))\n","W = np.random.randint(0,10, size= (3,3,3))\n","\n","scalar = np.sum(np.multiply(A, W)) ### numpy implementation to perform convolution\n","\n","print(scalar)\n","688\n","```\n","\n","\n","\n","\n","#CNN Terminologies\n","\n","\n","1. Filter, Kernel, or Feature Detector - The small matrix that you slide over actual data to perform convolution.\n","\n","2. Stride - The number of row or column by which the filter is slid after one convolution.\n","\n","3. Receptive field - The local region of the input data over which convolution is performed (same as filter size).\n","\n","4. Convolved Feature, Activation Map or Feature Map - The output that you obtain after performing strided convolution.\n","\n","#Output Dimension\n","Once you have performed the convolution what would be the dimension of feature map?\n","\n","If\n","\n","input dimension is an n \\times nn×n matrix\n","the filter is of size f \\times ff×f\n","s is the stride\n","then the dimension of the feature map will be\n","\n","(\\frac{n - f}{s} + 1) \\times (\\frac{n - f}{s} + 1)(\n","​s\n","​\n","​n−f\n","​​ +1)×(\n","​s\n","​\n","​n−f\n","​​ +1)\n","\n","For example, convolving 7 x 7 matrix with 3 x 3 filter and stride = 2 we will get 3 x 3 output.\n","\n","Pattern Detection\n","Now you know how to perform convolution operation, but what is its actual use case?\n","\n","The answer is Pattern detection\n","\n","\n","For example, let's say we have an actual image with two contrasting colors, dark on the left and white on the right separated by a vertical edge.\n","\n","From the GIF, you can observe that on convolving the actual image with a 3 x 3 filter, the resulting feature map shows a vertical edge between two contrasting patterns.\n","\n","The filter chosen is a vertical edge detector having a vertical patch between two different patterns.\n","\n","Filters applied to the real image.\n","\n","You will be soon implementing the same in hands-on!\n","\n","\n","#Padding\n","Consider you have a 7 x 7 input and a 3 x 3 filter and stride = 1, then the output dimension after convolution would be a 5 x 5 feature vector.\n","\n","If stride = 4, then the dimension of the feature vector would reduce to 2 x 2 which is of no use for detecting patterns.\n","\n","To preserve the size of the image after convolution, a technique called zero padding is performed.\n","\n","#Zero Padding\n","You can imagine the output after zero padding as original image with few borders of zero around it.\n","\n","If the image (or input data) is of dimension 32 x 32 x 1 and if you want convolution output to be of the same dimension of input then you have to pad two borders of zeros around the image symmetrically.\n","\n","For a filter dimension ff, the padding p = \\frac{f-1}{2}p=\n","​2\n","​\n","​f−1\n","​​  in order t preserves the dimension of output image provided striding = 1.\n","\n","Final output dimension = \\small(\\frac{n - f + 2p}{s} + 1)(\n","​s\n","​\n","​n−f+2p\n","​​ +1) x \\small(\\frac{n - f + 2p}{s} + 1)(\n","​s\n","​\n","​n−f+2p\n","​​ +1)\n","\n","Calculate Output size by this formula;;;\n","\n","Tensor size or shape:(width =28, height=28)\n","convolution filter size(F): (F_width=5, F_heigth=5)\n","Padding(P)=0\n","Stride(S):1\n","\n","Using equation: output_width = ((W-F+2*P)/S)+1\n","eg ((28-5+2*0)/1)+1  = 24\n","Hence,\n","output dim = (24,24)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bo9ZwnDvYUWR","colab_type":"text"},"source":["Soon after convolution, the feature map is still large, to reduce the size of the output as a technique called pooling is performed.\n","\n","In pooling, we take a filter of size p x p and stride over the input. On each stride, we either take a maximum value or the average of values covered by the filter.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/669/230/large/058f48e8e214b1acc480144649072aa3c3a66560/pooling.jpeg)\n","\n","As you can infer from the image, pooling has reduced the dimensions of the image yet preserving the important feature to detect the image.\n","\n","This reduces the number of parameters for the network to learn which significantly boosts the speed of training.\n","\n","Pooling also reduces overfitting by extracting only the notable features from the network.\n","\n","Note that the filter is empty, there aren't any parameters in the filter for the network to learn.\n","\n","Role of Neural Network\n","Enough of Image Processing\n","\n","Until now, we only dealt with simple filters. If a machine needs to detect a human face, it has to be provided with filters that will scan all over the image to detect complex patterns (like eyes, ears and other obvious patterns) to predict with confidence that it's a human face.\n","\n","Now the role of the neural network is to learn to build these complex filters by providing the labeled data.\n","\n","Padding - NumPy\n","```\n","def zero_pad(data, pad):\n","  data_pad = numpy.pad(array = data, pad_width = ((0,0),(pad,pad), (pad,pad), (0,0)), mode = 'constant', constant_values = 0)\n","  return data_pad\n","The above numPy.pad() command adds constant value zero along the rows and columns of image data.\n","```\n","\n","Strided Convolution Using NumPy\n","A detailed explation of this code is provided in the next card.\n","\n","```\n","def conv_forward(A_prev, W, b, hparams):\n","  stride = hparams[\"stride\"]\n","  pad = hparams[\"pad\"]\n"," \n","  m, h_prev, w_prev, c_prev = A_prev.shape\n","   \n","  number of channels in the input\n","  f, f, c_prev, n_c = W.shape\n","  \n","  n_h = int((h_prev - f + 2*pad)/stride) + 1\n","  n_w = int((w_prev - f + 2*pad)/stride) + 1\n","  \n","  Z = np.zeros((m, n_h, n_w, n_c))\n","  A_prev_pad = zero_pad(A_prev, pad)\n","  for i in range(m):\n","    for h in range(n_h):\n","      for w in range(n_w):\n","        for c in range(n_c):\n","           w_start = w * stride\n","           w_end = w_start + f \n","           h_start = h * stride\n","           h_end = h_start + f\n","        \n","           Z[i,h,w,c] = conv_single_step(A_prev_pad[i, h_start:h_end, w_start:w_end, :], W[:,:,:,c], b[:,:,:,c])\n","  return Z\n","```\n","\n","\n","Strided Convolution - Program Flow\n","Amount of stride and padding are retrieved from the dictionary hparams.\n","\n","m, h_prev, w_prev, c_prev are the dimensions of input.\n","\n","f, f, c_prev, n_c is the size of the filter, c_prev refers to the number of channels in input data and n_c refers to the number of channels in the filter.\n","\n","n_h, n_w hight and width of output (also known as feature map)\n","\n","The feature map z is initialized to zeros.\n","\n","The input is padded with zeros.\n","\n","Each cell in the feature map is filled with corresponding convolution operation result.\n","\n","w_start, w_end, h_start and h_end defines position of filter at each iteration.\n","w_start = w * stride, w_end = w_start + f, h_start = h * stride, h_end = h_start + f moves the filter by the specified number of strides.\n","\n","\n","Pooling - NumPy Implementation\n","The below method performs max pooling on the given input using filter size and padding provided by hparam\n","\n","\n","```\n","def max_pool(input, hparam):\n","    m, h_prev, w_prev, c_prev = input.shape\n","    f = hparam[\"f\"]  ## f is the filter size to use for pooling\n","    stride = hparam[\"stride\"]\n","    h_out = int(((h_prev - f)/stride) + 1)\n","    w_out = int(((w_prev -f)/stride) + 1)\n","    output = np.zeros((m, h_out, w_out, c_prev))\n","    for i in range(m):\n","        for c in range(c_prev):\n","            for h in range(h_out):\n","                for w in range(w_out):\n","                    w_start = w * stride\n","                    w_end = w_start + f\n","                    h_start = h * stride\n","                    h_end = h_start + f\n","                    output[i, h, w, c] = np.max(input[i,h_start:h_end, w_start:w_end, c])\n","    print(output.shape)\n","    assert output.shape == (m, h_out, w_out, c_prev)\n","    return output\n","```\n","\n","Applying Activation\n","Soon after performing convolution, each element is of convolved output is activated by applying an activation function.\n","\n","Relu activation is widely used in case of CNN.\n","\n","You will learn more about using activations in the TensorFlow implementation of CNN."]},{"cell_type":"markdown","metadata":{"id":"etzvN9fcEkql","colab_type":"text"},"source":["#CNN Routine\n","Until now, you have learned how to implement padding, strided convolution and pooling, in CNN terms usually the pipeline of these three steps is together referred to as one convolution step.\n","\n","The output of one convolution step is known as a feature vector. This feature vector becomes the input to the next convolution step.\n","\n","Likewise, a series of convolutions are performed before feeding to fully connected neural network.\n","\n","\n","\n","#Le-Net 5\n","Le-Net5 is a basic CNN architecture from which current day architectures evolved.\n","\n","It consists of the following layers:\n","INPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC\n","\n","It was first used on MNIST data set to recognize written digits.\n","\n","It has the least number of parameters to learn compared to modern architectures.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/759/large/f538fa52230c285f08b30042813490948836f1ea/Alexnet.jpeg)\n","\n","The table shows the output dimensions and number of parameters to learn at each step of convolution for Le-Net5.\n","\n","#Observation\n","The only function of pooling layer is to reduce the size of the feature vector hence it has no parameters to learn.\n","\n","The size of the input data diminishes as it passes through the convolution layers.\n","\n","The total number of parameters is much less than a fully connected network without convolution operations.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KTK12kzzFJTz","colab_type":"text"},"source":["#CNN - Architectures\n","Before building a CNN network from scratch, it's better to use the architectures that have been previously trained and tested.\n","\n","Most of the deep learning framework provides these trained models that you can retrain them on your data.\n","\n","The training speed will be much faster as these pre-trained models have already learned to recognize most of the features.\n","\n","Some of the well-known architectures are LeNet5, AlexNet, VGG-16, and Inception net.\n","\n","LeNet5 has already been covered in the previous topic. Let's discuss in detail the rest of the architectures in this topic.\n","\n","#AlexNet\n","AlexNet was designed to recognize complex image data.\n","\n","It was trained to recognize over thousand image labels.\n","\n","It is much more complicated than LeNet in terms of the number of processing layers and number of trainable parameters.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/826/large/ea366bea63cae870b1a9418809c34a54628fa60f/VGG_table.jpeg)\n","The table shows the configuration of AlexNet. All the images are resized to 227 x 227 x 3 before feeding to the network.\n","\n","\n","#VGG-16\n","VGG net is similar to AlexNet with comparatively more filters.\n","\n","It is currently the most preferred choice in the community for extracting features from images.\n","\n","The weight configuration of the VGGNet is publicly available and has been used in many other applications and challenges as a baseline feature extractor.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/760/large/af40ac8b4ed629c7896350b303a92c2c9d3794ee/vgg_16.jpeg)\n","\n","The image shows the configuration of VGG-16 network.\n","\n","#ResNet\n","As the network goes deep, it encounters vanishing gradient problems.\n","\n","This problem is profound in CNN. To encounter this problem, a CNN architecture called ResNet makes use of a concept called the residual network.\n","\n","#Residual Network\n","For a given layer ll we have:\n","\\small a^{[l]} = g(z^{[l]})a\n","​[l]\n","​​ =g(z\n","​[l]\n","​​ )\n","\\small a^{[l+1]} = g(z^{[l+1]})a\n","​[l+1]\n","​​ =g(z\n","​[l+1]\n","​​ )\n","\\small a^{[l+2]} = g(z^{[l+2]})a\n","​[l+2]\n","​​ =g(z\n","​[l+2]\n","​​ )\n","\n","In case of a residual network, the activation from one layer is fed to another layer deep into the network.\n","\n","So \\small a^{[l+2]} = g(z^{[l+2]} + a^{[l]})a\n","​[l+2]\n","​​ =g(z\n","​[l+2]\n","​​ +a\n","​[l]\n","​​ ), if in case \\small z^{[l+2]}z\n","​[l+2]\n","​​  tends to zero due to vanishing gradient problem, then \\small a[l+2] = a[l]a[l+2]=a[l]. Hence network can still learn from the information of previous layers.\n","\n","\n","#Inception Network\n","Inception network uses multiple filters in the same convolution layer and concatenates the convolution operation of each filter.\n","\n","This allows the network to learn more useful features without increasing the number of layers.\n","\n","Swipe next to get a better understanding of this network.\n","\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/758/large/b8e1afad66a9800d7c0de8ef7a0a63a71a14ca08/Inception_model.jpeg)\n","\n","The image shows one convolution layer of inception network known as inception module.\n","\n","You can see that multiple filters are used on same input, and the result is concatenated to obtain one big block of a feature vector.\n","\n","Note that the padding is kept same to match the dimension while performing concatenation.\n","\n","#Reducing Computation Cost\n","As you saw in the previous image, one inception module resulted in a feature vector of size 28 x 28 x 256 (i.e., after concatenation).\n","\n","In the next layer, if you were to perform convolution with 5 x 5 filter of 32 channels, the number of computations would be (28 x 28 x 256) x (5 x 5 x 32) closely equal to 160 million.\n","\n","To reduce this huge computation cost, a 1x1 convolution is performed on the output to reduce the dimension of the feature vector.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/002/615/756/large/0b8fcb2e16c6908c457bd23286632c0142bb4c86/Expensive_operation.jpeg)\n","\n","\n","#1x1 Convolution\n","\n","As shown in the image, the input is convolved with a 1 x 1 x 256 filter of size 16 channels whose output is called as a bottleneck layer.\n","\n","The number of computations to obtain the bottleneck layer will be (28 x 28 x 256) x (1 x 1 x 16) = 3.2 million.\n","\n","When the bottleneck layer is convolved with 5 x 5 filter of 32 channels, it results in closely 10 million computation, so the total number of computation will be 13.2 million which is much less than 160 million, you saw in the previous card."]},{"cell_type":"markdown","metadata":{"id":"KPC9wJULJ4Uk","colab_type":"text"},"source":["Initializing Weights for Filter\n","\n","```\n"," W = tf.Variable(tf.random_normal([5,5,3,32]))\n"," b = tf.Variable(tf.random_normal([32]))\n"," ```\n","\n","\n","The above code initailzes 32 filters with random weights each of dimension 5 x 5 x 32, where b is the bias parameter, which is equal to the number of filters.\n","\n","\n","\n","onvolution Operation\n","\n","```\n","conv = tf.nn.conv2d(input = input_, filter = W, stride = [1,stride,stride,1], padding=\"SAME\")\n","conv_bias = tf.nn.bias_add(conv, b)\n","conv_out = tf.nn.relu(conv_bias)\n","```\n","The first step performs the convolution operation on the input using filter W.\n","The parameter stride represents the sliding window for each dimension of the input, the first dimension provides the number of samples to stride over, and the last dimension represents the number of channels to stride over in each input. Since we don't want to skip the samples as well as channel information, they are always kept one.\n","\n","\n","The padding = \"SAME\"; make sure the output dimension after convolution remains the same.\n","\n","\n","\n","Convolution Operation\n","The second step adds the bias term for each of the elements of convolved output.\n","\n","The third step applies the activation function for each element to output a feature vector.\n","\n","\n","#Max Pooling - TensorFlow\n","```\n","conv_pool = tf.nn.pool(input = conv_out, window_shape = [3,3], padding=\"VALID\", pooling_type=\"MAX\")\n","```\n","The above code performs max pooling on input over a 3x3 window size.\n","\n","Padding = \"Valid\" dosn't perform any zero padding hence the dimension of input shrinks after pooling. Padding = \"SAME\" retained the input dimensions after pooling.\n","\n","pooling_type refers to type of pooling, \"MAX\" for max pooling and \"AVG\" for average pooling.\n","\n","\n","#Story So Far\n","All the concepts you have seen so far can be considered as preprocessing performed on the data before feeding to a fully connected neural network.\n","\n","CNN is mostly used where you need to learn patterns from the data not only for images but for speech and text as well."]},{"cell_type":"code","metadata":{"id":"Y-x_01l9An33","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ComputerVision.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6CG60lqDU/FOii+ZKy0S2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gKI1B7HNgo7l","colab_type":"text"},"source":["#Background\n","Before you begin with this course, it is assumed you have completed the course Convolutional Neural Networks.\n","You are now familiar with the concepts on convolution, striding, padding, pooling, and object classification.\n","Going forward you will be using these concepts not only to classify different objects in the same image but also to locate them.\n","\n","#Computer Vision\n","Computer vision is an application of deep learning that enables a computer to see, identify and locate the objects in an image (or videos) in the same way a human does.\n","One of the important applications of Computer vision is self-driving cars wherein the machine learns to identify what is in front of it to decide on its next move.\n","Other applications include face recognition, video surveillance, gesture recognition, etc.\n","\n","\n","#To be Covered\n","In this course, you will be learning the state of art approach for object detection called \n","\n","**YOLO algorithm.**\n","\n","Face recognition using Siamese network.\n","Transfer learning - A way to customize the pre-trained model to your application.\n"]},{"cell_type":"markdown","metadata":{"id":"ve406ZCFg1hw","colab_type":"text"},"source":["#Introduction\n","Object detection algorithm is slightly different from CNN network that you have learned so far.\n","Here, we eliminate the final fully connected layer as demonstrated in next card.\n","\n","\n","#FC to Conv Layer\n","FC to Conv Layer\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/709/large/f95169c305dea420db34e55104af34ff87c14914/FC_to_Conv_Layer.jpeg)\n","\n","\n","In this approach, we replace the fully connected layer of a CNN model with a 1x1 convolution layer as shown in the figure.\n","\n","\n","#Sliding Window\n","Sliding Window\n","Let say that we have a test image of 16 x 16 x 3.\n","\n","Using the previous model trained on 14 x 14 x 3 image, we scan the test image by sliding by two pixels.\n","\n","\n","#Sliding Window Drawback\n","As observed in the previous card, the CNN model has to run four times to scan the full image.\n","\n","If the test image is of size 1000 x 1000, we need to run the model number of times, and since we are sliding by only a few pixels, the model scans the same area redundantly.\n","\n","To overcome this, we convolutionally implement the sliding window as shown in the next card.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/708/large/fdec626aeabccbb96d453804897b33f744a299b5/Conv_Implementation.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"Tq2qQkV7DDp3","colab_type":"text"},"source":["#Object Localization\n","Object Localization\n","Unlike classification networks such as ResNets or VGG net, the object detection algorithm has to identify multiple objects and specify their exact location as shown in the image.\n","This property of predicting the bounding boxes around the objects is known as object localization.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/481/399/large/0d6eef16ccbc80d79ffeb771f3a4c550fb307831/Object_localization.jpeg)\n","\n","#Grid Cells\n","Grid Cells\n","Object localization needs to predict the height, width and location of bounding box around the image.\n","Before specifying the bounding box attributes of each object the image is divided into (S \\times S)(S×S) grid cells as shown in the picture.\n","If the centre of the object falls on in a grid cell then that grid cell is responsible for predicting the object.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/481/398/large/9948cc4900322c9c128529fdad6066138d8c1848/Grid_cell.jpeg)\n","\n","#Bounding Box\n","Bounding Box\n","Each bounding box are defined by attributes centre $$(b_x, b_y)(b\n","​x\n","​​ ,b\n","​y\n","​​ )$$, height $b_hb$\n","$​h$\n","​​  and width $b_wb\n","​w$\n","​​ , each of which lies in the range 0 to 1.\n","\n","The image shows the computation of the bounding box attributes. Note that the calculation of $$(b_x, b_y)(b\n","​x\n","​​ ,b\n","​y\n","​​ )$$are relative to the center grid cell.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/481/400/large/c0a45339323ea3d2b1e2a9f906a13cc6d40762c7/Bounding_box.jpeg)\n","\n","#Target Label\n","The target label yy defines each of the grid cells.\n","\n","$y$ is a vector given by $$y = \\begin{bmatrix}p \\\\ b_x \\\\ b_y\\\\ b_h\\\\ b_w \\\\ c_1 \\\\ c_2 \\vdots \\\\ c_n\\end{bmatrix}\n","​⎣\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎢\n","​⎡\n","​​ \n","​p\n","​b\n","​x\n","​​ \n","​b\n","​y\n","​​ \n","​b\n","​h\n","​​ \n","​b\n","​w\n","​​ \n","​c\n","​1\n","​​ \n","​c\n","​2\n","​​ ⋮\n","​c\n","​n\n","​​ \n","​​ \n","​⎦\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎥\n","​⎤\n","​​  .$$\n","\n","$p$ is known as object confidence that gives the probability of the presence of an object in the bounding box.\n","\n","c_1, c_2 ... c_n is the class confidence intervals For example, if you have one of the two classes to identify pedestrian or a car, then c_1 gives the probability that the grid cell has a car and c_2 gives the probability of the presence of a pedestrian.\n","\n","\n","\n","#Object Confidence vs Class Confidence\n","Note the object confidence \\bold pp is different from that of the class confidence \\bold cc.\n","\n","1. p is the probability of the presence of an object within the bounding box irrespective of the class of object.\n","2. c is the probability of the object belonging to a particular class under the probability p. \n","\n","\n","#IOU\n","Intersection over union(IOU) is a measure of the accuracy of the predicted bounding box against the ground truth box (the actual bounding box).\n","\n","It is the ratio of area covered by the intersection of ground truth box and predicted box to the area covered by the union of these to boxes.\n","\n","The maximum possible value of IOU is 1. If the measured IOU is greater than the set threshold, we can conclude that predicted bounding box is close to the ground truth box.\n","\n","\n","#Sharing Centers\n","Sharing Centers\n","Consider a case where two objects share the same center as shown in the image.\n","\n","If the two objects remain in the same bounding box then our model ends up predicting one of the objects which should not be the case.\n","\n","Swipe next to see how the concept of anchor boxes eliminates this problem.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/481/395/large/43c8a39a40df8367d060b3e4a6d0dcc137569677/Sharing_centers.jpeg)\n","\n","\n","#Anchor Boxes\n","Anchor Boxes\n","Any object has its own height and width ratios, for example in the image, the car is defined by the horizontal box and the pedestrian is defined by the vertical box.\n","\n","These predetermined boxes are known as anchor boxes.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/481/394/large/2e6b7767423d4512ca247ee2438790f338b75a07/Anchor_boxes.jpeg)\n","\n","Anchor Boxes\n","Assuming the object in each grid cell can either fit in one of two fixed anchor boxes and we have one of the two class of objects to be identified, then the target label y is defined as\n","$$\\small y = \\begin{bmatrix}p_1 \\ b_{x1} \\ b_{y1} \\ b_{h1} \\ b_{w1} \\ c_{11} \\ c_{12} \\ p_2 \\ b_{x2} \\ b_{y2} \\ b_{h2} \\ b_{w2} \\ c_{21} \\ c_{22} \\end{bmatrix}y=[\n","​p\n","​1\n","​​  b\n","​x1\n","​​  b\n","​y1\n","​​  b\n","​h1\n","​​  b\n","​w1\n","​​  c\n","​11\n","​​  c\n","​12\n","​​  p\n","​2\n","​​  b\n","​x2\n","​​  b\n","​y2\n","​​  b\n","​h2\n","​​  b\n","​w2\n","​​  c\n","​21\n","​​  c\n","​22\n","​​ \n","​​ ]$$\n","\n","In general, if image is divided in S $$\\times SS×S$$ grid and each grid is defined by B number of bounding box and each box is responsible for predicting C number of classes then the dimension target, $y$ is $$\\small S \\times S \\times (B*(5 + C))S×S×(B∗(5+C)).$$\n","\n","\n","#Hand Engineering\n","The dataset to train the object detection model is slightly different from the object classification model.\n","\n","Each image in the training data for object detection is divided manually into S x S grid cells.\n","\n","If the center of the object of interest falls in a grid cell and fits in one of the anchor boxes, then p-value of that anchor box and the class value of that object are set to 1 along with the bounding box attributes."]},{"cell_type":"markdown","metadata":{"id":"OAuA995KHMBs","colab_type":"text"},"source":["#Overview\n","In You Only Look Once (YOLO) algorithm, you run the image through a CNN model and detect the object through a single pass.\n","\n","This algorithm identifies multiple bounding boxes for the same object. Hence, we use a method called non-max suppression to filter out single prediction box for each object in the image. Rest of the cards show you step by step procedure of how YOLO algorithm works.\n","\n","#Yolo-v2 Configuration\n","Yolo-v2 Configuration\n","YOLO architecture recognizes objects falling in 7x7 grid cells.\n","\n","\n","#Yolo Output\n","Yolo Output\n","Once you pass an image through a YOLO model, the output will have lots of overlapping predictions across adjacent grid cells as shown in the image.\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/706/large/35096bd46210d90c306e6891d0253ad56269c79c/yolo_output.jpeg)\n","\n","\n","We selectively filter out bounding boxes using a non-max suppression technique to have one bounding box for each object.\n","\n","\n","#Non-Max Suppression\n","Following steps are performed in non-max suppression:\n","Discard all the boxes with object confidence P <= 0.6\n","while there are any remaining boxes,\n","\n","pick the boxes with the largest P and output that as a prediction.\n","\n","eliminate any remaining boxes with IOU>=0.5 with the box output in the previous step.\n","\n","repeat the previous two steps for the next largest P value.\n","\n","The threshold for object confidence and IOU is not always fixed to 0.6 and 0.5 respectively, they may vary depending on the problem statement."]},{"cell_type":"markdown","metadata":{"id":"mUJc2llIH6UX","colab_type":"text"},"source":["#Yolo Loss Function\n","Yolo loss function includes four parts\n","1. Error in bounding box centers\n","2. Error in bounding box dimensions\n","3. Loss related to confidence score\n","4. Object classification loss\n","\n","The summation of above four losses corresponds to final loss JJ .\n","\n","#Bounding Box Centers\n","$$\\lambda_{coord}\\sum^{i = s^2}_{i = 0} \\sum^{j = B}_{j = 0 } 1^{obj}_{ij}(x_i -\\hat x_i)^2 +(y_i -\\hat y_i)λ\n","​coord\n","​​ ∑\n","​i=0\n","​i=s\n","​2\n","​​ \n","​​ ∑\n","​j=0\n","​j=B\n","​​ 1\n","​ij\n","​obj\n","​​ (x\n","​i\n","​​ −\n","​x\n","​^\n","​​ \n","​i\n","​​ )\n","​2\n","​​ +(y\n","​i\n","​​ −\n","​y\n","​^\n","​​ \n","​i\n","​​ )$$\n","\n","The above equation computes the loss related to error in predicted bounding box centers.\n","This computes error in each bounding box (predicted anchor boxes) $j = (0 . . . B)j=(0...B)$ in each grid cell $(i = 0 . . . S^2)(i=0...S\n","​2\n","​​ )$\n","\n","\n","$1^{obj}1\n","​obj$\n","​​  is:\n","1 if the $$i^{th}i\n","​th$$\n","​​  grid cell and $$j^{th}j\n","​th$$\n","​​  anchor box is responsible for object's prediction 0 otherwise\n","$$(x_i, y_i)(x\n","​i\n","​​ ,y\n","​i\n","​​ )$$ and $$(\\hat{x_i}, \\hat{y_i})(\n","​x\n","​i\n","​​ \n","​^\n","​​ ,\n","​y\n","​i\n","​​ \n","​^\n","​​ )$$ are coordinates of actual and predicted bounding boxes respectively.\n","$\\lambdaλ$ is an hyper parameter.\n","\n","#Dimension Loss\n","$$\\small \\lambda_{coord}\\sum^{i = s^2}_{i = 0} \\sum^{j = B}_{j = 0 } 1^{obj}_{ij}(w_i -\\sqrt{\\hat{w_{i}}})^2 +(h_i -\\sqrt{\\hat{h_{i}}})^2λ\n","​coord\n","​​ ∑\n","​i=0\n","​i=s\n","​2\n","​​ \n","​​ ∑\n","​j=0\n","​j=B\n","​​ 1\n","​ij\n","​obj\n","​​ (w\n","​i\n","​​ −√\n","​\n","​w\n","​i\n","​​ \n","​^\n","​​ \n","​\n","​​ )\n","​2\n","​​ +(h\n","​i\n","​​ −√\n","​\n","​h\n","​i\n","​​ \n","​^\n","​​ \n","​\n","​​ )\n","​2\n","​​ $$\n","\n","The formula for dimension loss is similar to location loss.\n","\n","The square root of the dimension makes sure that small deviation in larger box matters less than in small boxes.\n","\n","\n","\n","#Object Confidence Loss\n","\n","\n","\\small \\sum^{i = s^2}_{i = 0} \\sum^{j = B}_{j = 0 } 1^{obj}_{ij}(C_i -\\hat{C_{i}})^2 +\\lambda_{noobj}\\sum^{i = s^2}_{i = 0} \\sum^{j = B}_{j = 0 } 1^{noobj}_{ij}{obj}_{ij}(C_i -\\hat{C_{i}})^2∑\n","​i=0\n","​i=s\n","​2\n","​​ \n","​​ ∑\n","​j=0\n","​j=B\n","​​ 1\n","​ij\n","​obj\n","​​ (C\n","​i\n","​​ −\n","​C\n","​i\n","​​ \n","​^\n","​​ )\n","​2\n","​​ +λ\n","​noobj\n","​​ ∑\n","​i=0\n","​i=s\n","​2\n","​​ \n","​​ ∑\n","​j=0\n","​j=B\n","​​ 1\n","​ij\n","​noobj\n","​​ obj\n","​ij\n","​​ (C\n","​i\n","​​ −\n","​C\n","​i\n","​​ \n","​^\n","​​ )\n","​2\n","\n","\n","The above loss is with respect to object confidence error. Here C is the actual object confidence score, which is equal to one and $\\hat{C}\n","​C\n","​^$\n","​​  is the intersection over union between predicted and ground truth box.\n","$$1_{noobj}1\n","​noobj$$\n","​​  is one when there is no object in the cell and zero otherwise.\n","The parameter \\lambdaλ that you have seen in the dimension loss and in this formula is used to differentially weigh the parts of loss function.\n","\n","#Classification Loss\n","$$\\sum_{i=0}^{i=s^2} 1^{obj}_{i}\\sum_{c \\epsilon classes}(\\hat p(c)_i -\\hat p(c)_i)^2∑\n","​i=0\n","​i=s\n","​2\n","​​ \n","​​ 1\n","​i\n","​obj\n","​​ ∑\n","​cϵclasses\n","​​ (\n","​p\n","​^\n","​​ (c)\n","​i\n","​​ −\n","​p\n","​^\n","​​ (c)\n","​i\n","​​ )\n","​2$$\n","​​ \n","\n","The above loss is with respect to classification error.\n","\n","$1_i^{obj}1\n","​i\n","​obj$\n","​​  is one only if the object is present in the $i^{th}i$\n","​th\n","​​  grid cell. This term is used since we don't want to penalize the error. The term is used when there is no object present in the cell."]},{"cell_type":"markdown","metadata":{"id":"oO-JvWKwKolq","colab_type":"text"},"source":["#Keras\n","Keras API is built on top of TensorFlow.\n","It is more user friendly and easier to use when compared to TensorFlow.\n","For now, we will use only a few APIs of keras for Object detection.\n","You will learn more on keras in transfer learning.\n","\n","\n","#YOLO Pretrained Model\n","In the upcoming hands-on, you will be using yolo-v2 pre-trained model for object detection.\n","The model weights has been downloaded for you from the darknet (https://pjreddie.com/darknet/yolo/)  website.\n","\n","#Model Details\n","The yolo-v2 model has been trained over images of shape (m, 608, 608, 3), where m is the number of samples.\n","The model predicts 5 anchor boxes for each of 19 x 19 grid cells and can classify over 80 different classes.\n","Each of the anchor boxes has five parameters i.e. $b_xb\n","​x\n","​​ , b_yb\n","​y\n","​​ , b_hb\n","​h\n","​​ , b_w$ and $p_cp$\n","​$c\n","​​ .$\n","Thus the final dimension of output of an entire image is (19 x 19 x 5 x 85).\n","\n","\n","#Output Tensors\n","For the sake of computation, we rearrange the yolo out dimension as:\n","\n","box_confidence: tensor of shape (19 \\times 19, 5, 1)(19×19,5,1) containing p_cp\n","​c\n","​​  (confidence probability that there's some object) for each of the 5 boxes predicted in each of the 19x19 cells.\n","\n","boxes: tensor of shape (19 \\times 19, 5, 4)(19×19,5,4) containing $(b_x, b_y, b_h, b_w)$ $(b\n","​x\n","​​ ,b\n","​y\n","​​ ,b\n","​h\n","​​ ,b\n","​w\n","​​ )$ for each of the 5 boxes per cell.\n","\n","box_class_probs: tensor of shape (19 $\\times$ 19, 5, 80)(19×19,5,80) containing the detection probabilities $(c_1, c_2, ... c_{80})$ $(c\n","​1\n","​​ ,c\n","​2\n","​​ ,...c\n","​80\n","​​ )$ for each of the 80 classes for each of the 5 boxes per cell.\n","\n","\n","#Probability Score\n","Now in each grid cell for each of the five predicted anchor box we compute the probability score that the box contains a certain class.\n","\n","This is computed by multiplying the confidence score $p_cp\n","​c$\n","​​  of an anchor box with its corresponding class probabilities.\n","\n","This score is computed for each anchor box in entire 19 x 19 grid cells.\n","```\n","box_scores = box_confidence * box_class_probs  \n","```"]},{"cell_type":"markdown","metadata":{"id":"nL4d587NMJUB","colab_type":"text"},"source":["#Class Score\n","Once you have computed the scores, its now time to find the class with highest probability in each anchor boxes.\n","\n","We have totally 80 scores for each anchor box in a cell.\n","\n","Out of these 80 scores the index of the maximum score gives the class predicted by that anchor box.\n","\n","The below code snippet extracts class indices of maximum score and their corresponding score values\n","###import keras  with TensorFlow backend\n","```\n","from keras import backend as K \n","box_classes = K.argmax(scores,  axis = -1)\n","class_score = K.max(scores,  axis = -1)\n","\n","```\n","#Filtering Boxes\n","Now we apply mask to box_class_scores, boxes and box_classes to filter out the boxes we don't want. Finally we should be left with boxes with scores above a threshold.\n","\n","```\n","filtering_mask = (box_class_scores > threshold)\n"," \n","import tensorflow as tf\n"," \n","scores = tf.boolean_mask(box_class_scores, filtering_mask)\n","boxes = tf.boolean_mask(boxes, filtering_mask)\n","classes = tf.boolean_mask(box_classes, filtering_mask)\n","```\n","\n","\n","#Non-Max Suppression\n","Until now, we have eliminated boxes with least scores.\n","But still, we have overlapping boxes predicting the same object.\n","\n","So, its time to implement non-max suppression.\n","\n","Luckily, we have direct implementation of non-max suppression in tensorflow given by \n","\n","```\n","tf.image.non_max_suppression\n","nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, iou_threshold)\n","final_scores = tf.gather(scores, nms_indices)\n","final_boxes = tf.gather(boxes, nms_indices)\n","final_classes = tf.gather(classes, nms_indices)\n","\n","```\n","\n","max_box_tensor is an array initialized whose length is equal to the maximum number of objects we like to predict in the image.\n","\n","tf.gather() samples only those boxes filtered by non-max suppression.\n"]},{"cell_type":"code","metadata":{"id":"KiVKIU5lgnPt","colab_type":"code","colab":{}},"source":["##########  FACE DECTECTION #####################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-iXpgzpjZUC","colab_type":"text"},"source":["#Face Recognition\n","A face recognition system should be able to recognize a person's identity provided that it has been trained to identify that person among other people uniquely.\n","\n","One way to build this system is to train a CNN network with many images of the same person.\n","\n","The flaw of this approach is that we have limited training images and every time if there is a new person, the network has to be retrained.\n","\n","\n","#One Shot Learning\n","One shot learning algorithm uses fewer training data to detect the object class.\n","\n","Given the single image of a person, this algorithm learns to recognize the same person at a later point of times.\n","\n","#Similarity Function\n","One shot learning is based on similarity function.\n","\n","Consider, we have two images img1img1 and img2img2. The similarity function gives the degree of difference between these two images given by d(img1, img2)d(img1,img2) .\n","\n","If the difference is lesser than the threshold, then it means the two images are similar.\n","\n","#Siamese Network\n","Siamese Network\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/705/large/20a0a8c8f471e6fd2330d9decc5be32115a0a42c/SiameseNetwork.jpeg)\n","\n","The objective of the Siamese network is to determine how similar two comparable things are.\n","\n","The Siamese network has two identical networks with the same parameters.\n","\n","Swipe next to see how this network works.\n","\n","\n","#Distance Function\n","Once we got the two vectors, we pass these vectors into distance function to see the similarity of the two vectors.\n","\n","$$\\small d(x1 , x2) = ||(f(x1) - f(x2))^2||d(x1,x2)=∣∣(f(x1)−f(x2))\n","​2\n","​​ ∣∣$$\n","\n","If the distance is lesser than the threshold then it means the two images are the same person, if not they are two diverse persons.\n","\n","\n","#Image to Vector\n","The first network takes the input image and passes through a series of convolution layers and a fully connected network to output a vector $x^{(1)}x\n","​(1)$\n","​​ .\n","\n","Another image is passed through the same network to output a vector $x^{(2)}x\n","​(2)$\n","​​  .\n","\n","The two vectors are the numeric encodings of corresponding images.\n","\n","#Training the Network\n","Now the network has to learn parameter to generate good encoding.\n","\n","To train the network, we apply gradient descent on triplet loss function, which is a function of encodings of three different images an anchor image, a positive image, and a negative image .\n","\n","1. Anchor image (A) - the image of the person you want to represent.\n","2. Positive image (P) - a different picture of the same person.\n","3. Negative image (N) - an image of a completely different person.\n","\n","\n","#Triplet Loss\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/685/large/1458f3eaf9792bf9edf1e3875ff0e9c4025e4dd8/triplet_loss.jpeg)\n","\n","\n","Now the objective is to minimize the distance between the encodings of Anchor image and Positive image and at the same time maximize the distance between encodings of Anchor image and Negative image.\n","\n","The above criteria are defined by triplet loss which is given by:$$\n","\\small ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= -\\alpha∣∣f(A)−f(P)∣∣\n","​2\n","​​ −∣∣f(A)−f(N)∣∣\n","​2\n","​​ <=−α$$\n","\n","The value alpha is to make sure that the difference is large.\n","\n","\n","#Cost Function\n","The loss function for each sample comprising of an Anchor, the positive and a negative image is given by:\n","\n","$$\\small L(A, p, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha, 0)L(A,p,N)=max(∣∣f(A)−f(P)∣∣\n","​2\n","​​ −∣∣f(A)−f(N)∣∣\n","​2\n","​​ +α,0)$$\n","\n","So the above equation sees if the difference between the anchor and the negative image surpasses the difference between the anchor and the positive image by set margin $\\alpha$ .\n","\n","Now the cost function over n samples the summation of triplet loss across n samples $$J = \\sum _{i=1}^{i=n}(L(A^{(i)}, P^{(i)}, N^{(i)})J=∑\n","​i=1\n","​i=n\n","​​ (L(A\n","​(i)\n","​​ ,P\n","​(i)\n","​​ ,N\n","​(i)\n","​​ )$$\n"]},{"cell_type":"markdown","metadata":{"id":"lMM9581AgPU_","colab_type":"text"},"source":["#Transfer Learning\n","Let's consider a scenario where you are learning a new programming language and for any programming language the fundamental concepts of data structures, control flows, loops are pretty much the same.\n","\n","If you already know these fundamental concepts, then you can learn a language at a faster pace or else you end up spending more time.\n","\n","Transfer learning is analogous to the above scenario wherein you use already trained model and customize it to your data.\n","\n","\n","#Pre-Trained Models\n","Pre-trained models are one that are already built by others.\n","\n","Some of the well known pre-trained models in CNN are Yolo-v2, VGG-net, Inception-net that have been trained over billions of images over thousand of classes.\n","\n","Training the above models from scratch would take weeks together in spite of using GPUs.\n","\n","In the case of images, since lower layers of CNN learns pretty much the same features instead of creating a model from ground level. For similar problems, we can use the pre-trained model as a starting point.\n","\n","#When to use pretrained model?\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/003/483/686/large/85fc1b7a790bce695185d44c81c8f946f8639266/transfer_learning.jpeg)\n","\n","It is not worth to use the pre-retrained model when the problem statement it was trained on is very different from what you have.\n","\n","For example, using a model trained on speech recognition performs badly on image recognition task.\n","\n","\n","#Transfer Learning Types\n","Transfer learning is mainly of three types:\n","\n","1. Feature extraction\n","2. Architecture re-use\n","3. Partial training\n","\n","\n","#Feature Extraction\n","Feature Extraction\n","In this mechanism, we are only interested in the features extracted by the pre-trained model.\n","\n","For example, we already have trained version of the VGG net on image net dataset that can classify 1000 different classes. But we are interested in recognizing one among three different classes.\n","\n","Now, we replace the final softmax layer of the VGG net having 1000 nodes with three nodes one for each class and retrain the network.\n","\n","\n","#Architecture Reuse\n","Here, we don't use the trained weights of the network.\n","\n","We only adopt the network configuration like the number of layers, type of initialization and retrain the model from scratch.\n","\n","By this way, we save more time to come up with the right architecture.\n","\n","#Partial Training\n","Partial Training\n","Here, we use a pre-trained model and update the weights of only specific layers and freeze the rest of the layers.\n","\n","Starting from the final layer, we iteratively unfreeze the preceding layer till you get the considerable accuracy."]},{"cell_type":"code","metadata":{"id":"y_kZF9PGm4VP","colab_type":"code","colab":{}},"source":["#   TRANSFER LEARNING IN KERAS ################################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BiMeiiZm92e","colab_type":"text"},"source":["#Building a Simple Model - Keras\n","Before implementing transfer learning, let's look into building a model using Keras.\n","\n","The below code snippet shows how to build a simple CNN model using Sequential() constructor of Keras.\n","```\n","from Keras import Sequential, Droupout, Flatten, Dense\n","model = Sequential()\n","\n","### Adding two convolution layers of 3 x 3 x 32 \n","model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))\n","model.add(Convolution2D(32, 3, 3, activation='relu'))\n","\n","### Adding max-pooling layer of filter size 2 x 2\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","### Adding droupout to previous layer\n","model.add(Dropout(0.25))\n"," \n"," ### Convilve to FC layer\n","model.add(Flatten())\n","\n","#### adding a fully connected layer of 128 nodes\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","#### Final softmax layer\n","model.add(Dense(10, activation='softmax')) \n","```\n","\n","\n","#Compiling the Model\n","Before training the model, we need to decide on the type of optimization, loss function and accuracy metric.\n","\n","This is done using compile() method as shown below\n","\n","```\n","model.compile(loss='categorical_crossentropy',  optimizer='adam', metrics=['accuracy'])\n","```\n","\n","\n","Here, we are mentioning the model to use cross entropy loss, the adam optimizer and the default accuracy metric for cross entropy.\n","\n","#Model Summary\n","Once you have defined the model, you will be able to view the model structure using the summary() function.\n","\n","```\n","model.summary()\n","\n","output:\n","\n","Layer(type)\tOutput Shape\tParam #\n","conv2d_4 (Conv2D)\t(None, 26, 26, 32)\t320\n","conv2d_5 (Conv2D)\t(None, 24, 24, 32)\t9248\n","max_pooling2d_1 (MaxPooling2\t(None, 12, 12, 32)\t0\n","dropout_1 (Dropout)\t(None, 12, 12, 32)\t0\n","flatten_1 (Flatten)\t(None, 4608)\t0\n","dense_1 (Dense)\t(None, 128)\t589952\n","dropout_2 (Dropout)\t(None, 128)\t0\n","dense_2 (Dense)\t(None, 10)\t1290\n","Total params: 600, 810\n","Trainable params: 600, 810\n","Non-trainable params: 0\n","```\n","\n","\n","#Loading Pre-Trained Model\n","Keras has an implementation to import some of the well known pre-trained model.\n","The below code snippet imports VGG-16 model.\n","\n","```\n","from keras.applications.vgg16 import VGG16\n","\n","### VGG16 model, with weights pre-trained on ImageNet dataset\n","model = VGG16(weights='imagenet', include_top=True)\n","```\n","\n","The parameter include_top = True makes sure that final softmax layer is included.\n","\n","\n","\n","#Customization\n","The VGG16 model we imported is trained to classify over thousand categories. Let's replace the final softmax layer to classify over sixteen categories.\n","\n","```\n","from keras.models import Model\n","model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (256, 256, 3))\n","\n","##adding custom layers\n","\n","x = model.output\n","x = Flatten()(x)\n","x = Dense(1024, activation=\"relu\")(x)\n","x = Dropout(0.5)(x)\n","x = Dense(1024, activation=\"relu\")(x)\n","predictions = Dense(16, activation=\"softmax\")(x)\n","\n","###initite the model\n","\n","This model will include all layers required in the computation of predictions given for the given input.\n","final_model = Model(input = model.input, output = predictions)\n","\n","# compile the model \n","final_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"f8DrHvcOpQgr","colab_type":"text"},"source":["#Summary\n","In this course, you have learned:\n","The elements of object detection\n","Used pre-trained yolo model to identify, localize, and label the objects\n","Face recognition technique using siamese network\n","Customize the pre-trained model to your data using transfer learning."]}]}
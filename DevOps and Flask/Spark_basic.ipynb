{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spark_basic.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBmxYeIl1OYC9ukYVVM6VP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hHaKmXL4hI-H","colab_type":"text"},"source":["#Getting to Know Spark\n","Welcome to the course on Spark Preliminaries. Here, you will learn about:\n","\n","Spark and its ecosystem\n","\n","The need for Spark and its applications\n","\n","Spark Data Model and Runtime Architecture\n","\n","Storage and Caching Concepts\n","\n","And followed by an assessment.\n","\n","#According to Wikipedia,\n","\n","Apache Spark is an open source cluster computing framework that provides an interface for entire programming clusters with implicit data parallelism and fault-tolerance.\n","\n","Apache Spark is devised to serve as a general-purpose and fast cluster computing platform.\n","\n","Spark runs computations in memory and provides a quicker system for complex applications operating on disk.\n","\n","Spark covers various workloads needing a dedicated distributed systems namely streaming, interactive queries, iterative algorithms, and batch applications.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/111/275/large/0e1d388c97707d05f4e9ed8834e2404d94d947c0/History.jpeg)\n","\n","Spark was founded by Matei Zaharia at AMPLab in UC Berkeley in 2009. It was later open-sourced under a BSD license in 2010.\n","\n","The Spark project was donated to the Apache Software Foundation in 2013 and then licensed to Apache 2.0.\n","\n","Spark was recognized as a Top-Level Apache Project in February 2014.\n","\n","M. Zaharia's company Databricks created a\n","\n","new world record using Spark in large scale sorting.\n","\n","Spark 2 was launched in June 2016.\n","\n","#Key Features\n","Performance:\n","\n","Faster than Hadoop MapReduce up to 10x (on disk) - 100x (In-Memory)\n","\n","Caches datasets in memory for interactive data analysis\n","\n","In Spark, tasks are threads, while in Hadoop, a task generates a separate JVM.\n","\n","Rich APIs and Libraries\n","\n","Offers a deep set of high-level APIs for languages such as R, Python, Scala, and Java.\n","\n","Very less code than Hadoop MapReduce program because it uses functional programming constructs.\n","\n","Scalability and Fault Tolerant\n","\n","Scalable above 8000 nodes in production.\n","\n","Utilizes Resilient Distributed Datasets (RDDs) a logical collection of data partitioned across machines, which produces an intelligent fault tolerance mechanism.\n","\n","\n","#Key Features...\n","Supports HDFS\n","\n","Integrated with Hadoop and its ecosystem\n","\n","It can read existing data.\n","\n","Realtime Streaming\n","\n","Supports streams from a variety of data sources like Twitter, Kinesis, Flume, and Kafka.\n","We defined a high-level library for stream processing, utilizing Spark Streaming.\n","Interactive Shell\n","\n","Provides an Interactive command line interface (in Python or Scala) for horizontally scalable, low-latency, data exploration.\n","\n","Supports structured and relational query processing (SQL), via Spark SQL.\n","\n","Machine Learning\n","\n","Higher level libraries for graph processing and machine learning.\n","\n","Various machine learning algorithms such as pattern-mining, clustering, recommendation, and classification.\n","\n","\n","#Advantages of Spark Over MapReduce\n","Let us compare MapReduce and Spark based on the following essential aspects in detail.\n","\n","Solving Iterative problems\n","\n","Solving Interactive problems\n","\n","In the subsequent slides, we will look into these two problems in detail.\n","\n","\n","#The figure demonstrates how MapReduce and Spark respectively handle iterative problems.\n","\n","The first figure shows how in MapReduce, the intermediate results of each iteration are stored to disk and then read back for the next processing.\n","\n","The second figure shows how in the case of Spark processing, the results can be kept in RAM and fetched easily for each iteration. Thus there is no disk i/o related latency.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/112/443/large/8436bf4c59171d1fdba99594b0ae8cf0ee6bbe03/solve_Interactive_Problems_v2.jpeg)\n","\n","The figure demonstrates how Spark handles interactive problems.\n","\n","In MapReduce, the same data is repeatedly read from disk for different queries.\n","\n","The figure shows how in the case of Spark processing, the input is read just once into memory where different queries act on the data to give their results.\n","\n","#Spark vs MapReduce\n","The other aspects by which Spark differs from MapReduce are summarized below.\n","\n","Difficulty: Apache Spark is a simpler to program and does not require any abstractions whereas MapReduce is hard to program with abstractions.\n","\n","Interactivity: Spark provides an interactive mode whereas MapReduce has no inbuilt interactive mode except for Pig and Hive.\n","\n","#Spark vs MapReduce...\n","Streaming: Hadoop MapReduce offers batch processing on historical data whereas Spark provides streaming of data and processing in real-time.\n","\n","Latency: Spark caches partial results over its memory of distributed workers thereby ensuring lower latency computations. In contrast to Spark, MapReduce is disk-oriented.\n","\n","Speed: Spark places the data in memory, by storing the data in Resilient Distributed Databases (RDD). Spark is 100X quicker than Hadoop MapReduce for big data processing.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/700/large/d3b996d2be3b62a964e363af3ee3f6ad7320e319/Spark-Ecosystem.jpeg)\n","\n","\n","#Spark Ecosystem\n","\n","\n","Now let us have a look at the components that make up Spark Ecosystem.\n","\n","Spark Core:\n","\n","Includes the primary functionality of Spark, namely components for task scheduling, fault recovery, memory management, interacting with storage systems, etc.\n","\n","Home to the API that represents RDD, which is the primary programming abstraction of Spark.\n","\n","Spark SQL:\n","\n","Package for working with structured data.\n","\n","Enables querying data through SQL and as the Apache Hive variant of SQL — termed as the Hive Query Language (HQL).\n","\n","Supports various data, including JSON, Parquet, and Hive tables.\n","\n","\n","#Spark Ecosystem\n","Spark Streaming:\n","Spark component that allows live-streaming data processing. Eg: includes log files created by production web servers, or queues of messages including status updates raised by web service users.\n","\n","MLlib: Spark appears with a library including common machine learning (ML) feature, named MLlib. Here, MLlib offers many types of machine learning algorithms, namely collaborative filtering, clustering, regression, and classification.\n","\n","GraphX: A library for performing graph-parallel computations and manipulating graphs.\n","\n","#Supported Languages\n","Apache Spark currently supports multiple programming languages, including Java, Scala, R and Python. The final language is chosen based on the efficiency of the functional solutions to tasks, but most developers prefer Scala.\n","\n","Apache Spark is built on Scala, thus being proficient in Scala helps you to dig into the source code when something does not work as you expect.\n","\n","Scala is a multi-paradigm programming language and supports functional as well as object oriented paradigms. It is a JVM based statically typed language that is safe and expressive.\n","\n","Python is in general slower than Scala while Java is too verbose and does not support Read-Evaluate-Print-Loop (REPL).\n","\n","#Applications of Spark\n","Interactive analysis – MapReduce supports batch processing, whereas Apache Spark processes data quicker and thereby processes exploratory queries without sampling.\n","\n","Event detection – Streaming functionality of Spark permits organizations to monitor unusual behaviors for protecting systems. Health/security organizations and financial institutions utilize triggers to detect potential risks.\n","\n","Machine Learning – Apache Spark is provided with a scalable Machine Learning Library named as MLlib, which executes advanced analytics on iterative problems. Few of the critical analytics jobs such as sentiment analysis, customer segmentation, and predictive analysis make Spark an intelligent technology.\n","\n","#Companies Using Spark\n","Companies that use Apache Spark are:\n","\n","Uber – Deploys HDFS, Spark Streaming, and Kafka for developing a continuous ETL pipeline.\n","\n","Conviva – Uses Spark for handling live traffic and optimizing the videos.\n","\n","Pinterest – Deploys Spark Streaming to know about customer engagement information.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SeHlUF_hkJR5","colab_type":"text"},"source":["#SparkConf\n","SparkConf stores configuration parameters for a Spark application.\n","\n","These configuration parameters can be properties of the Spark driver application or utilized by Spark to allot resources on the cluster, like memory size and cores.\n","\n","SparkConf object can be created with new SparkConf() and permits you to configure standard properties and arbitrary key-value pairs via the set() method.\n","\n","\n","#SparkConf\n","\n","```\n","val conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"FirstSparkApp\")\n","\n","val sc = new SparkContext(conf)\n","\n","```\n","\n","Here, we have created a SparkConf object specifying the master URL and application name and passed it to a SparkContext.\n","\n","\n","#SparkContext\n","Main entry point for Spark functionality\n","\n","SparkContext can be utilized to create broadcast variables, RDDs, and accumulators, and denotes the connection to a Spark cluster.\n","\n","To create a SparkContext, you first have to develop a SparkConf object that includes details about your application.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/664/large/df9ed1596ad5e9c32c495c32c8320a8efd5566bb/SparkContext.jpeg)\n","\n","As shown in the diagram, the Spark driver program uses SparkContext to connect to the cluster manager for resource allocation, submit Spark jobs and knows what resource manager (YARN, Mesos or Standalone) to communicate.\n","\n","Via SparkContext, the driver can access other contexts like StreamingContext, HiveContext, and SQLContext to program Spark.\n","\n","\n","#SparkContext\n","There may be only one SparkContext active per JVM. Before creating a new one, you have to stop() the active SparkContext.\n","\n","In the Spark shell, there is already a special interpreter-aware SparkContext created in the variable named as sc.\n","```\n"," val sc = new SparkContext(conf)\n","```\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/669/large/aec01620a224fc9eab167b61d315736c283d16b8/RDD.jpeg)\n","\n","\n","Resilient distributed datasets (RDDs) are the known as the main abstraction in Spark.\n","\n","It is a partitioned collection of objects spread across a cluster, and can be persisted in memory or on disk.\n","\n","Once created, RDDs are immutable.\n","\n","#Features of RDDs\n","Resilient, i.e. tolerant to faults using RDD lineage graph and therefore ready to recompute damaged or missing partitions due to node failures.\n","\n","Dataset - A set of partitioned data with primitive values or values of values, For example, records or tuples.\n","\n","Distributed with data remaining on multiple nodes in a cluster.\n","\n","\n","#Creating RDDs\n","Parallelizing a collection in driver program.\n","\n","E.g., here is how to create a parallelized collection holding the numbers 1 to 5:\n","\n","```\n","val data = Array(1, 2, 3, 4, 5)\n","val newRDD = sc.parallelize(data)\n","```\n","Here, newRDD is the new RDD created by calling SparkContext’s parallelize method.\n","\n","Referencing one dataset in an external storage system, like a shared filesystem, HBase, HDFS, or any data source providing a Hadoop InputFormat.\n","\n","For example, text file RDDs can be created using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI) and reads it as a collection of lines to produce RDD newRDD.\n","\n","```\n","val newRDD = sc.textFile(\"data.txt\")\n","```\n","\n","#DataFrames\n","Similar to an RDD, a DataFrame is an immutable distributed set of data.\n","\n","Unlike an RDD, data is arranged into named columns, similar to a table in a relational database.\n","\n","Created to make processing simpler, DataFrame permits developers to impose a structure onto a distributed collection of data, enabling higher-level abstraction.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/667/large/a882d873e3b438e3de2a6bf9d0341c17e8375e2b/Creating-DataFrames.jpeg)\n","\n","\n","DataFrames can be created from a wide array of sources like existing RDDs, external databases, tables in Hive, or structured data files.\n","\n","\n","#Creating DataFrames...\n","Applications can create DataFrames from a Hive table, data sources, or from an existing RDD with an SQLContext.\n","\n","The subsequent example creates a DataFrame based on the content of a JSON file:\n","```\n","val sc: SparkContext // An existing SparkContext.\n","val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n","val df = sqlContext.read.json(\"home/spark/input.json\")\n","// Shows the content of the DataFrame to stdout\n","df.show()\n","```\n","\n","#SQL on DataFrames\n","The sql function on a SQLContext allows applications to run SQL queries programmatically and returns the result as a DataFrame.\n","```\n","val sc: SparkContext // An existing SparkContext.\n","val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n","val df = sqlContext.read.json(\"home/spark/input.json\")\n","input.registerTempTable(\"students\")\n","val teenagers = sqlContext.sql(\"SELECT name, age FROM students WHERE age >= 13 AND age <= 19\")\n","```\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/666/large/b9d3baee847310f050bc809c39860902a447b3eb/Dataset.jpeg)\n","\n","\n","Dataset is a new interface included in Spark 1.6, which provides the advantages of RDDs with the advantages of Spark SQL’s optimized execution engine.\n","\n","It is an immutable, strongly-typed set of objects that are mapped to a relational schema.\n","\n","A DataFrame is a known as a Dataset organized into named columns.\n","\n","Dataset will act as the new abstraction layer for Spark from Spark 2.0.\n","\n","#Creating a Dataset\n","```\n","val lines = sqlContext.read.text(\"/log_data\").as[String]\n","val words = lines\n","  .flatMap(_.split(\" \"))\n","  .filter(_ != \"\")\n","\n","```\n","\n","Here, we have created a dataset lines on which RDD operations like filter and split are applied.\n","\n","#Benefits of Dataset APIs\n","Static-typing and runtime type-safety\n","\n","High-level abstraction and custom view into structured and semi-structured data\n","\n","Higher performance and Optimization\n","\n","#SparkSession - a New Entry Point\n","SparkSession, introduced in Apache Spark 2.0, offers a single point of entry to communicate with underlying Spark feature and enables programming Spark with Dataset APIs and DataFrame.\n","\n","In previous versions of Spark, spark context was the entry point for Spark. For streaming, you required StreamingContext for hive HiveContext and for SQL SQLContext.\n","\n","As Dataframe and DataSet APIs are the new standards, Spark 2.0 features SparkSession as the new entry point.\n","\n","SparkSession is a combination of HiveContext, StreamingContext, and SQLContext. All the APIs available on these contexts are available on SparkSession also. It internally has a spark context for actual computation.\n","\n","#Creating SparkSession\n","A SparkSession can be built utilizing a builder pattern. The builder will automatically reuse an existing SparkContext if one exists; and create a SparkContext if it does not exist.\n","```\n","val dataLocation = \"file:${system:user.dir}/spark-data\"\n","// Create a SparkSession\n","val spark = SparkSession.builder().appName(\"SparkSessionExample\").config(\"spark.sql.data.dir\", dataLocation).enableHiveSupport().getOrCreate()\n","\n","```\n","\n","#Configuring Properties\n","Once the SparkSession is instantiated, you can configure the runtime config properties of Spark. E.g: In this code snippet, we can alter the existing runtime config options.\n","```\n","//set new runtime options\n","spark.conf.set(\"spark.executor.memory\", \"1g\")\n","spark.conf.set(\"spark.sql.shuffle.partitions\", 4)\n","//get all settings\n","val configMap:Map[String, String] = spark.conf.getAll()\n","```\n","\n","\n","#Running SQL Queries\n","SparkSession is the entry point for reading data, akin to the old SQLContext.read. It can be utilized to execute SQL queries across data, getting the results back as a DataFrame.\n","\n","```\n","val jsonData = spark.read.json(\"/home/user/employee.json\")\n","\n","display(spark.sql(\"select * from employee\"))\n","```\n","\n","\n","#Access to Underlying SparkContext\n","SparkSession.sparkContext returns the subsequent SparkContext, employed for building RDDs and managing cluster resources.\n","```\n","spark.sparkContext\n","res17: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2debe9ac\n","```\n","\n","\n","#Shared Variables\n","Usually, when a function passed to a Spark operation is run on a remote cluster node, it runs on individual copies of all the variables used in the function.\n","\n","These variables are copied to every machine, and no updates to the variables on the remote machine are delivered back to the driver program.\n","\n","Spark offers two limited types of shared variables for two common usage patterns: accumulators and broadcast variables.\n","\n","\n","#Broadcast Variables\n","Enables the programmer to keep a read-only variable cached on each machine instead of shipping a copy of it with tasks.\n","\n","Generated from a variable v by calling SparkContext.broadcast(v)\n","\n","Its value can be accessed by calling the value method. The subsequent code shows this:\n","```\n","scala> val broadcastVar = sc.broadcast(Array(1, 2, 3, 4, 5))\n","broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)\n","scala> broadcastVar.value\n","res0: Array[Int] = Array(1, 2, 3, 4, 5)\n","\n","```\n","\n","#Accumulators\n","Accumulators are known as the variables that are only “added” via an associative and commutative operation and can, hence, be efficiently supported in parallel.\n","\n","They can be utilized to implement sums or counters.\n","\n","Programmers can include support for new types.\n","\n","Spark natively offers support for accumulators of numeric types.\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/663/large/e2760781738ff1e5a629322f5fd22350720084ee/named_accumulator.jpeg)\n","\n","\n","You can create unnamed or named accumulators as a user. As seen in the image, a named accumulator (here, counter) will be displayed in the web UI for the stage that modifies that accumulator. Spark shows the value for each accumulator modified by a task in the “Tasks” table.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KWA1dXc9nlTR","colab_type":"text"},"source":["#Introduction to RDD Operations\n","The following example shows the implementation of Spark - MapReduce’s example, word count. You can view that Spark offers support for operator chaining.\n","\n","It turns out to be handy when doing pre- or post-processing on data, like filtering data before running a complex MapReduce job.\n","```\n","val file = sc.textFile(\"hdfs://.../wordcounts-*.gz\")\n","val counts = file.flatMap(line => line.split(\" \"))\n","                   .map(word => (word, 1))\n","                   .reduceByKey(_ + _)\n","counts.saveAsTextFile(\"hdfs://.../wordcountOutput\")\n","```\n","\n","Here flatMap, map,reduceByKey and saveAsTextFile are the operations on the RDDs.\n","\n","#Transformations\n","Transformations are functions that use an RDD as the input and return one or more RDDs as the output.\n","\n","randomSplit, cogroup, join, reduceByKey, filter, and map are examples of few transformations.\n","\n","Transformations do not change the input RDD, but always create one or more new RDDs by utilizing the computations they represent.\n","\n","By using transformations, you incrementally create an RDD lineage with all the parent RDDs of the last RDD.\n","\n","Transformations are lazy, i.e. are not run immediately. Transformations are done on demand.\n","\n","Transformations are executed only after calling an action.\n","\n","\n","#Example of Transformations\n","filter(func): Returns a new dataset (RDD) that are created by choosing the elements of the source on which the function returns true.\n","\n","map(func): Passes each element of the RDD via the supplied function.\n","\n","union(): New RDD contains elements from source argument and RDD.\n","\n","intersection(): New RDD includes only common elements from source argument and RDD.\n","\n","cartesian(): New RDD cross product of all elements from source argument and RDD.\n","\n","#Actions\n","Actions return concluding results of RDD computations.\n","\n","Actions trigger execution utilizing lineage graph to load the data into original RDD, and then execute all intermediate transformations and write final results out to file system or return it to Driver program.\n","\n","Count, collect, reduce, take, and first are few actions in spark.\n","\n","\n","#Example of Actions\n","count(): Get the number of data elements in the RDD\n","\n","collect(): Get all the data elements in an RDD as an array\n","\n","reduce(func): Aggregate the data elements in an RDD using this function which takes two arguments and returns one\n","\n","take (n): Fetch first n data elements in an RDD computed by driver program.\n","\n","foreach(func): Execute function for each data element in RDD. usually used to update an accumulator or interacting with external systems.\n","\n","first(): Retrieves the first data element in RDD. It is similar to take(1).\n","\n","saveAsTextFile(path): Writes the content of RDD to a text file or a set of text files to local file system/HDFS.\n","\n","\n","#Sample Code Snippet\n","Let us look at the outline of a code snippet that uses transformations and actions on a RDD.\n","```\n","records = spark.textFile(“hdfs://...”)\n","errors = records.filter(_.startsWith(“ERROR”))\n","messages = errors.map(_.split(‘\\t’)(2))\n","cachedMessages = messages.cache()\n","cachedMessages.filter(_.contains(“400”)).count\n","\n","```\n","\n","In this program, records is the Base RDD and errors is a transformed RDD created by applying the filter transformation.\n","\n","Count is the action called upon which the transformations start to execute.\n","\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/662/large/2bdc231dcaaac1c8692713adb2c9e4b83b98100d/lineage-Graph.jpeg)\n","\n","\n","In the above figure, you can view how transformations like map, filter, combineByKey act on each RDD.\n","\n","RDDs maintain a graph of one RDD transforming into another termed as lineage graph, which assists Spark to recompute any common RDD in the event of failures. This way Spark achieves fault tolerance.\n","\n","\n","#Lazy Evaluation\n","When we call a transformation on RDD, the operation is not immediately executed. Alternatively, Spark internally records meta-data to show this operation has been requested. It is called as Lazy evaluation.\n","\n","Loading data into RDD is lazily evaluated as similar to how transformations are.\n","\n","In Hadoop, developers often spend a lot of time considering how to group together operations to minimize the number of MapReduce passes. It is not required in case of Spark.\n","\n","Spark uses lazy evaluation to reduce the number of passes it has to take over our data by grouping operations together. Hence, users are free to arrange their program into smaller, more controllable operations."]},{"cell_type":"markdown","metadata":{"id":"2u-qwjVqpsc2","colab_type":"text"},"source":["![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/660/large/d87b5005ba89313de9abfd2c860870696c3b79c1/Spark-Sources-1.jpeg)\n","\n","#Storages\n","\n","The Data Sources API offers a single interface for storing and loading data using Spark SQL.\n","\n","In addition to the sources that come prepackaged with the Apache Spark distribution, this API offers an integration point for external developers to add support for custom data sources.\n","\n","#File Formats\n","The following are some of the file formats supported by Spark.\n","\n","Text\n","\n","JSON\n","\n","CSV\n","\n","Sequence File\n","\n","Parquet\n","\n","Hadoop InputOutput Format\n","\n","#Storage/Source Integrations\n","Although often linked to the Hadoop Distributed File System (HDFS), Spark can combine with various open source or commercial third-party data storage systems, including:\n","\n","Google Cloud\n","\n","Elastic Search\n","\n","JDBC\n","\n","Apache Cassandra\n","\n","Apache Hadoop (HDFS)\n","\n","Apache HBase\n","\n","Apache Hive\n","\n","Developers are most expected to choose the data storage system they are previously utilizing elsewhere in their workflow.\n","\n","#Hive Integration\n","Hive comes packaged with the Spark library as HiveContext that inherits from SQLContext. Utilizing HiveContext, you can create and find tables in the HiveMetaStore and write queries on it using HiveQL.\n","\n","When hive-site.xml is not configured, the context automatically produces a metastore named as metastore_db and a folder known as warehouse in the current directory.\n","\n","#Data Load from Hive to Spark\n","Now let us see an example code to load data from Hive in Spark.\n","\n","Consider the following example of employee in a text file named employee.txt. We will first create a hive table, load the employee record data into it using HiveQL language, and apply some queries on it.\n","\n","Use the subsequent command for initializing the HiveContext into the Spark Shell\n","```\n","scala> val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\n","```\n","Let us now create a table named employee with the fields id, name, and age using HQL.\n","```\n","scala> sqlContext.sql(\"CREATE TABLE IF NOT EXISTS employee(id INT, name STRING, age INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\")\n","```\n","\n","#Data Load from Hive to Spark...\n","Now we shall load the employee data into the employee table in Hive.\n","\n","```\n","scala> sqlContext.sql(\"LOAD DATA LOCAL INPATH 'employee.txt' INTO TABLE employee\")\n","```\n","\n","Next, we shall fetch all records using HiveQL select query.\n","\n","```\n","scala> val result = sqlContext.sql(\"FROM employee SELECT id, name, age\")\n","```\n","To show the record data, call the show() method on the result DataFrame.\n","\n","```\n","scala> result.show()\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"BETZ2Qcgq8-D","colab_type":"text"},"source":["#Spark Cluster\n","Spark Cluster ![alt text](https://docs-cdn.fresco.me/system/attachments/files/000/108/668/large/baf57011c755bd6ceed6692549b7b9562197e7cc/Spark-Cluster.jpeg)\n","\n","\n","A Spark application includes a single driver process and a collection of executor processes scattered over nodes on the cluster.\n","\n","Both the executors and the driver usually run as long as the application runs.\n","\n","#Spark Driver\n","Program that produces the SparkContext, connecting to a given Spark Master.\n","\n","Declares the actions and transformations on RDDs of data.\n","\n","#Spark Executors\n","Runs the tasks, return results to the driver.\n","\n","Offers in memory storage for RDDs that are cached by user programs.\n","\n","Multiple executors per nodes possible\n","\n","\n","#Cluster Managers\n","Standalone – a simple cluster manager added with Spark that makes it simple to establish a cluster.\n","\n","Apache Mesos – a cluster manager that can run service applications and Hadoop MapReduce.\n","\n","Hadoop YARN – the resource manager in Hadoop 2.\n","\n","#Launching Applications with Spark-submit\n","\n","```\n","./bin/spark-submit\n","\n","--class <main-class>\n","\n","--master <master-url>\n","\n","--deploy-mode <deploy-mode>\n","\n","--conf <key>=<value>\n","\n","... # other options\n","\n","<application-jar>\n","\n","[application-arguments]\n","\n","```\n","\n","#Commonly Used Options\n","class: entry point for your application (e.g. org.apache.spark.examples.SparkPi)\n","\n","master: master URL for the cluster (e.g. spark://23.195.26.187:7077)\n","\n","deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client)\n","\n","conf: Arbitrary Spark configuration property in key=value format.\n","\n","application-jar: Path to a bundled jar with the application and dependencies.\n","\n","application-arguments: Arguments passed to the main method of your main class, if any.\n","\n","#Deployment Modes\n","Choose which mode to run using the --deploy-mode flag\n","\n","Client - Driver runs on a dedicated server (e.g.: Edge node) inside a dedicated process. Submitter starts the driver outside of the cluster.\n","\n","Cluster - Driver runs on one of the cluster's Worker nodes. The Master selects the worker. The driver operates as a dedicated, standalone process inside the Worker.\n","\n","#Running a Spark Job\n","Spark submit program initiated with spark driver; creates logical DAG.\n","\n","Spark Driver program checks with the cluster manager-YARN (Mesos or Standalone) for resource availability for executors and launches it.\n","\n","Executors created in Nodes, register to spark driver.\n","\n","Spark driver converts the actions and transformations defined in the main method and allocate to executors.\n","\n","Executors performs the transformations and; actions return\n","\n","values to the driver.\n","\n","While reading from an HDFS, each executor directly applies the subsequent Operations, to the partition in the same task."]},{"cell_type":"markdown","metadata":{"id":"WH0gEGxTt1Gm","colab_type":"text"},"source":["#RDD Caching and Persisting\n","In Spark, you can utilize few RDDs multiple times. If we repeat the same process of RDD evaluation every time, it is needed or taken into action, this task can be time-consuming and memory-consuming, especially for iterative algorithms that look at data multiple times.\n","To resolve the problem of repeated computation, the method of caching or persistence came into the picture.\n","RDDs can be cached with the help of cache operation. They can also be persisted using persist operation.\n","\n","Cache persists with default storage level MEMORY_ONLY.\n","\n","RDDs can also be unpersisted to eliminate RDD from a permanent storage like memory and disk.\n","\n","#How to Assign a Storage Level\n","Let us see how to persist an RDD to a storage level.\n","\n"," result = input.map(<Computation>) \n","\n"," result.persist(LEVEL)\n","\n","By default, Spark uses the algorithm of Least Recently Used (LRU) to remove old and unused RDD to release more memory.\n","\n","We can also manually remove remaining RDD from memory by using unpersist().\n","\n","#Storage Levels\n","![alt text](https://image.slidesharecdn.com/sparkarchitecture-jdkievv04-151107124046-lva1-app6892/95/apache-spark-architecture-70-638.jpg?cb=1446900275)\n","\n","\n","These are the list of storage levels that can be assigned to an RDD"]},{"cell_type":"code","metadata":{"id":"YKH1Azh5f__6","colab_type":"code","colab":{}},"source":["########## THESE ARE ALL SCALA commands so they won't work in Python, don't bother learning. Its ridiculous ###########\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKDU6aGnuHib","colab_type":"code","colab":{}},"source":["##########  RDD Creation  #################  \n","\"\"\"\n","Enter the Spark Shell\n","Enter the Spark Scala IDE:\n","\n","spark-shell\n","\n","Import all required packages:\n","\n","import org.apache.spark.sql.SparkSession\n","\n","In the spark-shell,\n","\n","Spark context available as 'sc'.\n","\n","Spark session available as 'spark'.\n","\n","\"\"\"\n","import org.apache.spark.sql.SparkSession\n","spark-shell\n","\n","\n","\n","\"\"\"\n","Create an RDD\n","Create an Array of integers data from 1 to 9.\n","\n","Create a RDD rdd from the given array of elements.\n","\"\"\"\n","val data = Array(1, 2, 3, 4, 5)\n","val newRDD = sc.parallelize(data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdrhZ87svTEW","colab_type":"code","colab":{}},"source":["\"\"\"\n","Create a json file called \"Students.json\"\n","Installation:\n","\n","Complete installation prior to creating tasks.\n","\n","apt-get update; apt-get install vim -y;\n","\n","Task\n","Create a json file called \"Students.json\" using the command vim Students.json in the terminal, and then copy the following data to the file through IDE:\n","\n","{\"name\":\"Ben\",\"age\":\"23\"} {\"name\":\"Alen\",\"age\":\"22\"}\n","\n","Editor Tips\n","Press i to insert content in the file.\n","Press Esc followed by :wq to save the file.\n","\n","\"\"\"\n","\n","# created json file and pasted above data in it\n","\n","\n","\n","\n","\"\"\"\n","Creating DataFrames\n","Enter the Spark Scala IDE:\n","\n","spark-shell\n","\n","Import all the required packages:\n","\n","import org.apache.spark.sql.SparkSession\n","\n","In the spark-shell,\n","\n","Spark context available as 'sc'.\n","\n","Spark session available as 'spark'.\n","\n","Perform the following tasks:\n","\n","Create an sqlContext from the SparkContext sc.\n","\n","Read the json file using the sqlContext created.\n","\n","Display the results to stdout.\n","\n","\"\"\"\n","\n","\n","\n","############  For third handson- this helped a lot,\n","\n","# 1 crete json file, copy table to it\n","\n","val sc: SparkContext // An existing SparkContext.\n","val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n","val df = sqlContext.read.json(\"home/spark/input.json\")\n","df.registerTempTable(\"students\")\n","val teenagers = sqlContext.sql(\"SELECT name, age FROM students WHERE age >= 13\")\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sg8Z6dfn5_D6","colab_type":"code","colab":{}},"source":["########## Final Hands-On ####\n","\n","val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/scrapbook/tutorial/Spark/matches.csv\")\n","\n","val rddFromFile = spark.sparkContext.textFile(\"/home/scrapbook/tutorial/Spark/matches.csv\")\n","\n","val rdd = rddFromFile.map(f=>{f.split(\",\")})\n","\n","val reduced_rdd = mofm.reduceByKey(_ + _).map(item => item.swap).sortByKey(false).take(5)\n","\n","val rd = sc.sc.parallelize(List(data))\n","\n","rd.saveTextFile"],"execution_count":0,"outputs":[]}]}
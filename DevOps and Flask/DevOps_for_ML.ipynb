{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DevOps_for_ML.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSGigvPDFcSC/AYwRG8xby"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wVGbCUm1cr4C","colab_type":"text"},"source":["#DevOps for Data Science\n","The machine learning model built by a data scientist has to be deployed into production for usage.\n","\n","When a new version of a model is available, then the old version in production has to be replaced with a new one.\n","\n","This process of deploying machine learning models is very similar to the deployment of software code.\n","\n","As a reason, DevOps practices and habits can be followed while deploying machine learning models into production.\n","\n","#About the Course\n","This course mainly aims for those, who need support in the deployment of machine learning models in production.\n","\n","In this course, you will learn about a set of tools, used for automating the deployment of machine learning models in production.\n","\n","The course also covers how to deploy the models as a service and containerize these services.\n","\n","You will also learn about securing models and monitoring them in production.\n","\n","#Prerequisites of this Course\n","Good knowledge of Machine Learning and its concepts.\n","Knowledge of building models using simpler algorithms like linear regression, logistic regression.\n","Basic knowledge of how the Python micro web framework, Flask works.\n","\n","\n","#Data Collection\n","The first part of any Data Science project is Data Collection. It enables an organization to derive answers to specific questions.\n","\n","The data can be collected from multiple sources such as Relational Databases, NoSQL Databases, Spreadsheets, Logfiles and so on.\n","\n","Collecting high-quality data is essential for getting correct insights from the data.\n","\n","In reality, most of the raw data is redundant and can be structured, unstructured or semi-structured.\n","\n","\n","#Feature Engineering\n","Feature Engineering refers to the process of deriving new data features, from existing data, which are used further by a machine learning algorithm as input.\n","\n","Feature Engineering is the most crucial step in Machine Learning.\n","\n","Some of the tasks carried in feature engineering are:\n","\n","Identification of correlated data columns. Highly correlated data columns do not add much to the predictive power of the generated model.\n","Creating new data columns from one or more existing columns.\n","\n","#Building Models\n","Building a Model is the most significant step in machine learning.\n","\n","You have to choose a machine learning algorithm and train the algorithm with extracted features dataset.\n","\n","The training process involves initialization of random values, known as weights, for the chosen algorithm and then perform the predictions.\n","\n","The predictions are compared with the known output.\n","\n","The weights are then readjusted and above step is repeated until model gets better prediction ability.\n","\n","#Running ML Models\n","Once models are validated, they can be used for prediction in two ways.\n","In the first case, the predict methods, associated with a model, can be called directly from a script and obtain the result.\n","Most of the times, the script is run on a terminal.\n","In the second case, the model is moved into production and users interact with it over the internet. In this case, predict methods of a model are not directly accessible.\n","Many data scientists are familiar only with the first case."]},{"cell_type":"markdown","metadata":{"id":"A97Q6v4odX63","colab_type":"text"},"source":["#Environments in DevOps Cycle\n","Software developers, in general, use three different environments: **development**, **staging**, and **production** environments.\n","\n","Development of code, required for building model, and unit tests happen in development environment.\n","\n","In staging environment complete testing is performed in an integrated environment. Here model interaction with external systems such as databases, cache, and other application is tested.\n","\n","Finally, the model is deployed into production environment. This is the environment exposed to users.\n","\n","#Deploying ML models in Production\n","In the last topic, you have seen various activities performed by a data scientist, while developing a machine learning model.\n","\n","Now, in this topic, you will get familiar with different approaches of moving the build model into production.\n","\n","In general, data scientists use R/Python languages for building models and for consuming these models, software developers deploy them in a different stack environment.\n","\n","\n","#Deploying ML Models in Production\n","The two majorly followed approaches for deploying models in production are:\n","\n","Rewrite the whole code of building and accessing model in language, supported by stack environment.\n","Develop APIs for accessing models.\n","The first approach is very tedious and time-consuming.\n","\n","Alternatively, the second option is a reliable one. In this case, Model can be developed in language and stack environment can be supported by another language.\n","\n","The front end application interacts with the model through an API.\n","\n","\n","#Deploying Models with Automation\n","Machine Learning models are generally used for either classifications or regression problems based on some input data.\n","\n","When the input data to a model, deployed in production, changes a new model needs to be trained, built and deployed in production again.\n","\n","The above process is iterative, and it is similar to software program development.\n","\n","The process of deploying models frequently into production is known as **Continuous Integration.**\n","\n","Continuous Integration process can be automated using a popular tool - **Jenkins.**\n","\n","#Deployment Practices\n","The major practices to be followed for a deployment of machine learning model are:\n","\n","Using a Version Control System for models\n","Perform Canary Deployment\n","Secure Models in Production\n","Monitor Performance of Models in Production\n","\n","\n","#Using Version Control System\n","To keep track of different versions of a model, you can use version control systems.\n","\n","One of the popular version control system is Git.\n","\n","Git is a distributed version control system.\n","\n","Git is Open source and it is fast.\n","\n","Git allows multiple local branches that are entirely independent of each other\n","\n","\n","#Canary Deployment\n","In a canary deployment, new changes to a model are exposed to a limited proportion of entire users.\n","\n","This can be achieved when multiple application servers are used for a service. The new model is deployed to one of the existing application servers.\n","\n","In this deployment, only a few users use the new model. The new model usage, it's error rate, and other metrics are measured to verify if any problems exist with a new model.\n","\n","In case if load balancers are used to distribute load across servers, then load balancer can be configured to distribute traffic to each server.\n","\n","#Securing ML models in Production\n","Models have to be secured in Production.\n","\n","Models can be secured using access controls like authentication and authorization.\n","\n","Models in production must be protected against different type of security attacks which compromise model's integrity or availability. This can be achieved by :\n","\n","Reviewing the training data at regular intervals.\n","Reviewing test data and model predictions.\n","Performing random testing and reviewing the predictions.\n","You should also follow practices such as encryption, operations security and disaster recovery.\n","\n","\n","#Performance Monitoring in Production\n","The performance of models, deployed into production, must be monitored continuously to ensure that everything is working properly.\n","\n","Consumption of various resources such as CPU, memory, disk, network I/O must be monitored for checking how efficiently the model is running.\n","\n","Data Scientists must primarily check for drift i. e change in model input data. A **drift** indicates, model input data is not similar to one used in training, thus making the model out dated.\n","\n","Monitoring further required for meeting SLA's agreed for the business."]},{"cell_type":"code","metadata":{"id":"2loH1PD6ca8N","colab_type":"code","colab":{}},"source":["############## USING GIT ##################\n","\n","\"\"\"\n","Create a folder SampleProject using the mkdir command: mkdir SampleProject\n","\n","Go to the SampleProject folder using the cd command. cd SampleProject\n","\n","Create helloworld.py by clicking the following expression:\n","\n","echo \"print('Hello World')\">helloworld.py\n","\n","\n","View the contents of the folder by using the ls command: ls'\n","\n","Initialize a git repository using the git command: git init\n","#>>Initialized empty Git repository in /home/scrapbook/tutorial/SampleProject/.git/\n","\n","Run the command ls -la, and observe if a new directory .git is created.\n","\n","total 16\n","drwxr-xr-x 3 scrapbook scrapbook 4096 Feb 25 22:35 .\n","drwxr-xr-x 1 scrapbook scrapbook 4096 Feb 25 22:33 ..\n","drwxr-xr-x 7 scrapbook scrapbook 4096 Feb 25 22:35 .git\n","-rw-r--r-- 1 scrapbook scrapbook   21 Feb 25 22:34 helloworld.py\n","\n","\n","\n","Files in the SampleProject folder are not tracked.\n","\n","\n","\n","\n","\n","###########\n","\n","Adding 'helloworld.py' file to Git for tracking\n","To track helloworld.py, add the file to 'git' by using the add command. The file is added to the staging area.\n","\n","git add helloworld.py\n","\n","Still, 'git' does not track the contents of the helloworld.py file. View the status by using the status command:\n","git status\n","\n","Commit the changes available in staging area by using the commit method:\n","\n","git commit -m \"Initial Version\"\n","\n","View the status again:  git status\n","\n","\n","\n","\n","The initial 'commit' creates a master branch. View the list of all branches by using the 'branch' command: git branch\n","\n","\n","##########\n","\n","Creating a new branch\n","\n","Create a new branch new_branch using:\n","git branch new_branch\n","\n","View the list of all available branches by using the branch command. The active branch is prefixed with an asterisk symbol. git branch\n","\n","Go to new_branch by using the checkout command:  git checkout new_branch\n","\n","View all the branches, and check if the active branch is new_branch git branch\n","\n","\n","######\n","\n","Making Changes to 'helloworld.py' file\n","Append the following line to the helloworld.py file.\n","\n","echo \"print('Newly added line in new_branch')\">>helloworld.py\n","\n","Add the changes done to helloworld.py to 'git': git add helloworld.py\n","\n","Commit the changes using the commit command: git commit -m \"Added a feature in new_branch\"\n",">>[new_branch 6c8cc83] Added a feature in new_branch\n"," 1 file changed, 1 insertion(+)\n","\n"," #######\n","\n"," Merge 'new_branch' with 'master'\n","View the contents of the helloworld.py file in new_branch. It will contain two lines. cat helloworld.py\n","\n","Go to the master branch, and view:\n","git checkout master\n","\n","View the contents of the helloworld.py file in master. It will contain only one line, indicating that the master branch and new_branch are different. cat helloworld.py\n","\n","Merge new_branch with master branch by using the merge command. git merge new_branch\n","\n","View the contents of the helloworld.py file again to confirm the merge. cat helloworld.py\n","\n","After merging successfully, if the new_branch is not required, it can be deleted. git branch -d new_branch\n","\n","Run the branch command to view the list of branches. git branch\n","\"\"\"\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcGCGzuziHvO","colab_type":"text"},"source":["#What is a Web Service?\n","A Web Service is a software program available to users over the internet.\n","\n","A user invokes a web service by sending a web request and waits for its response.\n","\n","The logic over here is that you make use of the widely used interface, i.e. internet, to call or invoke the software program and also receive the response from the service, over the interface.\n","\n","In this topic, you will hear about RESTful Web Services / API's which are invoked using HTTP protocols.\n","\n","\n","#Running a Model as a Service\n","To run a machine learning model as a service, the model must be wrapped in a program/API, which must be capable of invoking model functions like predict.\n","\n","When a model is embedded in a program/API, it acts as a service. This service can be invoked through HTTP protocols.\n","\n","When a model is moved into production, it's function can be invoked as a service.\n","\n","Also, a service also acts as an abstraction to a machine learning model from users.\n","\n","\n","#RESTful APIs for ML Model Services\n","A RESTful API invokes a machine learning model functionality, executes it and captures the response.\n","\n","Machine Learning APIs are invoked using HTTP Requests, which send the inputs required for executing a model functionality.\n","\n","**A HTTP Request includes an URL and a HTTP method**.\n","\n","The allowed HTTP methods are : GET, POST, PUT, and DELETE.\n","\n","#HTTP Methods\n","The four HTTP methods allowed in a RESTful API are:\n","\n","GET: GET method sends the required input data for the model, as a query string of the requested URL. It then processes the input data and returns a response.\n","\n","POST: POST method sends the required input data for the model, through the HTTP message body. It further processes the data and returns a response.\n","\n","PUT: PUT method is used generally to edit some content, which is already present at the server side.\n","\n","DELETE: DELETE method is generally used to delete some data, available at the server side.\n","\n","While dealing with machine learning models, you mostly deal with GET and POST methods.\n","\n","#Using Plumber\n","Plumber is used to quickly deploy models, built using R language, as services.\n","\n","A REST API can be easily created by decorating a R function with special comments.\n","\n","API Definition of three examples, written in serviceAPI file, are shown below.\n","\n","```\n","# serviceAPI.R\n","\n","#* Echo back the input\n","#* @param msg The message to echo\n","#* @get /echo\n","function(msg=\"\"){\n","  list(msg = paste0(\"The message is: '\", msg, \"'\"))\n","}\n","\n","#* Plot a histogram\n","#* @png\n","#* @get /plot\n","function(){\n","  rand <- rnorm(100)\n","  hist(rand)\n","}\n","\n","#* Return the sum of two numbers\n","#* @param a The first number to add\n","#* @param b The second number to add\n","#* @post /sum\n","function(a, b){\n","  as.numeric(a) + as.numeric(b)\n","}\n","```\n","\n","#Best Practices for API Design\n","Some of the best practices to be followed for API design are listed below:\n","\n","Use a RESTful interface because it is easy for developers to work with HTTP and JSON.\n","\n","Use GET or POST methods to invoke machine learning models over HTTP.\n","\n","Document the API using tools like Swagger.\n","\n","Include the term api in the path of URLs. This distinguishes a standard HTTP request from an API call.\n","\n","Also use version number in an API call.\n","\n","Use API keys for controlling access to services."]},{"cell_type":"code","metadata":{"id":"eC3rz8imkHtx","colab_type":"code","colab":{}},"source":["#################  Runnig model as a Service #############\n","\n","\"\"\"\n","Installation\n","Complete installation prior to creating tasks.\n","\n","Click the following commands to install the required packages automatically:\n","\n","pip3 install --user --upgrade setuptools\n","\n","pip3 install --user numpy\n","\n","pip3 install --user scikit-learn\n","\n","pip3 install --user flask\n","\n","pip3 install --user flask-restful\n","\n","\n","\n","#######################\n","\n","\n","Generating a Model\n","Create a folder SampleProject by using the mkdir command:\n","mkdir SampleProject\n","\n","Change the working directory to SampleProject by using the cd command: cd SampleProject\n","\n","Create an empty folder models to store the created models mkdir models\n","\n","Create a file model_generator.py using the following content:\n","\n","```\n","echo \"from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","import pickle\n","\n","iris = datasets.load_iris()\n","validation_size = 0.20\n","seed = 100\n","X_train, X_test, Y_train, Y_test = train_test_split(iris.data,\n","                                                    iris.target,test_size=validation_size,\n","                                                    random_state=seed)\n","\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, Y_train)\n","\n","with open('models/iris_classifier_model.pk', 'wb') as model_file:\n","    pickle.dump(knn, model_file)\">model_generator.py\n","\n","```\n","Run the model_generator.py script. It creates the required model, and stores it in the models folder. python3 model_generator.py\n","\n","\n","\n","####################\n","\n","\n","Developing a RESTful API using Flask\n","Add the following API to the file iris_classifier.py:\n","\n","echo \"from flask import Flask, request\n","from flask_restful import Resource, Api\n","import pickle\n","\n","app = Flask(__name__)\n","\n","api = Api(app)\n","\n","\n","def classify(petal_len, petal_wd, sepal_len, sepal_wd):\n","    species = ['Iris-Setosa', 'Iris-Versicolour', 'Iris-Virginica']\n","    with open('models/iris_classifier_model.pk', 'rb') as model_file:\n","        model = pickle.load(model_file)\n","\n","    species_class = int(model.predict([[petal_len, petal_wd, sepal_len, sepal_wd]])[0])\n","    return species[species_class]\n","\n","\n","class IrisPredict(Resource):\n","   def get(self):\n","        sl = float(request.args.get('sl'))\n","        sw = float(request.args.get('sw'))\n","        pl = float(request.args.get('pl'))\n","        pw = float(request.args.get('pw'))\n","\n","\n","        result = classify(sl, sw, pl, pw)\n","\n","        return {'sepal_length':sl,\n","                'sepal_width':sw,\n","                'petal_length':pl,\n","                'petal_width':pw,\n","                'species':result}\n","\n","\n","api.add_resource(IrisPredict, '/classify/')\">iris_classifier.py\n","\n","\n","\n","####################################################\n","\n","Running Model as a service\n","\n","Set the FLASK_APP environment variable:  export FLASK_APP=iris_classifier.py\n","\n","Set the other environment variables required: export LC_ALL=C.UTF-8\n","\n","export LANG=C.UTF-8\n","\n","Start the server and expose the API as a service: python3 -m flask run --host=0.0.0.0 --port=8000\n","\n","Open a new terminal and try to access the API with the classify end point. curl \"http://0.0.0.0:8000/classify/?sl=5.1&sw=3.5&pl=1.4&pw=0.3\"\n","\n","\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHbtAUKemaNs","colab_type":"text"},"source":["#What are Containers?\n","A container is a standard unit of software that packages up the code and all its dependencies, so the application runs quickly and reliably from one computing environment to another. - www.docker.com\n","\n","A container consists of an application, all of its dependencies, libraries and other configuration files required for running it.\n","\n","Applications running in containers access shared operating system kernel.\n","\n","Containers are reliable when moving software from one environment to another such as movement from a staging environment to production.\n","\n","\n","# Containers and VM \n","Containers are different from virtual machines.\n","\n","\n","#Application Scaling\n","Application Scaling refers to the alteration of computing resources required by the application, as load changes.\n","\n","Applications can be scaled vertically or horizontally.\n","\n","Vertical Scaling: It increases computing resources by adding more resources, directly, to an existing server.\n","\n","Horizontal Scaling: It increases computing resources by adding more physical machine or servers. The application runs on multiple servers, present in the cluster.\n","\n","#Introduction to Docker\n","Docker is the widely used containerization tool.\n","\n","The three main components of Docker are:\n","\n","Docker Engine: An application which runs Docker images and also contains a command line utility.\n","\n","Docker Client: It contains tools for building and running Docker images. It helps in interacting with other Docker components.\n","\n","Docker Registry: It stores Docker images. Docker Hub is a public registry, which can be used by anyone to share and access public Docker images.\n","\n","#Docker Images\n","A Docker Image is a binary file, containing details of resources required to execute an application.\n","\n","A Docker Image can be obtained either by building it using a Dockerfile or from a Docker Registry.\n","\n","A Dockerfile is a configuration file that specifies the components to be included in an image.\n","\n","A Dockerfile also contains commands, which are invoked when Docker Image gets executed.\n","\n","\n","#Building Docker Images\n","Docker Images are built from Docker Files.\n","\n","Watch  a video\n","\n","#uilding Docker Image Example\n","The below video illustrates the process of building a Docker Image from a Docker File.\n","\n","Watch a video\n","\n","\n","#Docker Registry\n","Docker Registry: Docker Registry is a storage and delivery system for named Docker Images.\n","\n","It can be taught as a collection of repositories identified by a name.\n","\n","A Repository is just a collection of different versions of a single Docker Image.\n","\n","Docker Images can be stored either in a public registry like Docker Hub or a private registry.\n","\n","pull and push commands are used to retrieve and store an image from a Docker Registry.\n","\n","\n","#Using Docker Registries\n","Docker Images can be managed using Docker Registries.\n","\n"]},{"cell_type":"code","metadata":{"id":"pdcVIMDjn3MJ","colab_type":"code","colab":{}},"source":["##########################  Hands on Running ML service in a Container  #####################\n","\n","\n","\"\"\"\n","\n","Cloning Latest version from Git\n","Create a folder ClassifierExample by using the mkdir command: mkdir ClassifierExample\n","\n","Go to the ClassifierExample folder by using the cd command: cd ClassifierExample\n","\n","Clone the git repository samplemlproject.git by using the following command: git clone https://github.com/gpacju/samplemlproject.git\n","\n","\n","#####################################\n","\n","Create a Dockerfile\n","Create a Dockerfile using the following contents in the ClassifierExample folder.\n","\n","echo 'FROM ubuntu\n","\n","ENV INSTALL_PATH /home/samplemlproject\n","RUN mkdir -p $INSTALL_PATH\n","\n","WORKDIR $INSTALL_PATH\n","\n","LABEL classifier_version=\"1.0\"\n","LABEL owner=\"TCS ML Dept.\"\n","\n","# Install libraries and packages\n","RUN apt-get update && apt-get install -y \\\n","python3-pip \\\n","python3-dev \\\n","python3-numpy \\\n","python3-scipy\n","\n","RUN pip3 install scikit-learn flask flask-restful\n","\n","COPY samplemlproject/iris_classifier.py /home/samplemlproject/iris_classifier.py\n","COPY samplemlproject/models /home/samplemlproject/models\n","\n","# Expose the port for the API\n","EXPOSE 5000\n","\n","# Set Environment variables\n","ENV environment PRODUCTION\n","ENV LC_ALL C.UTF-8\n","ENV LANG C.UTF-8\n","ENV FLASK_APP iris_classifier.py\n","\n","# Run the API\n","CMD [\"python3\", \"-m\", \"flask\", \"run\", \"--host\", \"0.0.0.0\"]'>Dockerfile\n","\n","###############################################\n","Build a Docker Image\n","Build a docker image named mlexample by using the docker build command: docker build -t mlexample .\n","\n","lot of bullshit loading here 16/16 steps completed\n","##########################\n","\n","Run the Docker Image\n","Run the docker image by using the following command: docker run -p 8000:5000 mlexample\n","\n","Open a new terminal and run the following curl command: curl \"http://localhost:8000/classify/?sl=5.1&sw=3.5&pl=1.4&pw=0.3\"\n","\n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcEMn86UpdF6","colab_type":"text"},"source":["#Need for Running Multiple Instances\n","Till now, you have studied, how to deploy a machine learning model as a service and how to deploy a service in a container.\n","\n","If the load on the application does not vary much, then a single server would suffice to match the load.\n","\n","However, in general, the load is dynamic. When the load is high, there is a need to scale your application.\n","\n","Scaling can be achieved by running multiple instances, in a cluster, and distributing the load across them.\n","\n","In this topic, you will learn how to run multiple instances of a service in a cluster.\n","\n","In a cluster, multiple application servers are run by deploying machine learning code in multiple containers.\n","\n","#Introduction to Kubernetes\n","Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io\n","\n","A Kubernetes cluster has two types of nodes, masters and minions.\n","\n","Master node is a single node that manages the entire cluster. It coordinates activities like scheduling applications, maintaining applications, scaling applications and so on.\n","\n","Minions are the virtual machines that run containers having machine learning service. They serve as worker machines.\n","\n","Each minion node contains a Kubelet, the agent which manages the node and communicates with master.\n","\n","#More about Kubernetes\n","Kubernetes helps in scaling the containers.\n","\n","Wattch a Video\n","\n","\n","#Creating a Kubernetes Cluster\n","A Kubernetes cluster can be deployed either on a physical or a virtual machine.\n","\n","In this course, you will see the creation of Kubernetes cluster using Minikube.\n","\n","Installation of Minikube is different for different operating systems.\n","\n","You can interact with the cluster using Minikube command line interface, kubectl.\n","\n","After successfully installing Minikube you can start a kubernetes cluster using below shown command.\n","```\n","$ minikube start\n","```\n","#Deploying Containers in Kubernetes Cluster\n","A Kubernetes deployment is all about running a set of pods, which implements a service.\n","The specification of deployment are mentioned in a configuration file, generally a .yaml file.\n","\n","#Scaling up a Kubernetes Cluster\n","By modifying the contents of the deployment file, you can scale up or down a kubernetes cluster.\n","\n","#Auto Scaling a Kubernetes Cluster\n","Auto scaling of containers can be achieved using kubectl autoscale command.\n","\n","The below example shows auto scaling of deployment tomact-deployment.\n","\n","kubectl autoscale deployment tomcat-deployment --cpu-percent=50 --min=1 --max=10\n","The autoscaler maintains between 1 and 10 replicas of pods with tomcat-deployment.\n","\n","The autoscaler either increases or decreases the number of replicas and maintains average CPU utilization across all pods to be 50%."]},{"cell_type":"code","metadata":{"id":"50oiL48iqu6d","colab_type":"code","colab":{}},"source":["############################ Scaling ML service using Kubernetes ######################\n","\n","\"\"\"\n","Start Cluster\n","Launch the minikube cluster by using the following command:\n","\n","minikube start\n",">>>Starting local Kubernetes v1.10.0 cluster...\n","Starting VM...\n","Getting VM IP address...\n","Moving files into cluster...\n","Setting up certs...\n","Connecting to cluster...\n","Setting up kubeconfig...\n","Starting cluster components...\n","Kubectl is now configured to use the cluster.\n","Loading cached images from config file.\n","\n","\n","\n","##########################################################\n","Use the following yaml to deploy iris-classifier-site to the cluster.\n","\n","echo \"apiVersion: apps/v1\n","kind: Deployment\n","metadata:\n","  name: iris-classifier-site\n","  labels:\n","      app: web\n","spec:\n","  replicas : 1\n","  selector :\n","    matchLabels:\n","       app : iris-classifier\n","  template :\n","    metadata :\n","        labels : { app : iris-classifier }\n","    spec:\n","      containers:\n","        - name: mlexample\n","          image: gpcplay/playimages:mlexample\n","          ports:\n","              - containerPort: 5000\">iris-classifier-deployment.yaml\n","\n","\n","Hint: Use kubectl on the iris-classifier-deployment.yaml file.\n","\n","Wait till the pods are created.\n","\n","$ kubectl create -f iris-classifier-deployment.yaml\n","\n","\n","\n","\n","\n","######################################################################\n","\n","Create a Service for App\n","Use the following yaml to expose port 30800 outside the cluster. This service will be used by the iris-classifier-site app.\n","\n","\n","echo \"apiVersion: v1\n","kind: Service\n","metadata:\n","    name: iris-classifier-svc\n","spec:\n","  selector:\n","    app: iris-classifier\n","  type: NodePort\n","  ports:\n","    - port: 5000\n","      nodePort : 30800\n","      targetPort: 5000\">iris-classifier-service.yaml\n","\n","\n","Hint: Use kubectl on the iris-classifier-service.yaml file.\n","$ kubectl create -f iris-classifier-service.yaml\n","\n","\n","\n","######################################################################################################\n","\n","\n","Verify if the application is running by using the following command: kubectl get pods\n","\n","List the service in the current cluster by using the following command: kubectl get services\n","\n","Create an environment variable NODE_PORT by using the following expression:\n","\n","export NODE_PORT=$(kubectl get services/iris-classifier-svc -o go-template='{{(index .spec.ports 0).nodePort}}')\n","echo NODE_PORT=$NODE_PORT\n","\n","\n","Test if the app is exposed outside the cluster by using curl: curl \"http://$(minikube ip):$NODE_PORT/classify/?sl=5.1&sw=3.5&pl=1.4&pw=0.3\"\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZOcjraauYpG","colab_type":"text"},"source":["#Course Summary\n","You have arrived at the culmination of the course. In this course, you have explored the following topics:\n","\n","What is DevOps and why is it important.\n","Various activities carried out in building and using machine learning models.\n","How to deploy models in production and got familiar with few deployment practices?\n","How models are run as a service in production using frameworks like Plumber, Flask?\n","How are machine learning services containerized using Docker?\n","How are containerized applications scaled using Kubernetes?"]}]}
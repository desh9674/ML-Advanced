{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sagemaker.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyuxDsyzuCmDMljQyIklF/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tQGB0bvMSWec","colab_type":"text"},"source":["#In this Course\n","You will learn:\n","\n","Overview of SageMaker\n","Algorithm Implementation\n","Training and Deploying Models\n","Performance Tuning of Models\n","Reinforcement Learning with SageMaker\n","Ground Truth - Automating Data Labeling\n","Scaling and Commercializing Models\n","Authentication, Monitoring, and Security\n","Limits and Integrating Frameworks\n","Swipe over to the next cards to start learning.\n","\n","#What is SageMaker?\n","SageMaker provides a Platform for Data Scientists and Developers to train and build various machine learning models and deploy them into a production environment.\n","It offers instances backed by environments integrated with Jupyter Notebook to help in every step of the Data Science life cycle.\n","It provides optimized in-built algorithms along with support for external frameworks that can be operated on large amounts of data in a distributed environment.\n","It is flexible for any workflow, and billing is without commitments/minimum fees in use and pay manner.\n","\n","![alt text](https://docs-cdn.fresco.me/system/attachments/files/004/823/891/large/95bf857c820ed273d46d2d03cc59a8d77c93dc7f/Datascience_lifecycle.jpeg)\n","\n","SageMaker is used in the entire Data Science Life cycle.\n","It ranges from Data cleaning, preparation, pre-processing, training, evaluation an deployment.\n","Machine Learning is a continuous process, SageMaker facilitates this through inference collection and model retraining with new data.\n","\n","#How Does This Work?\n","SageMaker leverages several Amazon/AWS Services to achieve its functionality.\n","It uses Amazon S3, API, Lambda, EC2, and CloudWatch.\n","These services are used to store, compute, monitor and serve the applications in real-time within milliseconds.\n","Considering the Data Science Lifecycle,\n","Data can be stored and fetched from Amazon S3.\n","Cleaning/Preprocessing of data is facilitated through Jupyter Notebooks integrated Instances.\n","Training can be done on both CPU and GPU Instances with pre-loaded or customized algorithms.\n","Evaluation and inference collection can be done by AWS SDK or SageMaker SDK and Jupyter Notebooks.\n","For Deployment, Models can be reengineered for application integration or can be deployed through Hosting Services involving simple API calls.\n","Move over to the next cards to learn it in detail.\n","\n","\n","#What's Next?\n","Once an Instance is created,\n","\n","Data can be imported from S3 using Jupyter Notebook Scripts.\n","Data exploration, transformation and pre-processing are facilitated through Jupyter Notebooks.\n","Once the Data is prepared models are trained, deployed and validated as per required.\n","You can find Step by Step Instructions here for Data Download and Pre-processing, Model Training, Deployment, Validation, Cleaning Up and Considerations.\n","\n","We will look in-depth about training and deployment in the next cards.\n","\n","#Training a Model\n","Training a model through SageMaker requires a Training Job.\n","A Training Job is created using S3 bucket details for training data, EC2 registry with training code, compute resources and output destination.\n","Once a training job starts, artifacts are created and stored as output files in the S3 bucket mentioned as an output destination.\n","Training Job can be triggered by a management console, but considerations must be taken such that memory issues do not cause a system failure.\n","\n","#Deploying a Model\n","A model can be deployed using Hosting Services.\n","Using this method, when a model is created, HTTPS end-point can be configured and created.\n","Different versions of end-points can be created and updated as per requirements.\n","Once a model is deployed, predictions from the model can be achieved through one prediction at once or through the batch transform.\n","Batch transform is preferred for inference generation and when real-time processing is not needed.\n","It is made possible through integration with Amazon S3.\n","\n","#Validating Models\n","Validation can be done in two methods:\n","\n","Offline: Using \"hold-out\" datasets where some part of training data is retained and used for evaluation. Typically 20-30% is kept aside for this practice.\n","\n","Online: In this method, multiple models can be deployed using the same endpoint and endpoint can be configured to send small samples of live data to each model as specified. Once each model is evaluated, the required model can be configured to take in all the traffic.\n","\n","SageMaker Console is the preferred method to perform the tasks related to training, deployment, and validation.\n","\n","\n","#Instances\n","Instances in SageMaker are EC2 instances with Jupyter Notebook application running in it. It is generally called a SageMaker Notebook Instance.\n","Jupyter Notebook can be considered as a REPL environment for code execution where ML models can be built, trained, evaluated, and deployed.\n","Creating a Jupyter Notebook:\n","Creates Virtual Private Cloud - Subnet to access instance by default.\n","Launches ML Compute Instance - Inside VPC subnet where Notebook Instance runs.\n","Installs Anaconda Packages - Anaconda common packages and packages like TensorFlow and Apache MXNet are installed along with other deep learning packages in an instance.\n","Creates Storage Volume - 5 GB (default) to 16TB of storage to store data in 1GB increments.\n","Copy Jupyter Notebooks Samples - Operating samples are loaded into the instance.\n","\n","\n","#Interacting with Instances\n","Instances can be accessed via Management Console or through API.\n","Access can be secured via IP Filter or VPC Interface.\n","Example Notebooks are available in the instance as well as GitHub to understand the working of SageMaker.\n","Kernel support is provided for Python 2/3, TensorFlow, PySpark and MXNet by default.\n","Kernels can be installed for Scala, R, and Theano.\n","They can further be integrated with Git Repositories.\n","\n","#Instance Selection\n","EC2 instance types must be specified to run these training algorithms.\n","Recommeded instances are:\n","ml.m4.xlarge, ml.m4.4xlarge, and ml.m4.10xlarge\n","ml.c4.xlarge, ml.c4.2xlarge, and ml.c4.8xlarge\n","ml.p2.xlarge, ml.p2.8xlarge, and ml.p2.16xlarge\n","Based on algorithm, GPU instances can also be used but may incur additional costs.\n","Cost effective solution may be selected at the later point according to scale.\n","\n","#Algorithm Selection\n","To build a model, SageMaker provides algorithms in three ways.\n","Built-In Algorithms: These algorithms are already present in SageMaker and are further tested extensively for performance and accuracy. SageMaker provides hyperparameter tuning service which can further improve the performance of these algorithms without much effort.\n","Custom Algorithms: We bring these algorithms from Git or built using the SageMaker Platform. They are custom built for our specific problem when in-built algorithms are not much useful.\n","AWS Market Place: Algorithms can be subscribed or can be paid for in AWS Market Place for from their respective publishers if they satisfy our needs.\n","\n","\n","#Built-In Algorithms\n","SageMaker provides 17 built-in algorithms that include algorithms for classification, text processing, image processing, Clustering, etc.\n","Based on the algorithm selected, it can be trained, tested, and validated accordingly.\n","For text-based algorithms, the csv/text data format is used for training.\n","Training data can be collected using Amazon Athena, AWS Glue, Amazon RDS, Amazon EMR, and Amazon Redshift, but it must be pre-processed before training job is called.\n","\"Hold-out\" datasets for fine-tuning models can be specified while training is initiated.\n","Training Logs can be monitored via Amazon CloudWatch.\n","\n","\n","#Available Built-in Algorithms\n","Algorithm\tUsage\tAlgorithm\tUsage\n","BlazingText\tNLP\tDeepAR Forecasting\tForecasting Scalar time series\n","Factorization Machine\tClassification and Regression\tImage Classification\tSupervised Image Classification\n","IP Insights\tUnsupervised IPv4 classification\tK-Means\tUnsupervised Clustering\n","K - Nearest Neighbors\tNon-parametric Classification/regression\tLatent Dirichlet Allocation\tText Processing\n","Linear Learner\tClassification/Regression\tNeural Topic Model\tUnsupervised Text Processing\n","Object2Vec\tGeneral Purpose Neural Embedding\tObject Detection\tImage Classification\n","Principal Component Analysis\tUnsupervised Feature Reduction\tRandom Cut Forest\tUnsupervised Anamoly Detection\n","Semantic Segmentation\tComputer Vision\tSequence-to-Sequence\tMachine Translation\n","XGBoost\tClassification/Regression\t\t\n","\n","\n","#Custom Algorithms\n","SageMaker Algorithms are packed as Docker Images.\n","This facilitates flexibility in case of both built-in and custom algorithms.\n","Custom Algorithms can be implemented in multiple ways:\n","Using a custom Inference code with built-in algorithms.\n","Using custom algorithms with Sagemaker Inference code.\n","Using custom inference code and algorithms packed as Docker Image.\n","Writing code in Jupyter Notebook Instance using advanced frameworks in SageMaker and using it.\n","\n","\n","#Extending Custom Algorithms\n","Custom Algorithms once created can further be extended as:\n","Algorithm Resource: The Algorithm along with Inference Code(optional) can be created into an Algorithm Resource which can be used for training jobs. It can further be published into AWS Marketplace for monetizing it.\n","Model Package Resource: Along with inference code and/or algorithm resource, and artifact location, a Model Package Resource can be created and published which can be directly used for creating deployable models.\n","These resources can either be monetized, or we can even use/subscribe to resources of others using the AWS Market Place.\n","8 of 11\n","\n","#From AWS Market Place:\n","Algorithm resources can be used to create training jobs, hyperparameter tuning, and model package resources.\n","Model package resources can be used to create models, publish endpoints through hosting services and perform batch transform/live inference jobs.\n","\n","\n","#Training Metrics\n","In the previous topics, we saw how training jobs can be performed on algorithm selection and how they can be validated.\n","\n","Once a training job is run, the details such as training error and prediction accuracy are sent as logs to Amazon CloudWatch, where they can be analyzed and visualized. They can be monitored to analyze model performance.\n","\n","Metrics are pre-defined for built-in algorithms. However, for custom algorithms, you must define what must be sent as metrics through the Management Console or SDK.\n","\n","SageMaker uses regular expressions to capture details among the log files.\n","\n","You can find more on this, here.\n","\n","#Incremental Training\n","Incremental training helps you to use existing artifacts or part of them along with expanded dataset. This is to be considered when inference accuracy decreases over time.\n","This saves time and resources when we can use parts of public artifacts or our artifacts for training job as a training need not be done from scratch.\n","It can be used to continue paused/stopped training job and enables training using hyperparameter tuning or different datasets while saving resources and being cost effective.\n","It can be either done via a management console or SDK.\n","\n","#Including Metadata\n","To include Metadata with the dataset for the training job, an Augmented Manifest file must be used.\n","This file must also be stored in S3, but the file type must be specified beforehand.\n","Also, the file must be in JSON Lines format.\n","This can be performed either via the Management console or SDK.\n","\n","#Hyperparameter Tuning\n","Hyperparameters are parameters set on the model of a specific algorithm before training starts, which at a higher level decides the capacity and speed of learning of model.\n","For each algorithm, there can be multiple hyperparameters which have multiple ranges.\n","SageMaker supports automatic Hyperparameter tuning, where multiple models are trained by varying the hyperparameters in specified ranges to find the best possible model.\n","It is supported for both built-in and custom algorithms but must be specified for the custom algorithms.\n","\n","#How this Works?\n","You must specify the metric that you are trying to improve using Hyperparameter tuning before using it in SageMaker.\n","For in-built algorithms, the metric specification is not required.\n","In SageMaker, Bayesian Optimization is used for Hyperparameter tuning.\n","The first set of hyperparameter values are tested, and the results are evaluated through regression to find the best possible combination of hyperparameters.\n","Sometimes, it may try to explore other values which are never tried to find possible improvement.\n","However, Hyperparameter tuning may improve productivity by selecting the best possible values but cannot always improve the model in case of complex algorithms or scenarios.\n","\n","\n","#Working with Hyperparameter Tuning\n","First, we must choose proper metrics (up to 20) to be monitored out of which one can be specified for improvement.\n","Ranges for these Hyperparameters must be specified to run the jobs.\n","The Best Practices include:\n","Selecting limited number of hyperparameters.\n","Limiting the ranges of hyperparameters.\n","Specifying if parameters are linear or log scaled.\n","Reduce compute resources through optimal selection of concurrent running jobs.\n","Use multiple instance to distribute tuning task.\n","\n","\n","#Pause, Continue, and Stop\n","Warm Start Hyperparameter Tuning jobs use the previous point of tuning as a start point.\n","Although they take more time, they might help in iterative tuning, tuning over new data, hyperparameter customization from previous runs and start a stopped tuning job.\n","Also, few algorithms such as XGBoost and Linear Learner support stopping of tuning job.\n","This is preferred when tuning job is ineffective in improving the chosen metric.\n","It saves time and compute resources and can also be continued once the results are analyzed and hyperparameters are re-customized.\n","\n","\n","#Deploying and Inferences\n","Once a model is trained and tuned, it can be deployed to get inferences i.e predictions/outputs for input data.\n","Publishing the model as HTTPS endpoint using hosted services can attain us this functionality.\n","However, there can be a few more customizations with respect to SageMaker to get inferences.\n","These include:\n","Inference Pipelines\n","SageMaker Neo\n","Elastic Inference\n","\n","\n","#Inference Pipelines\n","Inference Pipeline is a combination of 2 to 5 containers in a linear sequence which can be used for pre-processing, prediction and post-processing of input data.\n","These can be made from built-in models or custom models.\n","The pipeline is completely managed, and each container communicates with the next container in the pipeline using HTTP requests.\n","The final output can be sent to the client, and the pipelines can also be configured for the batch transform.\n","The containers run on the same EC2 instance reducing latency, and their order can be configured and changed using management console or API.\n","\n","\n","#Using the Pipeline\n","Inference Pipelines can be used to eliminate external pre-processing.\n","Spark and Sci-kit learn jobs, for example, can be run on containers before prediction using Amazon Glue and SageMaker packaging, respectively.\n","Few lines of code can help to eliminate external processing in the pipeline.\n","The containers and pipeline can be monitored continuously using Amazon CloudWatch.\n","\n","\n","#Suggested Practices\n","It is recommended to deploy multiple instances in different Availability Zones for the service endpoints to avoid outages in case of instance failure.\n","VPC used to access instance is also suggested to have atleast two subnets in two Availability Zones for the same.\n","Reliable performance can be attained with small instances over multiple Availability Zones.\n","\n","\n","#Moving Further\n","In this topic, you have learned how a model can be optimized while training, how you can add multiple steps in inference generation and improve performance, and manage resources efficiently.\n","\n","Refer the following to learn more about Hyperparameter Tuning and Inferences, and get step-by-step guides for doing the same.\n","\n","Now, move over to the next topics to explore, how models in Amazon SageMaker can be scaled, organized and monetized.\n","\n","\n","#Search in SageMaker\n","Search Functionality in SageMaker can be used for:\n","\n","Finding, Organizing and Evaluating Training jobs. This can be done based on metadata, properties or hyperparameters.\n","Ranking of models based on metrics such as validation accuracy or training loss.\n","Tracing models to their roots such as training jobs and related resources. These resources can be datasets for example.\n","Additionally, tags can be used to group related resources.\n","\n","Note: Search is currently in preview.\n","\n","\n","#Searching Related Resources\n","Amazon SageMaker Search Related Information on resources such as models and training jobs can be investigated to analyze possible issues in scenarios of degrading model performance.\n","Investigations can be done as following:\n","Find training job, metrics, algorithm, hyperparameters, and dataset associated with the model.\n","Find training job, model, and an endpoint associated with a dataset.\n","Investigate model containers and find models associated with them and the total pipeline they are connected in.\n","Search is supported on through both Management console and API.\n","\n","#Auto Scale\n","SageMaker supports Automatic Scaling for production environments.\n","Based on workload, instances can be added or deleted as needed.\n","It is facilitated through Amazon CloudWatch Metrics and configured using Scaling Policy.\n","It can be configured using Management Console, CLI or Auto Scaling API through setting a pre-defined metric to be monitored for scaling.\n","\n","#Configuring Scaling\n","Automatic Scaling requires the following components to be configured:\n","\n","Permissions: SagemakerFullAccessPolicy IAM policy must be active to perform any actions related to auto-scaling.\n","Service Linked Role: AWS Services are linked to IAM policies via Service Linked Role and permission must be given to AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint.\n","Target Metric: Target Metric must be selected to trigger auto-scaling policy as it using target tracking scaling policy.\n","Maximum and Minimum Capacity: Limits must be set on maximum and minimum no.of instances that can be maintained for scaling.\n","Cool Down Period: The period for which the scaling activities must wait between consecutive scaling jobs. The values of scale-in (reduce instances) and scale-out (increase instances) time must be specified(default 300 seconds).\n","\n","\n","#Other Considerations\n","Auto Scaling can be configured, edited or deleted via API or Management console.\n","It is recommended to perform load testing to check if auto-scaling is working.\n","Updating and deleting auto-scaled endpoints may require some work based on Permissions given to Application Auto Scaling.\n","When there is no traffic to the auto-scaled endpoint, auto-scaling may not work as metrics cannot be analyzed without traffic.\n","Step Scaling is also supported by SageMaker where the scaling is based only on capacity rather than target tracking.\n","\n","\n","#Monetize on Your Models\n","In previous topics, we discussed how Algorithms and Model packages could be subscribed from AWS Market Place and used.\n","Once we create a model, it can be transformed into a Model Package Resource. Similarly, algorithm resources can be created from algorithms.\n","These resources can be published for sale on the AWS Market Place.\n","Other users can subscribe to them where they can use them in their models and pay the amount.\n","AWS Private Market Place is also a place for pre-approved authorized products.\n","You can register as a seller and leverage these services to monetize your work."]},{"cell_type":"markdown","metadata":{"id":"cdlNio-MM6aY","colab_type":"text"},"source":["#Reinforcement Learning\n","Reinforcement Learning optimizes agent operation in an environment using policy.\n","The simplest example is a robot trying to get out of a maze in the lowest possible time.\n","In this method, the agent observes the environment, takes action and is rewarded based on the current state of the environment.\n","On a long term scenario, the goal is to get the maximum reward based on its actions.\n","It is suited in scenarios where an agent needs to take autonomous decisions.\n","\n","\n","#Where is it Used?\n","RL is used in particularly solving complex and large problems.\n","It is well suited for dynamic and uncertain environments as the agent learns continuously through reward and punishment for its actions.\n","It is currently being adopted in fields of Gaming AI, Heating, Ventilation, and Air-Conditioning (HVAC) systems, Supply Chains and Industrial Robotics.\n","\n","#Frameworks Supported\n","RL is based on Markov Decision Processes (MDPs).\n","It requires a Deep Learning Framework, RL toolkit and RL environment.\n","SageMaker supports Apache MXNet and TensorFlow frameworks for Deep Learning.\n","The Interaction between environment and agent is managed by RL toolkit and SageMaker supports Ray RLlin and Intel Coach Toolkits providing Industry leading RL algorithms.\n","SageMaker supports a wide variety of open-source and custom RL environments.\n","\n","\n","#More on RL Environments\n","RL Environments are simulations of Real-World scenarios used to simulate the working of the agent.\n","They are useful when real-world training is not possible due to safety considerations (Drone piloting) or when decision making takes more time(gameplay).\n","SageMaker provides OpenAI Gym Environments by default.\n","Users can also open source environments such as environments from EnergyPlus, RoboSchool, etc.\n","Commercial environments can be used from building their containers but must be licensed by the user (e.g., Simulink, MATLAB).\n","\n","#Markov Decision Process\n","An RL Problem must be defined in the form of MDP before it can be worked on. Consider a problem of auto-scaling where capacity must be changed based on a defined set of conditions. The conditions maybe thresholds, alarms or manual steps. The components of this problem include:\n","\n","Objective - Scaling the instance capacity to get the required load profile of an application.\n","Environment - A simulation with data of daily/weekly variations. This must be a custom environment with a load profile. The system is such that when a new instance is created to facilitate scaling, the instance might take some time before it starts taking the jobs/requests.\n","State - It represents the current load, working instances, and failed jobs/requests.\n","Action - Add, Remove, or keep the instances at the same number.\n","Reward - Positive when transactions are successful, the penalty when failing transaction threshold is crossed.\n","This is an example of Markov Decision Processes for Reinforcement Learning.\n","\n","#Workflow in SageMaker\n","For Reinforcement Learning in SageMaker, the following steps can be considered as the basic workflow.\n","\n","Formulating the RL problem - The problem is defined as the components of the Markov Decision Process.\n","Defining the RL environment\n","Defining the presets such as hyperparameters for the algorithm.\n","Writing the training code for the training job\n","Training the RL Model using Amazon SageMaker RLEstimator. This can be done on local or via SageMaker(captures Metrics for CloudWatch).\n","Visualizing Metrics on CloudWatch and over time we can check if the reward is affecting the performance of the model. -Evaluating the model using checkpointed data of previous models.\n","Deploying RL models via hosted services or AWS IoT Greengrass.\n","\n","\n","#Distributed Training and Tuning\n","Amazon SageMaker RL supports distributed training via multi-core and multi-instances distributed training.\n","Training and Environment rollout can also be distributed.\n","Currently, it supports:\n","Single training instance and multiple rollout instances with the same instance type.\n","Single trainer instance and multiple rollout instances with different instance types.\n","Single training instance with multiple cores for the rollout for single threaded lightweight scenarios.\n","Multiple instances for training and rollouts.\n","Hyperparameter tuning is supported for all parameter selections.\n","\n","\n","#Distributed Training and Tuning\n","Amazon SageMaker RL supports distributed training via multi-core and multi-instances distributed training.\n","Training and Environment rollout can also be distributed.\n","Currently, it supports:\n","Single training instance and multiple rollout instances with the same instance type.\n","Single trainer instance and multiple rollout instances with different instance types.\n","Single training instance with multiple cores for the rollout for single threaded lightweight scenarios.\n","Multiple instances for training and rollouts.\n","Hyperparameter tuning is supported for all parameter selections.\n","\n","#Moving Further\n","Now, you have a basic understanding of implementing Reinforcement Learning in Amazon SageMaker.\n","\n","You can learn more about this, here.\n","\n","Now, move on to the next topic to learn about Ground Truth, an advanced labeling service provided as part of Amazon SageMaker."]},{"cell_type":"markdown","metadata":{"id":"SUbcBke2PH16","colab_type":"text"},"source":["#What is Ground Truth?\n","Models require high-quality, large, labeled datasets. Ground Truth helps you to manage data labeling in an automated way.\n","Amazon can provide you it via Amazon Mechanical Turk, or through a vendor in Amazon Market Place or offer you a platform to manage your private workforce via Amazon SageMaker Ground Truth to label data.\n","The labeled data can be used for its models or SageMaker models.\n","There is an option for automated data labeling, where Ground Truth processes the data to be labeled to check if it requires manual work or not. This saves time and effort.\n","This can be done using pre-built or custom tools and templates.\n","Instructions can be given to workers based on labeling template.\n","\n","\n","#How Ground Truth Works?\n","First, the datasets must be stored on the Amazon S3 bucket.\n","The bucket contains the input data for labeling, input manifest file for ground truth to read data and output manifest file with the labeling job results.\n","A labeling job must be created configuring parameters like permissions and files.\n","The workers must be selected who must work on data.\n","The instructions to the workers must be configured.\n","Once this is done, the job can be monitored, cloned or stopped.\n","Find step-by-step instructions here.\n","\n","#Data Labeling\n","Each data object to be labeled is considered a task and the labeling completes when every task is completed.\n","On workforce selection, data objects are divided into batches and sent to workers, one batch after other.\n","Batches check there is no overloading of a workforce and help in iterative training with respect to automated labeling models.\n","Annotation consolidation is a feature where multiple labels from different workers are consolidated for multi-labeling as well as for determining the best label using a probabilistic estimate.\n","This might increase cost and time but improves accuracy and helpful for text, image classification for example.\n","Automated Data labeling saves time and manual effort when enabled by using Machine Learning and taking samples of data labeled by the workforce.\n","A random sample is selected and worked upon by workers, and then an ML model is built validated, and if accuracy is within the mentioned threshold, jobs are run. However, it may increase training and inference instance costs.\n","\n","\n","#Data Management and Instructions\n","All the input Data is to be stored in Amazon S3 along with manifest files which helps GroundTruth to read data.\n","Parts of Input data can be filtered and selected to be sent for labeling.\n","Output data is stored in multiple directories separated by purpose in selected Amazon S3 bucket.\n","Instructions can be given in short or full format.\n","Short format instructions are shown along with data object while labeling and recommended for simple jobs.\n","For complex jobs, detailed Full format instructions which can be shown in the dialog box are better.\n","\n","#More on Ground Truth\n","Ground Truth provides a service called Amazon Cognito to manage your workforce.\n","You can create teams among your private workforce for specific labeling jobs.\n","Also, Ground Truth provides flexibility for creating custom workflows for better application specific productivity."]},{"cell_type":"markdown","metadata":{"id":"yfSGs7cNQAy7","colab_type":"text"},"source":["#Authentication and Access Control\n","SageMaker resources such as instances can be configured for limiting access from unwanted personnel using AWS Identity and Access Management (IAM) in tandem with Amazon SageMaker.\n","IAM access depends on identities such as AWS Account Root User, IAM User, and IAM Role.\n","Resources that support fine-grained access control include Batch Transform Job, Endpoint, Endpoint Config, Hyperparameter Tuning Job, Model, Notebook Instance, Notebook Instance Lifecycle Configuration, and Training Job.\n","Generally, AWS IAM has resource-based policies and Identity-based policies. However, SageMaker only supports Identity-based policies.\n","Resource permission policies must be tagged to respective identities to give access.\n","Conditions to activate a particular policy can also be set.\n","\n","\n","#Using the Policies\n","Accessing the SageMaker Management Console and Ground Truth Console require a number of policies.\n","Based on the role the policies can be limited.\n","AWS managed policies include:\n","Administrator Access: Total Control over SageMaker\n","Data Scientist: Wide range of permissions covering the use cases of Data Scientists.\n","However, SageMakerFullAccess permission gives the identity total control over SageMaker functionality.\n","Also, tags on resources can be used to limit access to their identities.\n","Additional permissions must be given to all types of identities to use other services such as Amazon S3.\n","\n","\n","#CloudWatch Monitoring\n","Amazon CloudWatch is integrated with Amazon SageMaker and is currently the only way to monitor SageMaker based activities.\n","Instance Performance, Running Jobs, and Model Performance are all monitored by CloudWatch. Dashboards are provided to analyze and visualize metrics at an almost real time.\n","Metrics are refreshed with 1 min frequency and statistics are stored for up to 15 months.\n","Search in CloudWatch is limited to current resources whose metrics are updated at the latest point in time (less than two weeks ago). However, selecting a specific resource can give historical data.\n","All the logs from model/algorithm containers, notebook instances, training jobs, and endpoints are also managed/captured by Amazon CloudWatch.\n","\n","\n","#CloudTrail Monitoring\n","All the work happening in SageMaker is monitored via CloudWatch.\n","However, actions taken by user identities in SageMaker via triggering SageMaker API such as training jobs are captured to Amazon CloudTrail.\n","CloudTrail captures all the operations performed via the Management Console.\n","The logs from Non - CloudTrail also captures API jobs such as Automatic Model Tuning.\n","All the operations are shown as Event History and can be stored in a specified Amazon S3 bucket.\n","\n","\n","#Securing with VPC\n","Security in SageMaker can be managed using Amazon Virtual Private Cloud (VPC). It can be maintained at multiple levels using VPC. Once VPC is enabled, Notebook Instances can be accessed only via interface endpoint (PrivateLink) or a NAT gateway.\n","\n","Let's check various levels where VPC can be configured.\n","\n","Notebook Instance:\n","\n","SageMaker Notebook Instances are by default Internet-enabled. This helps to download popular open-source packages without restrictions. By disabling Internet access to instances and enabling VPC, we can ensure malicious code from unknown users/locations do not run on the instance.\n","\n","Training Jobs:\n","\n","Training Jobs store artifact data on Amazon S3. Internet-enabled containers may compromise data.\n","\n","\n","#Securing with VPC\n","Communication between ML instances in distributed training job: By default, training jobs run on VPC. However, a Private VPC can help to comply with regulations if any. Data transfer between instances can also be encrypted at the cost of training time.\n","\n","Endpoints: Models are by default hosted in VPC. However, communication between artifacts containing Amazon S3 and hosting service happens via the Internet. A Private VPC can provide security in such a case.\n","Batch Transform Jobs: Similar to Endpoints, batch transform jobs run on VPC. However, artifact containing Amazon S3 communication can be secured using Private VPC.\n","\n","Training and Inference Containers: These are Internet-enabled by default to access open-source resources, if any, and can be secured necessary.\n","\n","Note: Apart from this, all the resources in AWS Market Place are scanned for Common Vulnerabilities and Exposures (CVE) using The National Vulnerability Database (NVD)."]},{"cell_type":"markdown","metadata":{"id":"1k9SSQx1UTsj","colab_type":"text"},"source":["#Limits\n","SageMaker is only limited by resource allocation.\n","It has limitations with respect to Notebook instances, Training Instances, Automatic Model Tuning - Parameters, runtime concurrency, Hosting Instances and Batch Transform Instances.\n","You can find a list of detailed limits, here.\n","Regarding availability, it is currently available in 14 regions. Find the list, here.\n","\n","\n","#Integrating Frameworks\n","SageMaker is deeply integrated with:\n","\n","Apache Spark: Open-source cluster computing framework\n","Tensorflow: Open-source data flow programming and Neural networks library\n","Apache MXNet: Open-source Deep Learning Framework\n","Scikit-learn: Open-source Machine Learning library\n","Pytorch: Open-source Machine Learning library\n","Chainer: Open-source Deep Learning Framework\n","You can learn more on this, here.\n","\n"]},{"cell_type":"code","metadata":{"id":"WILOmW_qSVy_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
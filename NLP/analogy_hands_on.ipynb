{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"analogy_hands_on.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"eYx0eVqInuJW","colab_type":"text"},"source":[" demo code   http://fangzh.top/2018/dl-ai-5-2h/\n","\n","\n","Another  one  https://www.itdaan.com/blog/2018/02/12/a5b40db308cd9da76d5ec6f8fddf61ba.html"]},{"cell_type":"markdown","metadata":{"id":"SgDrFdSBnnBn","colab_type":"text"},"source":["### Welcome to you first hands-on on word embeddings.\n","#### In this hands-on you will be using pretrained GLoVe word vectors from stanford nlp which you can find [here](https://nlp.stanford.edu/projects/glove/)\n","#### Each word vectors is of dimension 50\n","#### You will be performing following operations:\n","    - Load the pretrained vectors from the text file\n","    - Write a function to find cosine similarity between two word vectors\n","    - Write an function to find analogy analogy problems such as King : Queen :: Men : __?__"]},{"cell_type":"code","metadata":{"id":"FTDbznskt_W1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NfQLqGuuDf1","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74},"outputId":"462f7b2e-f256-4522-df23-5be0ac5eada4","executionInfo":{"status":"ok","timestamp":1578580340516,"user_tz":-330,"elapsed":9714,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["from google.colab import files\n","\n","uploaded = files.upload()"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-fb88e77d-78b9-4339-a64d-2a99b4d5b267\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-fb88e77d-78b9-4339-a64d-2a99b4d5b267\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving word2vec.txt to word2vec.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ambu0khjnnBq","colab_type":"text"},"source":["### Task1\n","- A text file having the trained word vectors is provided for you as word2vec.txt in the same working directory.\n","- Each line in the file is space seperated values where first value is the word and the remaing values are its vector representation.\n","\n","### Define a function get_word_vectors()\n","    parameters: file_name  \n","    returns: word_to_vec: dictionary with key as the word and the value is the corresponding word vectors as 1-d array each element of type float32.  "]},{"cell_type":"code","metadata":{"id":"M8oH4qKxnnBr","colab_type":"code","colab":{}},"source":["import numpy as np\n","def get_word_vectors(gloveFile):\n","    ###Start code here\n","    print(\"Loading Glove Model\")\n","    f = open(gloveFile,'r')\n","    model = {}\n","    for line in f:\n","        splitLine = line.split(' ')\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print(\"Done.\",len(model),\" words loaded!\")\n","    return model\n","    \n","    \n","    \n","    ###End code\n","    #return word_to_vec"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqetYOVjnnBu","colab_type":"text"},"source":["### Using the function you defined above read the word vectors from the file word_vectors.txt and assign it to variable word_to_vec\n","\n","### Expected output  (showing only first few values of vectors)\n","   Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071 ...]  \n","   mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767 ....]  \n","\n","   "]},{"cell_type":"code","metadata":{"id":"bd5asHtEnnBv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"77b4c01e-b94b-4adc-bfb4-a5e7aec50026","executionInfo":{"status":"ok","timestamp":1578580346398,"user_tz":-330,"elapsed":788,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["word_to_vec = get_word_vectors('word2vec.txt')\n","father = word_to_vec[\"father\"]\n","mother = word_to_vec[\"mother\"]\n","print(\"Father: \", father)\n","print(\"mother: \", mother)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading Glove Model\n","Done. 124  words loaded!\n","Father:  [ 0.095496   0.70418   -0.40777   -0.80844    1.256      0.77071\n"," -1.0695     0.76847   -0.87813   -0.0080954  0.43884    1.0476\n"," -0.45071   -0.58931    0.83246   -0.038442  -0.73533    0.26389\n","  0.12617    0.57623   -0.23866    1.0922    -0.3367     0.081537\n","  0.84798   -2.4795    -0.40351   -0.84087    0.12034    0.29074\n","  1.9711    -0.50886   -0.45977   -0.13617    0.55613    0.22924\n"," -0.18947    0.43544    0.65151    0.043537  -0.1162     0.72196\n"," -0.66163   -0.17272    0.27367   -0.28169   -0.82025   -1.5089\n","  0.052787  -0.035579 ]\n","mother:  [ 0.4336     1.0727    -0.6196    -0.80679    1.2519     1.3767\n"," -0.93533    0.76088   -0.0056654 -0.063649   0.30297    0.52401\n","  0.2843    -0.38162    0.98797    0.093184  -1.1464     0.070523\n","  0.58012    0.50644   -0.24026    1.7344     0.020735   0.43704\n","  1.2148    -2.2483    -0.41168   -0.24922    0.31225   -0.49464\n","  2.0441    -0.012111  -0.19556    0.085665   0.27682    0.015702\n","  0.0067683  0.12759    0.87008   -0.40641   -0.21057    0.41651\n"," -0.021812  -0.53649    0.54095   -0.43442   -0.52489   -2.0277\n","  0.13136    0.11704  ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ol9wjBa2nnBy","colab_type":"text"},"source":["\n","### Task 2 Determine the cosine similarity between two word vectors\n","- The formula for cosine similarity is given by\n","  score = $\\large \\frac{U.V}{\\sqrt{||U||.||V||}}$ where ||U|| and ||V|| is the sum of the squares of the elemnts individual vectors\n","  \n","\n","### Define a function named cosine_similarity()\n","    - parameters u, v are the word vectors whose similarity has to be determined\n","    - returns - score: cosine similarity of u and v"]},{"cell_type":"code","metadata":{"id":"AWXl3n2znnBy","colab_type":"code","colab":{}},"source":["def cosine_similarity(u, v):\n","    ###Start code here\n","    \"\"\"\n","    Cosine similarity reflects the degree of similariy between u and v\n","        \n","    Arguments:\n","        u -- a word vector of shape (n,)          \n","        v -- a word vector of shape (n,)\n","\n","    Returns:\n","        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n","    \"\"\"\n","    \n","    distance = 0.0\n","    \n","    ### START CODE HERE ###\n","    # Compute the dot product between u and v (≈1 line)\n","    dot = np.dot(u,v)\n","    # Compute the L2 norm of u (≈1 line)\n","    norm_u = np.sqrt(np.dot(u,u))\n","    \n","    # Compute the L2 norm of v (≈1 line)\n","    norm_v = np.sqrt(np.dot(v,v))\n","    # Compute the cosine similarity defined by formula (1) (≈1 line)\n","    cosine_similarity = dot / (norm_u * norm_v)\n","    ### END CODE HERE ###\n","    \n","    return cosine_similarity\n","    \n","    \n","    \n","    ###End code\n","    #return score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qU2Z802ZnnB1","colab_type":"text"},"source":["#### Run the bellow cell to find the similarity between word vectors paris and rome\n","### Expected output\n","   similarity score : 0.7099411"]},{"cell_type":"code","metadata":{"id":"OxZMFXm4nnB1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d12fab7f-a024-4163-dff9-6fca3162888d","executionInfo":{"status":"ok","timestamp":1578580390926,"user_tz":-330,"elapsed":1237,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["paris = word_to_vec[\"paris\"]\n","rome = word_to_vec[\"rome\"]\n","print(\"similarity score :\", cosine_similarity(paris, rome))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["similarity score : 0.7099411341712597\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qSpwrf8RnnB5","colab_type":"text"},"source":["### Task 3\n","In the word analogy task, we complete the analogy . In detail, we are trying to find a word d, such that the associated word vectors $u_1, v_1, u_2, v_2$ are related in the following manner: $u_1 - v_1 \\approx u_2 - v_2$. We will measure the similarity between $u_1 - v_1$ and $u_2 - v_2$ using cosine similarity.\n","#### As an example,  to find the best possible word for the analogy King : Queen :: Men : __?_ you will perform following steps:\n","- extract word vectors of three words king, queen and men\n","- find the element wise difference between the two word vectors king and queen as V1\n","- Find the element wise difference between the word vector men and each word vector in word_to_vec ditionary as V2 (while doing so exclude the words of interest ie. king, queen and men)\n","- Find the cosine similarity between vector V1 and V2 and choose the word from the word_to_vec ditionary that has maximum similarity between V1 and V2.\n","### Define the function named find_analogy()\n","    - parameters: word1 - string corresponding to word vector $u_1$\n","                  word2 - string corresponding to word vector $v_1$\n","                  word3 - string corresponding to word vector $u_2$\n","                  word_to_vec - dictionary of words and their corresponding vectors\n","    - returns: best_word -  the word such that $u_1$ - $v_1$ is close to $v\\_best\\_word$ - $v_c$, as measured by cosine similarity\n"]},{"cell_type":"code","metadata":{"id":"7mZdx2fwnnB6","colab_type":"code","colab":{}},"source":["def find_analogy(word_a, word_b, word_c, word_to_vec_map):\n","    ####Start code here\n","    \"\"\"\n","    Performs the word analogy task as explained above: a is to b as c is to ____. \n","    \n","    Arguments:\n","    word_a -- a word, string\n","    word_b -- a word, string\n","    word_c -- a word, string\n","    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n","    \n","    Returns:\n","    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n","    \"\"\"\n","    \n","    # convert words to lower case\n","    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n","    \n","    ### START CODE HERE ###\n","    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n","    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n","    ### END CODE HERE ###\n","    \n","    words = word_to_vec_map.keys()\n","    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n","    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n","\n","    # loop over the whole word vector set\n","    for w in words:        \n","        # to avoid best_word being one of the input words, pass on them.\n","        if w in [word_a, word_b, word_c] :\n","            continue\n","        \n","        ### START CODE HERE ###\n","        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n","        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n","        \n","        # If the cosine_sim is more than the max_cosine_sim seen so far,\n","            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n","        if cosine_sim > max_cosine_sim:\n","            max_cosine_sim = cosine_sim\n","            best_word = w\n","        ### END CODE HERE ###\n","        \n","    return best_word\n","    \n","    \n","    ###End code\n","    return best_word"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3YWwPDMnnB8","colab_type":"text"},"source":["### Run the below  code to evaluate your above defined function\n","\n","#### Expected output:\n","    father -> son :: mother -> daughter\n","    india -> delhi :: japan -> tokyo"]},{"cell_type":"code","metadata":{"id":"aE5ro-PgnnB8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"3c5000c5-5f18-4864-9da1-2260b3747923","executionInfo":{"status":"ok","timestamp":1578580829289,"user_tz":-330,"elapsed":914,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["print ('{} -> {} :: {} -> {}'.format('father', 'son', 'mother',find_analogy('father', 'son', 'mother', word_to_vec)))\n","print ('{} -> {} :: {} -> {}'.format('india', 'delhi', 'japan',find_analogy('india', 'delhi', 'japan', word_to_vec)))\n","\n","word1 = find_analogy(\"spain\", 'india', 'tokyo', word_to_vec)\n","word2 = find_analogy(\"small\", 'smaller', 'large', word_to_vec)\n","\n","#with open(\"output.txt\", 'w+') as file:\n","    #file.write(word1+'\\n')\n","    #file.write(word2)\n","\n","print(word1)\n","print(word2)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["father -> son :: mother -> daughter\n","india -> delhi :: japan -> tokyo\n","delhi\n","larger\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gKbVcDHjnnB_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
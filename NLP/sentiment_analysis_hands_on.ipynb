{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"name":"sentiment_analysis_hands_on.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Up4U1IygzkKq","colab_type":"text"},"source":["### In this hands-on you will be building a sentiment classifier on for movie reviews using word vectors and LSTM."]},{"cell_type":"markdown","metadata":{"id":"gf4ItbvIzkKw","colab_type":"text"},"source":["### Import all the necessary packages in the below cell as and when you require"]},{"cell_type":"code","metadata":{"id":"Klb-_7VazkKx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"876a99c8-5c1c-45c2-d8e1-9b9b69d10bcb","executionInfo":{"status":"ok","timestamp":1578648932330,"user_tz":-330,"elapsed":3681,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["import keras\n","from keras.datasets import imdb\n","from keras.datasets.imdb import get_word_index\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"DGjq5W9hzkK1","colab_type":"text"},"source":["#### Downloading the dataset.\n","- Keras has a built in function to download movie review available in imdb. \n","- Each words in the review are represented by their unique index and the labels are in binary format representing positive or negative reviews\n","- The necessary code to download the dataset has been written for you.\n","- The variable **word_to_id** is a dictionary containing words and their corresponding ids\n","- Run the below cell to download the dataset"]},{"cell_type":"code","metadata":{"id":"BREe1BdgzkK2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"9315b2c7-1663-4a84-86ab-aaa545815fd6","executionInfo":{"status":"ok","timestamp":1578648954496,"user_tz":-330,"elapsed":6534,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["vocab_size = 5000\n","(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=vocab_size)\n","word_to_id = get_word_index()\n","print(\"word to id fist five samples {}\".format({key:value for key, value in zip(list(word_to_id.keys())[:5], list(word_to_id.values())[:5])}))\n","print(\"\\n\")\n","print(\"sample input\\n\", X_train[0])\n","print('\\n')\n","print(\"target output\", Y_train[0])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","word to id fist five samples {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'vani': 63951}\n","\n","\n","sample input\n"," [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n","\n","\n","target output 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G_Ry9WDIzkK4","colab_type":"text"},"source":["#### Each review in the dataset has some special tokens such as\n","    - <START> : to identify the start of the sentence\n","    - <UNK> : If some words are not identified in the vocabulary\n","    - <PAD> : The value to be filled if sequence requires padding\n","### Task 1:\n","    - offset the word_to_id dictionary by three values such that 0,1,2 represents START, UNK, PAD respectively\n","    - Once you perform the above step reverse the word_to_id dictionary to represent ids as keys and words as values. Assign the resulting dictionary to id_to_word variable"]},{"cell_type":"code","metadata":{"id":"_oNlOzeQzkK4","colab_type":"code","colab":{}},"source":["#word_to_id = keras.datasets.imdb.get_word_index()\n","#word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n","\n","word_to_id[\"PAD\"] = 0\n","word_to_id[\"START\"] = 1\n","word_to_id[\"UNK\"] = 2\n","id_to_word = {value:key for key,value in word_to_id.items()}\n","#print(' '.join(id_to_word[id] for id in X_train[0] ))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3qQSsvizkK7","colab_type":"text"},"source":["### Run the below code to view the first review in training samples\n","\n","### Expected output\n","START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNK is an amazing actor and now the same being director UNK father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for UNK and would recommend it to everyone to watch and the fly UNK was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also UNK to the two little UNK that played the UNK of norman and paul they were just brilliant children are often left out of the UNK list i think because the stars that play them all grown up are such a big UNK for the whole film but these children are amazing and should be UNK for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was UNK with us all"]},{"cell_type":"code","metadata":{"id":"D1pAuP37zkK7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"c3da69ee-1392-4f7d-ee08-91ff81eb19f6","executionInfo":{"status":"ok","timestamp":1578649544701,"user_tz":-330,"elapsed":981,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["print(\" \".join([id_to_word[i] for i in X_train[0]]))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["START as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room UNK it so heart shows to years of every never going UNK help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but UNK to story wonderful that in seeing in character to of 70s UNK with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other UNK in of seen over UNK for anyone of UNK br show's to whether from than out themselves history he name half some br of UNK odd was two most of mean for 1 any an boat she he should is thought UNK but of script you not while history he heart to real at UNK but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with UNK film want an\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ojx5JcVDzkK9","colab_type":"text"},"source":["from keras.preprocessing import sequence \n","max_review_length = 500\n","sequence.pad_sequences(transformed_data,  maxlen=max_review_length)#### Since each movie reviews are of variable lengths in terms of number of words, so it is necessay to fix the review lenght to few words say upto first 500 words.\n","### Task 3\n","   - For each of the samples of X_train and X_test sample upto first 500 words\n","   - If reviews are less than 500 words pad the sequence with zeros in the beginning to make up the length upto 500\n","   - Assign the padded sequence to X_train_pad and X_test_pad variables for train and test smaples respectively\n","   \n","   [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n","   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n","    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n","   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n","    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n","   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n","   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n","    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n","    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n","   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n","   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n","  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n","   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n","  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n","   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n"," 4472  113  103   32   15   16    2   19  178   32]"]},{"cell_type":"code","metadata":{"id":"gbFUD_UizkK-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":629},"outputId":"5247d3b4-6ffe-48f9-c5a1-3e45e663991f","executionInfo":{"status":"ok","timestamp":1578649750319,"user_tz":-330,"elapsed":1958,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["from keras.preprocessing import sequence \n","max_review_length = 500\n","\n","\n","X_train_pad = sequence.pad_sequences(X_train,  maxlen=max_review_length)\n","X_test_pad =  sequence.pad_sequences(X_test,  maxlen=max_review_length)\n","print(X_train_pad[0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n","   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n","    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n","   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n","    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n","   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n","   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n","    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n","    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n","   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n","   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n","  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n","   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n","  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n","   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n"," 4472  113  103   32   15   16    2   19  178   32]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uf8YDiyCzkLA","colab_type":"text"},"source":["### Using keras [Sequential](https://keras.io/getting-started/sequential-model-guide/) to build an LSTM model using the below specifications\n","- Add an embedding layer( the look up table) such that vacabulary size is 5000 and each word in the vocabulary is 32 dimension vector\n","- Add an LSTM layer with 100 hidden nodes\n","- Add a final sigmoid activation layer\n","- Use adam optimizer and binary cross entropy loss, and metrics as accuracy\n","\n","### Expected Output:\n","\n","#### Layer (type)                 Output Shape              Param #   \n","=================================================================  \n","embedding_5 (Embedding)      (None, None, 32)          160000      \n","_________________________________________________________________  \n","lstm_5 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________  \n","#### dense_5 (Dense)              (None, 1)                 101       \n","=================================================================    \n","Total params: 213,301   \n","Trainable params: 213,301   \n","Non-trainable params: 0  \n","_________________________________________________________________"]},{"cell_type":"code","metadata":{"id":"CfoLNhG4DjJd","colab_type":"code","colab":{}},"source":["from keras import backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.utils import np_utils\n","from keras.layers import Embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-FgCp6ezkLA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"d3f2aa86-a7c7-4b12-8afc-fa59a6c3077b","executionInfo":{"status":"ok","timestamp":1578653275323,"user_tz":-330,"elapsed":958,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["embedding_vector_length = 32 \n","model =  Sequential()\n","model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_review_length)) \n","model.add(LSTM(100)) \n","model.add(Dense(1, activation='sigmoid')) \n","model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n","print(model.summary()) \n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nmeYu_T7zkLD","colab_type":"text"},"source":["### Fit the model with X_train_pad and Y_train as train data and X_test_pad, Y_test as Validation set\n","    - set the number of epochs to 3\n","    - set batch size to 64"]},{"cell_type":"code","metadata":{"id":"00TUnoY1zkLE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"01b888e5-dddb-4d8e-cd72-5d92c636e69f","executionInfo":{"status":"ok","timestamp":1578657591652,"user_tz":-330,"elapsed":992861,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["### Start code here\n","model.fit(X_train_pad, Y_train, validation_data=(X_test_pad, Y_test), nb_epoch=3, batch_size=64) \n","#scores = model.evaluate(X_test, y_test, verbose=0) \n","###End code"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 25000 samples, validate on 25000 samples\n","Epoch 1/3\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","25000/25000 [==============================] - 327s 13ms/step - loss: 0.4540 - acc: 0.7800 - val_loss: 0.4551 - val_acc: 0.7937\n","Epoch 2/3\n","25000/25000 [==============================] - 333s 13ms/step - loss: 0.3040 - acc: 0.8768 - val_loss: 0.3352 - val_acc: 0.8660\n","Epoch 3/3\n","25000/25000 [==============================] - 331s 13ms/step - loss: 0.3027 - acc: 0.8718 - val_loss: 0.3146 - val_acc: 0.8685\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f3c52dcbbe0>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"tPhvWBsMQySe","colab_type":"code","colab":{}},"source":["print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJa9FW5GzkLI","colab_type":"text"},"source":["### Run the below cell to run the model prediction on custom samples"]},{"cell_type":"code","metadata":{"id":"lawwk5NvzkLJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"d8a989a5-db10-439b-83b6-4f15fd7d9d51","executionInfo":{"status":"ok","timestamp":1578659158967,"user_tz":-330,"elapsed":3657,"user":{"displayName":"dhairyashil deshpande","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBekp9I1RUanWY6fBLVDxE8k-iU14p_q2CyE-K2Kg=s64","userId":"08228054099836219584"}}},"source":["import numpy as np\n","bad = \"this movie was terrible and bad\"\n","good = \"i really liked the movie and had fun\"\n","for review in [good,bad]:\n","    tmp = []\n","    for word in review.split(\" \"):\n","        tmp.append(word_to_id[word])\n","    tmp_padded = sequence.pad_sequences([tmp], maxlen=500) \n","    print(\"%s. Sentiment: %s\" % (review,model.predict(np.array([tmp_padded][0]))[0][0]))\n","\n","accuracy = model.evaluate(X_test_pad[:1000], Y_test[:1000])[1]\n","#with open('output.txt', 'w') as file:\n","    #file.write(str(accuracy))\n","\n","print(str(accuracy))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["i really liked the movie and had fun. Sentiment: 0.69151026\n","this movie was terrible and bad. Sentiment: 0.7380902\n","1000/1000 [==============================] - 3s 3ms/step\n","0.86\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fC75D4ZRMVt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}